{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08427696",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76748ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ae7b1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 PDF files to process\n",
      "\n",
      " Processing : attention.pdf\n",
      " ✓ Loaded 15 pages\n",
      "\n",
      " Processing : langchain.pdf\n",
      " ✓ Loaded 75 pages\n",
      "\n",
      " Processing : log_linear_attention.pdf\n",
      " ✓ Loaded 21 pages\n",
      "\n",
      " Processing : NN_attention.pdf\n",
      " ✓ Loaded 22 pages\n",
      "\n",
      "Total documents loaded : 133\n"
     ]
    }
   ],
   "source": [
    "# Read all pdfs inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "\n",
    "    pdf_files = list(pdf_dir.glob(\"*/*.pdf\"))\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\n Processing : {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "            print(f\" ✓ Loaded {len(documents)} pages\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" ✗ Error : {e}\")\n",
    "    print(f\"\\nTotal documents loaded : {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in a data directory\n",
    "all_pdf_documents = process_all_pdfs(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7a76185",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextSplitter.__init__() got an unexpected keyword argument 'seperators'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMetadata : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_docs[\u001b[32m0\u001b[39m].metadata\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m split_docs\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m chunks = \u001b[43msplit_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_pdf_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m chunks\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36msplit_documents\u001b[39m\u001b[34m(documents, chunk_size, chunk_overlap)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msplit_documents\u001b[39m(documents, chunk_size = \u001b[32m1000\u001b[39m, chunk_overlap=\u001b[32m200\u001b[39m):\n\u001b[32m      4\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Splitting docs into smaller chunks for better RAG Performance\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     text_spitter = \u001b[43mRecursiveCharacterTextSplitter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlength_function\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseperators\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     split_docs = text_spitter.split_documents(documents)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSplit \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents into \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(split_docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/AI_Projects_Production/conda_envs/ai-dev/lib/python3.11/site-packages/langchain_text_splitters/character.py:96\u001b[39m, in \u001b[36mRecursiveCharacterTextSplitter.__init__\u001b[39m\u001b[34m(self, separators, keep_separator, is_separator_regex, **kwargs)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     89\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     90\u001b[39m     separators: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     93\u001b[39m     **kwargs: Any,\n\u001b[32m     94\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     95\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a new TextSplitter.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeep_separator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_separator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m     \u001b[38;5;28mself\u001b[39m._separators = separators \u001b[38;5;129;01mor\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     98\u001b[39m     \u001b[38;5;28mself\u001b[39m._is_separator_regex = is_separator_regex\n",
      "\u001b[31mTypeError\u001b[39m: TextSplitter.__init__() got an unexpected keyword argument 'seperators'"
     ]
    }
   ],
   "source": [
    "# Text Splitting into Chunks\n",
    "\n",
    "def split_documents(documents, chunk_size = 1000, chunk_overlap=200):\n",
    "    \"\"\"Splitting docs into smaller chunks for better RAG Performance\"\"\"\n",
    "    text_spitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        seperators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_spitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    # Show example chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata : {split_docs[0].metadata}\")\n",
    "    return split_docs\n",
    "\n",
    "chunks = split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3949ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db502bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78198f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3799af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b93b8bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'shireesha', 'date_created': '2025-12-23'}, page_content='Let this be the main content I am using to create RAG')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"Let this be the main content I am using to create RAG\",\n",
    "    metadata={\n",
    "        \"source\":\"example.txt\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"shireesha\",\n",
    "        \"date_created\": \"2025-12-23\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4b8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a simple txt file\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe023d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"data/text_files/python_intro.txt\":\"\"\"Python is a high-level, interpreted programming language known for its clear syntax and code readability, which was created in 1991. It is popular for beginners due to its gentle learning curve and is widely used in web development, data science, artificial intelligence, and automation. \n",
    "\n",
    "Key features include:\n",
    "Dynamic Typing: Variable types are automatically determined at runtime, which simplifies coding.\n",
    "Multiple Paradigms: It supports object-oriented, functional, and procedural programming styles.\n",
    "Extensive Libraries: A vast ecosystem of libraries and the Python Package Index (PyPI) offer ready-to-use code for various tasks.\"\"\",\n",
    "    \"data/text_files/machine_learning.txt\":\"\"\"Machine Learning (ML) is a type of Artificial Intelligence (AI) that lets computers learn from data to find patterns, make decisions, and predict outcomes, without being explicitly programmed for every task, using algorithms trained on vast datasets for tasks like image recognition, recommendations, and translation. Key types include Supervised Learning (labeled data, e.g., spam filters), Unsupervised Learning (unlabeled data, e.g., customer grouping), and Reinforcement Learning (learning via rewards/penalties for decision-making). \"\"\"\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "print(\"Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7effa2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/text_files/python_intro.txt'}, page_content='Python is a high-level, interpreted programming language known for its clear syntax and code readability, which was created in 1991. It is popular for beginners due to its gentle learning curve and is widely used in web development, data science, artificial intelligence, and automation. \\n\\nKey features include:\\nDynamic Typing: Variable types are automatically determined at runtime, which simplifies coding.\\nMultiple Paradigms: It supports object-oriented, functional, and procedural programming styles.\\nExtensive Libraries: A vast ecosystem of libraries and the Python Package Index (PyPI) offer ready-to-use code for various tasks.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Read the text using text loaders of langchain\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/text_files/python_intro.txt\", encoding=\"utf-8\")\n",
    "\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30b9787a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1441.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/text_files/python_intro.txt'}, page_content='Python is a high-level, interpreted programming language known for its clear syntax and code readability, which was created in 1991. It is popular for beginners due to its gentle learning curve and is widely used in web development, data science, artificial intelligence, and automation. \\n\\nKey features include:\\nDynamic Typing: Variable types are automatically determined at runtime, which simplifies coding.\\nMultiple Paradigms: It supports object-oriented, functional, and procedural programming styles.\\nExtensive Libraries: A vast ecosystem of libraries and the Python Package Index (PyPI) offer ready-to-use code for various tasks.'),\n",
       " Document(metadata={'source': 'data/text_files/machine_learning.txt'}, page_content='Machine Learning (ML) is a type of Artificial Intelligence (AI) that lets computers learn from data to find patterns, make decisions, and predict outcomes, without being explicitly programmed for every task, using algorithms trained on vast datasets for tasks like image recognition, recommendations, and translation. Key types include Supervised Learning (labeled data, e.g., spam filters), Unsupervised Learning (unlabeled data, e.g., customer grouping), and Reinforcement Learning (learning via rewards/penalties for decision-making). ')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"data/text_files\",\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "text_docs=dir_loader.load()\n",
    "text_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3017d9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  9.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 1}, page_content='1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 3}, page_content='Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 4}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 6}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 9}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 10}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 11}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 12}, page_content='Attention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 13}, page_content='The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 14}, page_content='The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 0}, page_content='Brandenburg Technical University Cottbus\\nSoftware Systems Engineering\\nChair of Practical Computer Science\\nMaster Thesis\\nExtracting Knowledge Graphs from User\\nStories Using Langchain\\nThayn´a Camargo da Silva\\nMatriculation Number: 5003023\\nMaster in Artificial Intelligence\\n29.07.2024\\n13.01.2025\\n1. Examiner: Prof. Dr. rer. nat. Leen Lambers\\n2. Examiner: Dr. Kate Cerqueira Revoredo, Humboldt-Universit¨at\\nzu Berlin\\narXiv:2506.11020v1  [cs.SE]  14 May 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 1}, page_content='Declaration\\nThe author declares that he / she has written the thesis at hand independently, without\\noutside help and without the use of any other but the listed sources. Thoughts taken\\ndirectly or indirectly from external sources (including electronic sources) are marked\\naccordingly without exception. Sources used verbatim and contentual were quoted ac-\\ncording to the recognised rules for scientific working. This thesis has not been submitted\\nin the same or similar form, not even partially, within the scope of a different examina-\\ntion.\\nThus far it also has not been publicised yet.\\nI herewith agree that the thesis will be examined for plagiarism with the help of a\\nplagiarism-detection service.\\nPlace, Date:\\nSignature:\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 2}, page_content='Acknowledgments\\nCompleting this thesis represents an important professional and personal milestone for\\nme, and I am deeply grateful to those who supported me throughout this process.\\nI would first like to express my gratitude to Prof. Dr. Leen Lambers, who allowed me\\nto be part of her research team. She not only recommended the initial inspiration for\\nthis research but also actively provided me with guidance and valuable feedback from\\nday one. I would also like to extend my thanks to Dr. Kate Revoredo who accepted the\\ninvitation to collaborate on this research and brought a new perspective to it. Both are\\noutstanding scientists and serve as an inspiration to me as I begin my journey in the\\ntechnology domain.\\nMy sincere appreciation goes to Dr. S´ebastien Mosser for his contribution, sharing\\nhis valuable experience, providing crucial data, and offering insightful feedback that\\ngreatly contributed to this research.\\nI am also grateful to the Artificial Intelligence\\nstudy program at BTU, especially my mentor, Prof. Dr. Douglas Cunningham, for his\\nsupport and guidance throughout my academic journey.\\nTo my friends around the world, thank you for your encouragement during both the\\nhighs and the lows of this process. Finally, I am deeply grateful to my family, who\\nprioritized education throughout my life and encouraged me to pursue my goals with\\ndetermination.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 3}, page_content='Abstract\\nRequirements engineering is a fundamental component in software engineering since it\\ndefines the software from the user’s perspective. In this context, user stories play a crucial\\nrole since they are simple, natural language descriptions of software requirements, and\\nthey are widely utilized in the industry, along with the agile development methodology.\\nUser stories, while valuable for capturing individual functionalities, offer a limited per-\\nspective of the overall system, hindering maintainability and comprehension. To address\\nthese challenges, extracting structured information and modeling user stories is crucial.\\nKnowledge graphs offer a promising approach by providing a visual and structured rep-\\nresentation of user stories, facilitating data storage and analysis, and reducing manual\\neffort retrieval, leading to a more coherent and manageable system.\\nSeveral methodologies to model stories employ Natural Language Processing (NLP)\\ntechniques, presenting limited precision, complex implementation, and difficulties to in-\\nterpret a sentence [47]. Recent research has explored using large language models, such\\nas ChatGPT-3.5 [1], to extract knowledge graph components (nodes and relationships)\\nfrom user stories. However, the solution relies on specific models and lacks comprehensive\\ndata processing pipelines for constructing knowledge graphs.\\nLangChain, a model-agnostic framework, is a promising tool that empowers the devel-\\nopment of applications centered around large language models. Its adaptability extends\\nto knowledge graph construction, facilitating the extraction of structured information\\nfrom textual data and seamless integration with graph databases.\\nThis thesis introduces a novel methodology for the automated generation of knowledge\\ngraphs from user stories by leveraging the advanced capabilities of Large Language Mod-\\nels (LLMs). Utilizing the LangChain framework as a basis, the UserStoryGraphTransformer\\n(USGT) module was developed to extract nodes and relationships from user stories using\\nan LLM to construct accurate knowledge graphs.This innovative technique was imple-\\nmented in a script to fully automate the knowledge graph extraction process. Addition-\\nally, the evaluation was automated through a dedicated evaluation script, utilizing an\\nannotated dataset for assessment. By enhancing the visualization and understanding of\\nuser requirements and domain concepts, this method fosters better alignment between\\nsoftware functionalities and user expectations, ultimately contributing to more effective\\nand user-centric software development processes.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 4}, page_content='Contents\\n1\\nIntroduction\\n8\\n1.1\\nMotivation\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n8\\n1.2\\nResearch Gap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n9\\n1.3\\nResearch Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n9\\n1.4\\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n10\\n1.5\\nResearch Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n10\\n2\\nBackground\\n11\\n2.1\\nRequirements engineering . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n11\\n2.1.1\\nUser Stories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n12\\n2.2\\nKnowledge Graph\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n2.2.1\\nOntology\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n2.2.2\\nNeo4j Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n17\\n2.3\\nNatural Language Processing . . . . . . . . . . . . . . . . . . . . . . . . .\\n17\\n2.4\\nLarge Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n2.5\\nLangchain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n20\\n2.5.1\\nLLMGraphTransformer\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n22\\n2.5.2\\nNeo4j integration . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n23\\n2.6\\nDataset Description\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n23\\n3\\nPrior Research as a Comparative Foundation\\n26\\n3.1\\nResearch Goal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n26\\n3.2\\nBackground . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n26\\n3.3\\nPrompt Engineering and API Usage\\n. . . . . . . . . . . . . . . . . . . . .\\n28\\n3.4\\nExperimental Setup\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n29\\n3.5\\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n30\\n4\\nDesign and Implementation\\n32\\n4.1\\nPreliminary Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32\\n4.2\\nCustom module: the UserStoryGraphTransformer . . . . . . . . . . . . . .\\n34\\n4.2.1\\nAligning the LLM Connector and the Graph Transformer . . . . .\\n35\\n4.2.2\\nPrompt Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n36\\n4.3\\nDesign of the Automated Knowledge Graph Extraction\\n. . . . . . . . . .\\n42\\n4.4\\nUserStoryGraphTransformer Implementation\\n. . . . . . . . . . . . . . . .\\n47\\n5\\nEvaluation\\n51\\n5.1\\nEvaluation Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n51\\n5.2\\nExperimental Setup\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 5}, page_content='5.3\\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n54\\n5.4\\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n59\\n6\\nState of the Art\\n63\\n6.1\\nModeling User Stories using NLP . . . . . . . . . . . . . . . . . . . . . . .\\n63\\n6.1.1\\nOntologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n64\\n6.2\\nLarge Language Models and Knowledge Graphs . . . . . . . . . . . . . . .\\n64\\n6.3\\nLarge Language Models and Requirements Engineering . . . . . . . . . . .\\n65\\n6.4\\nLangChain Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n66\\n7\\nConclusion\\n67\\n7.1\\nSummary\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n67\\n7.2\\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n67\\n7.3\\nLimitations and Threats to Validity\\n. . . . . . . . . . . . . . . . . . . . .\\n68\\n7.4\\nFuture Avenues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\nBibliography\\n71\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 6}, page_content='Acronyms\\nAI\\nArtificial Intelligence\\nCRF\\nConditional Random Fields\\nFN\\nFalse Negative\\nFP\\nFalse Positive\\nIE\\nInformation Extraction\\nKG\\nKnowledge Graph\\nLCEL LangChain Expression Language\\nLLM\\nLarge Language Model\\nLSTM Long Short-Term Memory\\nMC\\nMultiple-Classification\\nNER\\nNamed Entity Recognition\\nNLP\\nNatural Language Processing\\nPOS\\nPart-of-Speech\\nQA\\nQuestion Answering\\nRAG\\nRetrieval-Augmented Generation\\nRE\\nRequirements Engineering\\nRNN\\nRecurrent Neural Network\\nTP\\nTrue Positive\\nTS\\nToken-Similarity\\nUSGT UserStoryGraphTransformer\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 7}, page_content='1 Introduction\\n1.1 Motivation\\nTechnology became a constant across all facets of society, influencing how companies op-\\nerate, how education is delivered, and how public sectors function. In essence, technology\\npermeates the various aspects of daily living, including economic growth. Extending the\\ntheory proposed by Clayton M. Christensen [12], which suggests that customers ’hire’\\nproducts or services to accomplish specific tasks, it becomes integral to fully comprehend\\nand address user expectations. This understanding is essential to delivering products\\nthat provide real value to users.\\nCentral to the success of capturing user needs is the discipline of Requirements Engi-\\nneering. Requirements Engineering (RE) [52] is a fundamental domain which precedes\\nthe software development, that focuses on defining and managing the functionalities of\\nsoftware from the user’s perspective. A study by Robert Glass [27] concluded that most\\nIT system errors occur because of wrong or missing requirements. Conversely, through\\neffective RE practices, teams can ensure that the software not only meets technical\\nspecifications but also aligns with the needs and expectations of its users.\\nCommunicating requirements is a complex challenge. User stories [13], semi-structured\\nnatural language texts, are a popular tool for capturing requirements, by simplifying\\ncomplex requirements into concise, understandable segments that capture both the func-\\ntionality desired by the user and the value it provides. However, in a complex application,\\nuser stories can be hard to manage, and might lead to development delays [35].\\nWhile user stories are a valuable tool for capturing requirements, modeling them using\\na Knowledge Graph (KG) offers significant advantages [1] [35] [32]. A KG provides a\\nstructured representation of the information contained in user stories, facilitating the\\nidentification of relationships between them, uncovering potential inconsistencies or re-\\ndundancies, and enabling a deeper requirements analysis. In this context, the product\\nbacklog—a collection of N user stories—is represented as a unified knowledge graph,\\nwhere each user story is modeled as a set of nodes, and their relationships are captured\\nas edges. In essence, a KG promotes the transition from a collection of user stories,\\nalso known as product backlog, to a comprehensive understanding of user requirements.\\nTherefore, this thesis proposes the creation of a knowledge graph to model user stories,\\naiming to enhance the clarity, coherence, and utility of the captured requirements.\\nIn the age of Artificial Intelligence (AI), Large Language Models (LLMs) [8] and\\nKnowledge Graphs (KGs) have become fundamental technologies, driving significant ad-\\nvancements in how machines understand, reason, and process natural language. LLMs\\nhave proven to have excellent ability to assist in a wide array of tasks, such as generat-\\ning human-like text, and retrieving information to answer questions [31]. Among these\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 8}, page_content='innovations, LangChain [9] has emerged as a robust framework for developing LLM-\\npowered applications.\\nIt provides an integrated solution for constructing knowledge\\ngraphs, greatly simplifying the knowledge extraction process, by reducing the complex-\\nity involved in interacting with LLMs and offering modules that completely automate the\\ncreation of Knowledge Graphs. This thesis aims to explore the suitability of LangChain\\nas a tool to streamline coding, enhance automation of extracting Knowledge Graphs,\\nspecifically for the use case of user stories.\\n1.2 Research Gap\\nExisting researches [47][35] have demonstrated the feasibility of converting user sto-\\nries into Knowledge Graphs using Natural Language Processing techniques. However,\\nthese approaches often suffer from limitations in precision, and require a high degree of\\ntechnical expertise to implement. Furthermore, the automation process is not entirely\\nautonomous and still requires significant human intervention, particularly for tasks such\\nas validating extracted components and resolving ambiguities. This highlights the need\\nfor a method that achieves a balance between precision and implementation complexity.\\nA recent study explored the potential of Large Language Models (LLMs) for this\\ntask [1]. While promising, this research focused solely on extracting domain knowledge\\nin the form of graph components (nodes and relationships) from user stories and did\\nnot implement a solution to generate a Knowledge Graph from the user’s perspective-\\none that would enable graph visualization and querying. Additionally, it relies on a\\nspecific Large Language Model, which introduces limitations in terms of model stability,\\nflexibility, and potential bias.\\nThis highlights a significant gap in the understanding of how LLMs can be effectively\\nemployed to create a robust and automated solution for extracting Knowledge Graphs\\nfrom user stories within the broader field of requirements engineering. Addressing this\\ngap can enhance both the accuracy and efficiency of knowledge representation from nat-\\nural language requirements. Accuracy ensures the extracted Knowledge Graph faithfully\\nrepresents user stories, while efficiency minimizes the complexity of implementation and\\ncomputational overhead. This research seeks to balance these aspects, offering a flexi-\\nble methodology compatible with different language models, ultimately contributing to\\nimproved software development practices.\\n1.3 Research Questions\\nUnderstanding user requirements is critical for the development of successful software\\nsolutions. This thesis focuses on the potential of employing Large Language Models\\n(LLMs) through the advanced framework LangChain to enhance the process of extracting\\nKnowledge Graph from user stories.\\nSpecifically, this research seeks to address the\\nfollowing questions:\\n1. How does the accuracy of nodes and relationships extraction of the proposed so-\\nlution compare to the existing Large Language Model-based method [1]?\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 9}, page_content='2. How can a knowledge graph be automatically generated from user stories using\\nlarge language models without being dependent on a specific provider?\\n3. What strategies can be employed to build a fully automated solution to extract\\nknowledge graphs from user stories?\\n1.4 Contributions\\nThis thesis makes significant contributions to the fields of Requirements Engineering and\\nLarge Language Models. The key contributions are summarized as follows:\\n1. Development of a fully automated approach for extracting knowledge graphs from\\nuser stories using LLMs.\\nThis approach simplifies the process of transforming\\nnatural language requirements into structured, actionable knowledge, providing a\\nfoundation for better understanding and managing software requirements in re-\\nquirements engineering.\\n2. Development a reusable evaluation script for assessing the accuracy of the knowl-\\nedge graph extraction process. This script leverages standard metrics (e.g., pre-\\ncision, recall, F-measure) and relies on a pre-defined ground-truth for objective\\nevaluation.\\n3. Comparative evaluation of the proposed solution, including its accuracy against\\nexisting methods in terms of precision, recall, and F-measure in knowledge graph\\nextraction from user stories.\\n1.5 Research Outline\\nChapter 2 introduces the theoretical concepts that form the foundation of this research.\\nChapter 3 examines the prior work that this thesis builds upon, serving as a benchmark\\nfor results while identifying the limitations that this research aims to address. Chapter 4\\nfocuses on the design of the USGT module and implementation of a fully automated\\nsolution for generating knowledge graphs from user stories. Chapter 5 outlines the eval-\\nuation process, detailing the metrics used, experimental setup, results, and a discussion\\nof the findings. Chapter 6 reviews the current state of research in this field and high-\\nlights potential research gaps. Finally, Chapter 7 concludes the thesis by summarizing\\nthe contributions, addressing the limitations, and proposing directions for future work.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 10}, page_content='2 Background\\nThis chapter presents the concepts and theoretical foundation for understanding the\\nextraction of knowledge graphs from user stories using LangChain. This research area\\nsits at the intersection of several key concepts: LLMs, KGs, and user stories. Establishing\\na solid grasp of these concepts is crucial for effectively framing our research question,\\nmethodology, and anticipated contributions.\\nSection 2.1 introduces the field of requirements engineering and highlights its critical\\nrole in the software development process. Subsection 2.1.1 focuses on the significance of\\nuser stories within this domain.\\nSection 2.2 explores knowledge graphs, emphasizing their potential to enhance require-\\nments engineering. Subsection 2.2.1 discusses the ontology reintroduced in this study,\\nfollowed by Subsection 2.2.2, which introduces Neo4j, a graph database used for storing\\nand managing graph structures.\\nNext, Section 2.3 delves into NLP, presenting common techniques employed to extract\\nstructured information from user stories. Section 2.4 highlights LLMs as a transformative\\ntechnology in this context.\\nSection 2.5 introduces LangChain, detailing its features and how they are leveraged in\\nthis study. Lastly, Section 2.6 describes the dataset utilized in this research for evaluation\\npurposes.\\n2.1 Requirements engineering\\nAs technology evolves, software development emerges as a critical tool for companies and\\nacademia to tackle increasingly complex challenges. From this perspective comes the RE\\nthat states that well-defined requirements are essential to any system. These require-\\nments capture and communicate characteristics, capabilities, and quality expectations,\\nensuring the system fulfills its intended purpose and meets user needs [52].\\nA formal definition of a requirement was proposed by the IEEE Standard Glossary of\\nSoftware Engineering Terminology [14]:\\n1. A condition or capability needed by a user to solve a problem or achieve an objective.\\n2. A condition or capability that must be met or possessed by a system or a system\\ncomponent to satisfy a contract, standard, specification, or other formally imposed\\ndocument.\\n3. A documented representation of a condition or capability as in 1 or 2.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 11}, page_content='The RE life cycle [52] is a systematic process that guides the development and man-\\nagement of system requirements from initial conception to final validation. This cycle\\nis essential for ensuring that the system meets all intended goals and user expectations.\\nThe RE life cycle involves the phases [18]:\\n• Elicitation: Identifies stakeholders and captures their needs. This includes col-\\nlecting business and or user requirements, known as functional requirements, and\\nquality, known as non-functional requirements.\\n• Analysis: Captured requirements are then analyzed in detail during this phase.\\nThis involves decomposing high-level requirements into more specific and action-\\nable ones. Additionally, feasibility is assessed, priorities are negotiated, inconsis-\\ntencies and conflicts are identified, and unclear, incomplete, ambiguous, or contra-\\ndictory requirements are resolved.\\n• Specification: Following analysis, the well-defined requirements are documented\\nin a clear, consistent, and accessible manner.\\nThis specification serves as the\\nbaseline for the development process and ensures everyone involved has a shared\\nunderstanding of the system’s functionalities and constraints.\\n• Verification and validation: Focuses on ensuring that the documented require-\\nments accurately reflect the needs identified during elicitation and that they meet\\nthe project’s objectives. Various tests and evaluation methods are employed during\\nverification and validation to confirm that the requirements are complete, consis-\\ntent, achievable, and traceable throughout the development life cycle.\\nClear requirements act as a direction for successful software development. They guide\\neach stage of the process, from system design and development to testing, implementa-\\ntion, and ongoing operation. Misunderstandings at this initial stage can lead to wasted\\neffort and rework later on. Investing time in thorough planning through well-defined\\nrequirements can significantly save time and resources, in the long run, [52][46].\\n2.1.1 User Stories\\nAgile development, a software development paradigm emphasizing flexibility, collabora-\\ntion, and customer-centricity, emerged from the 2001 Agile Manifesto [4]. It promotes\\niterative development, breaking down projects into manageable units delivered in short\\ncycles. This iterative approach is facilitated by user stories, which capture functionalities\\nfrom the user’s perspective and break them down into manageable development tasks.\\nUser stories are a specific form of user requirements [52]. They are statements that\\ncapture desired functionalities, which deliver value to system users or acquirers in a\\nsimple natural language. In addition, they can be expressed as a tuple containing the\\nfollowing information [50]:\\n• User role: Also known as persona, it is the user role that has the perspective in\\nthe user story.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 12}, page_content='• Actions: They are specific activities that the user role wants to perform within the\\nsystem.\\n• Entities: They are objects that the actions are performed on or with.\\n• Benefit: It is an optional attribute, that represents the value or advantage that\\nthe persona gains by performing the actions over entities.\\nA user story typically follows the Connextra format [13]:\\nAs a <PERSONA>, I want <ACTIONS over ENTITIES> so that\\n<BENEFIT>.\\nScrum [50], a popular Agile framework, heavily relies on user stories. Scrum teams\\nmanage a prioritized list of work items called a product backlog. This backlog contains\\na set of user stories, along with other potential functionalities, bug fixes, or tasks. This\\nemphasis on user stories keeps the focus on delivering value throughout the iterative\\ndevelopment process.\\nTo assess the quality of user stories, the INVEST acronym is commonly employed [13]:\\n• I: Stands for Independent, meaning that each user story should be self-contained\\nand free of dependencies on other stories, allowing for flexible prioritization and\\ndevelopment.\\n• N: Represents Negotiable, emphasizing that user stories are not fixed contracts but\\nrather starting points for discussions between stakeholders and the development\\nteam.\\n• V: Refers to Valuable, ensuring that each story delivers clear value to the customer\\nor end user.\\n• E: Stands for Estimable, meaning the development team should be able to estimate\\nthe effort required to implement the story.\\n• S: Stands for Small, ensuring stories are granular enough to be completed within\\na single iteration or sprint.\\n• T: Represents Testable, highlighting the need for clear acceptance criteria that\\nallow for proper validation of the story’s implementation\\nBy adhering to these principles, user stories can be written in a way that enhances\\nclarity, collaboration, and deliverability within Agile projects.\\nThis thesis research contribution focuses on user stories, thereby contributing to mul-\\ntiple phases of the RE life cycle. In the RE life cycle, user stories are fundamentally tied\\nto the specification phase. They effectively capture stakeholders’ needs and articulate\\nwhat users require from the system in a simple and understandable format. However,\\ntheir utility extends beyond specification. During the elicitation phase, user stories can\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 13}, page_content='help to identify stakeholders involved. They also contribute to the analysis phase, where\\nthey play a crucial role in prioritizing requirements and identifying dependencies among\\ndifferent system components. Overall, user stories are a versatile and powerful tool in\\nthe RE process, providing a clear, semi-structured, and concise method for documenting\\nand managing requirements throughout the development life cycle.\\n2.2 Knowledge Graph\\nThe concept of a Knowledge Graph is not entirely new. Semantic networks, which are\\npredecessors to KGs, emerged in the 1960s as a way to connect pieces of knowledge, laying\\nthe foundation for knowledge representation and knowledge-based systems. However, the\\nmodern interpretation of KGs, characterized by their scale and focus, can be considered\\na relatively new phenomenon [24].\\nAs the name suggests, the basis of a KG is a graph. A Graph [57], denoted by G, is\\na fundamental structure used to model relationships between objects. It consists of two\\nsets: V, the set of vertices, and E, the set of edges that connect these vertices. Also,\\neach edge connects exactly two distinct vertices from V. The next part, Knowledge [24],\\nis generated by interpreting a graph, and relating its elements to real-world objects and\\nactions.\\nWhen both definitions are combined, it is possible to derive that a knowledge graph\\npresents its data and schema in a graph format, in which the vertices (V) represent\\nentities such as people, places, or concepts, and edges (E) represent the relationships\\nbetween these entities, such as ”is located in” or ”invented by”.\\nKnowledge Graphs offer a powerful approach to data management. Their core strength\\nlies in flexible data modeling. Unlike traditional methods, they can easily accommodate\\ndata from diverse sources by creating connections between them. This simplifies data\\nintegration, making them highly scalable and well-suited for handling information from\\nvarious sources.\\nParticularly for user stories, a KG provides an effective solution for modeling by\\nsystematically consolidating and organizing information. Diverging from the standard\\napproach that relies on interpreting problem descriptions written in natural language,\\nwhich can be ambiguous and challenging to manage, KGs represent user stories and\\ntheir attributes as structured data. This structured representation not only clarifies the\\nindividual elements of each user story but also highlights the dependencies and rela-\\ntionships between them in a product backlog. By visualizing these connections within\\na graph framework, KGs facilitate a clearer understanding of how user stories interact\\nand impact one another, thereby improving traceability and supporting more efficient\\nmanagement of requirements, and, consequently, of the development process [24].\\n2.2.1 Ontology\\nA KG relies on a well-defined underlying ontology to represent its domain [24]. Essen-\\ntially, an ontology [53] is a formally agreed-upon model of a system’s structure that\\nexplicitly defines the relevant entities and their relationships.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 14}, page_content='Figure 2.1: Conceptual model of a backlog [1].\\nThis thesis proposes a knowledge graph design for representing user stories, also known\\nas backlog items, within the Scrum framework [50]. The design adopts the ontology\\ndeveloped by Arulmohan et al. [1] (see Figure 2.1), provides a structured approach to\\ndefine user stories within a KG.\\nIn the metamodel, each product consists of one or more backlogs, where each backlog\\ncorresponds to a set of N user stories. Each user story is associated with a persona, and\\nat least one entity, and is composed of at least one action. Additionally, a user story\\ncan optionally include one benefit. A persona is associated with the primary level action\\n(also understood as the main action) through a triggers relationship, and each action is\\nassociated with an entity through a targets relationship. From the backlog perspective, it\\nencompasses a set of user stories along with the personas, entities, actions, and possibly\\nbenefit associated with those stories.\\nTo illustrate, consider the scenario in which the product is a web application. The user\\nstories are organized in one backlog referring to the system administration capabilities\\nof this web application, and they reflect the following user requirements:\\nUS1. As a user, I want to access my personal data, so that I can review it.\\nUS2. As an administrator, I want to be able to see the user’s email, so that\\nI can assign a new role.\\nUS3. As a user, I want to see my own role.\\nThese user stories can be modeled according to the ontology described on the meta-\\nmodel in Figure 2.1, resulting in the knowledge graph seen in Figure 2.2. The nodes are\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 15}, page_content='Figure 2.2: Example of a modeled product backlog of a web application following the\\nontology described in Figure 2.1.\\nclassified into 5 types: story, persona, action, entity, and benefit. The relationships from\\nthe user story to the remaining node types guarantee traceability, while the triggers,\\nand targets relationships provide insights into the focus of requirements and can help\\nprioritize their development.\\nIn the example provided, it becomes easier to identify relationships and dependencies\\nthat may not be immediately obvious in textual form, such as the connection between\\nthe persona in US1 and US3, which can help prioritize development tasks, improve role-\\nspecific features, and ensure consistency across stories. Changes to the functionalities\\nassociated with a particular persona can be analyzed more effectively. For instance, if a\\nfeature related to the user persona needs modification, the graph immediately highlights\\nwhich user stories and corresponding actions might be impacted.\\nFurthermore, the\\ngraph improves communication between stakeholders, as it becomes easier to track which\\npersonas interact with specific features and identify potential gaps or overlaps in the\\nbacklog.\\nEven with a simple example, it is already possible to see that adopting this ontology\\ncan ensure a consistent and well-defined structure for representing user stories within\\nKGs. This structured representation facilitates reasoning, analysis, and automation of\\ntasks related to user stories.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 16}, page_content='2.2.2 Neo4j Database\\nNeo4j [43] is a popular graph database product specifically designed to store and manage\\ninterconnected data. Opposed to some relational databases that try to mimic graph\\nstructures, Neo4j is built from the ground up to handle connected data efficiently. This\\nnative approach allows for faster performance and more natural graph queries.\\nNeo4j utilizes Cypher, a declarative query language specifically designed for graph\\ndata. Cypher allows you to write intuitive queries that reflect the relationships within\\nyour data, simplifying analysis compared to using SQL in relational databases.\\nIn this thesis, Neo4j was selected as the tool for storing and visualizing the resulting\\nKG due to its robust capabilities in modeling entities and relationships. Neo4j excels in\\nrepresenting complex connections within data, making it ideal for capturing the struc-\\nture of user stories and their dependencies. Additionally, its architecture supports easy\\nscalability, which is important considering the chosen dataset. Neo4j also seamlessly inte-\\ngrates with the existing technical setup, ensuring smooth implementation and operation\\nwithin the project’s framework.\\n2.3 Natural Language Processing\\nNatural Language Processing (NLP), a subfield of AI, focuses on facilitating the in-\\nteraction between computers and human language [40].\\nAmong its primary tasks is\\nInformation Extraction (IE), which involves the automatic conversion of unstructured\\ntext into structured data [7]. This structured information can then be leveraged for\\ndiverse applications, including populating knowledge graphs.\\nNLP techniques present considerable advantages for enhancing the quality of user\\nstories. Through the application of NLP, it is possible to effectively parse, extract, and\\nanalyze data from user stories. NLP has seen widespread use in the software engineering\\ndomain, addressing a variety of tasks. These techniques include [40]:\\n• Tokenization: This fundamental step breaks down a text into individual units,\\ntypically words or sentences.\\n• Part-of-Speech (POS) tagging: This process assigns a grammatical category (e.g.,\\nnoun, verb, adjective) to each word (or tokens) in a text.\\n• Dependency Parsing: This technique goes beyond POS tagging by analyzing the\\ngrammatical relationships between words in a sentence. It identifies how words\\ndepend on each other syntactically.\\n• Named Entity Recognition (NER): This subtask involves identifying and catego-\\nrizing named entities in a text into predefined categories (e.g. names of people,\\norganizations, or locations).\\nThese NLP techniques can be applied to user stories to extract key information, as well\\nas help improve user story clarity by identifying ambiguous language or inconsistencies.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 17}, page_content='For instance, POS tagging can help identify user roles (nouns) and actions (verbs), while\\nNER can distinguish specific functionalities mentioned in the story. This focus on clarity\\nensures a better understanding between stakeholders and development teams.\\nThese diverse applications highlight the potential of NLP to transform user stories\\nfrom unstructured to structured information. Recent advancements [47] show particular\\npromise in automating the modeling of information and extracting key abstractions.\\n2.4 Large Language Models\\nWithin the field of NLP and AI, LLMs stand out as powerful tools. These models are\\nessentially statistical methods trained on massive amounts of text data. Through this\\ntraining, LLMs learn the underlying statistical patterns and relationships between words\\nwithin a language. This capability allows them to predict the likelihood of specific word\\nsequences, enabling a wide range of NLP applications [37].\\nLLMs [8] are a type of advanced neural network architecture specifically designed for\\nprocessing and generating human language. They are characterized by huge amounts\\nof parameters, which allows them to capture complex relationships and patterns within\\nlanguage. In addition, they have the ability to learn and adapt to various language tasks.\\nTable 2.1 presents essential terminology to comprehend LLMs, as adapted from [23].\\nWhile concepts like role are specific to LLM-based chat applications, other terms such\\nas temperature, prompt, and prompt engineering are broadly applicable to a wide range\\nof LLM applications, including but not limited to chat-based systems.\\nThe fundamental component of LLMs is the transformer architecture [8]. This archi-\\ntecture was designed to address the limitations of previous sequence-to-sequence models\\nin NLP, which typically relied on Recurrent Neural Networks (RNNs) and Long Short-\\nTerm Memory (LSTM) networks. These earlier models processed data sequentially and\\nstruggled with long-range dependencies and parallelization. In contrast, the transformer\\nuses a novel mechanism called self-attention to handle sequences, enabling more efficient\\nprocessing and better performance on a wide range of tasks [54].\\nA notable capability of LLMs is in-context learning [37]. This refers to the model’s\\nability to enhance its understanding and response to prompts by incorporating additional\\ncontextual information directly into the prompt itself.\\nThis technique allows LLMs\\nto adapt to new tasks with minimal additional training, simply by providing relevant\\nexamples or supplementary data within the prompt [8]. This makes LLMs particularly\\nversatile and useful for applications where the model needs to adapt quickly to new and\\nvaried tasks.\\nPrompt engineering is a common used technique for interacting effectively with LLMs,\\nparticularly in applications like chatbots and virtual assistants [8][55].This approach\\ninvolves strategically crafting input prompts that guide the LLM towards generating\\naccurate and relevant responses. By effectively translating user intent into a format that\\nLLMs can process, prompt engineering optimizes interactions and significantly enhances\\nthe relevance and quality of the model’s outputs.\\nIn the context of RE, prompts can be used as natural language instructions given to an\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 18}, page_content='Concept\\nDescription\\nTemperature\\nTemperature controls the stochasticity of the model’s output.\\nIt\\nvaries from zero to one, lower values favor statistically frequent and\\npredictable responses.\\nRole\\nThe role describes who is interacting with an LLM. The standard\\nroles are ”user” (gives input), ”assistant” (gives output), ”system”\\n(sets context), and ”tool” (access external APIs or tools).\\nHallucination\\nIt is the phenomenon when generated responses are factually incor-\\nrect and do not correspond to real-life information.\\nTokens\\nTokens are the smallest units of information in the form of text the\\nLLM can handle. It is an enumeration and can be a word, subword,\\nand even punctuation.\\nPrompt\\nA prompt is a set of instructions or an initial piece of information\\nthat guides the LLM toward a desired outcome.\\nPrompt\\nengi-\\nneering\\nIt is the process of designing and improving prompts to achieve\\nspecific outputs.\\nInstruction\\ntuning\\nIt is the process of fine-tuning the LLM by training it on a dataset\\nof instructions to achieve desired outputs.\\nIn-context\\nlearning\\nEnables the LLM to learn new skills by incorporating examples of\\nthe desired output within the prompt.\\nZero-Shot\\nprompt\\nIt is a technique based on Zero-Shot Learning, that uses natural lan-\\nguage text to obtain a desired output without providing examples.\\nFew-Shot\\nprompt\\nIt builds on the zero-shot prompting, but it actually provides exam-\\nples of the expected output.\\nTable 2.1: Fundamental terms for understanding LLMs.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 19}, page_content='LLM to perform specific tasks, such as identifying whether a certain system functionality\\nis properly elicited from user stories [56], which can improve efficiency of tasks and reduce\\nhuman effort.\\nLarge Language Models have demonstrated exceptional proficiency in processing and\\nunderstanding natural language, making them invaluable tools across a wide range of\\ntasks.\\nTheir usability, even for non-technical users, underscores their potential as a\\ngroundbreaking technology suitable for diverse applications. This thesis aims to delve\\ninto the capabilities of LLMs, specifically focusing on their ability to automatically gener-\\nate knowledge graphs from user stories. By utilizing the natural language understanding\\nand generation strengths of LLMs, this research evaluates how effectively these mod-\\nels can extract and organize requirements from user stories into structured knowledge\\nrepresentations.\\n2.5 Langchain\\nWhile LLMs hold significant potential for various applications, their implementation in\\nreal-world scenarios can be complex and challenging, due to factors such as choosing the\\nright language model, designing effective prompts, and managing data processing work-\\nflows. LangChain [9], an open-source Python framework, addresses these complexities\\nby simplifying the development of LLM-powered applications, by providing clear con-\\nnectivity to various third-party LLMs (e.g., OpenAI, Google Gemini, Ollama), and by\\noffering a declarative syntax called LangChain Expression Language (LCEL) to manage\\ncomplex LLM interactions, and therefore becoming a strong candidate to address the\\nlimitations of previous researches.\\nLCEL [9] is the declarative syntax provided by LangChain that simplifies the imple-\\nmentation for users and handles the underlying execution details. It offers several key\\nbenefits: first, LCEL allows for rapid prototyping with facilitated transition to produc-\\ntion due to its flexibility and lack of code changes needed. Second, it optimizes data\\nprocessing through features like streaming support, asynchronous execution, and parallel\\nprocessing, leading to faster and more efficient workflows. Finally, LCEL enhances chain\\nreliability with retries, fallbacks, and access to intermediate results, while also ensuring\\ndata integrity through automatic input and output schema generation. These combined\\nfeatures make LCEL a powerful tool for building robust and scalable LLM applications\\nwithin the LangChain framework.\\nThe idea behind this framework is to build custom chains, and this can be achieved\\nby the Runnable protocol [9]. Designed to execute a specific task or series of operations.\\nIt acts as a modular component that can be composed with others to build custom\\nworkflows, where each Runnable performs a defined action, such as processing input data\\nor interacting with external systems. One key operation that a Runnable supports is the\\ninvoke method, which allows it to process a single input and return the corresponding\\nresult. This method is central to executing tasks within the chain, enabling the flow\\nof data and the execution of operations in a sequential manner. The components in\\nTable 2.2 are examples of Runnables.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 20}, page_content='Component\\nDescription\\nInput Type\\nOutput Type\\nPrompt\\nTem-\\nplate\\nTransform user in-\\nputs and parameters\\ninto instructions for\\na language model\\nDictionary\\nPrompt Value\\nChatModel\\nLanguage\\nModels\\nthat support distinct\\nroles\\nin\\nconver-\\nsations\\n(e.g.\\nAI,\\nhuman, system)\\nSingle string, list of\\nchat messages or a\\nPrompt Value\\nChatMessage\\nOutputParser\\nProcess the output\\nof a ChatModel into\\na desired format\\nThe\\noutput\\nof\\na\\nChatModel\\nDepends\\non\\nthe\\nparser (e.g., String,\\nJSON)\\nTable 2.2: LangChain Components.\\n1\\nfrom\\nlangchain_openai\\nimport\\nChatOpenAI\\n2\\nfrom\\nlangchain_core.output_parsers\\nimport\\nStrOutputParser\\n3\\nfrom\\nlangchain_core.prompts\\nimport\\nChatPromptTemplate\\n4\\n5\\nmodel = ChatOpenAI(model=\"gpt -3.5- turbo -0125\")\\n6\\nprompt = ChatPromptTemplate .from_template (\"what is the\\ncapital of\\n{country }?\")\\n7\\nchain = prompt | model | StrOutputParser ()\\n8\\n9\\nreply = chain.invoke ({\"country\": \"Brazil\"})\\nListing 2.1: Example of LangChain application.\\nLangChain itself doesn’t host any language models but relies on integration with third-\\nparty options. Additionally, the framework offers standardized parameters applicable to\\nany LLM, such as model name, temperature, and maximum tokens generated.\\nPrompt templates are another key feature, guiding a model’s response. They take a\\ndictionary as input, with each key representing a variable in the prompt template to fill\\nin. By providing a structured format for inputs, prompt templates help standardize and\\nrefine interactions with these models.\\nAn example in Listing\\n2.1 demonstrates how LangChain simplifies the creation of\\nstandardized interactions with LLMs using minimal code. This example chain consists\\nof three key components: a ChatModel to generate responses, a Prompt template to\\nstandardize the input format, and a String Output Parser to process the model’s output,\\nremoving all the metadata that comes with the output, and keeping only the model’s\\nreply. The chain relies on the pipe operator, a core feature of the LCEL syntax, to\\nconnect components sequentially and ensure the data flow remains clear and readable.\\nLangChain enhances the capabilities of language models by consistently integrat-\\ning them with external data sources and computational systems.\\nThis connectivity\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 21}, page_content='allows language models to act as reasoning engines while delegating tasks such as knowl-\\nedge retrieval and execution to other specialized systems. A prime example of this is\\nLangChain’s integration module for the Neo4j graph database. This module facilitates\\nquerying, updating, and even constructing knowledge graphs, aligning perfectly with the\\nobjectives of this research.\\nIn this thesis, LangChain was chosen as the development framework due to its fea-\\ntures that directly align with this research objective of exploring LLM-based knowledge\\ngraph generation from user stories. A key advantage of LangChain is its ability to fully\\nautomate the creation of knowledge graphs from user stories, from extracting nodes and\\nrelationships to processing the LLM’s output and storing it in a graph database. This\\npractical aspect significantly improves the usability of the approach, especially for less\\ntechnical users who may not have extensive experience with the technologies involved in\\nmodeling user stories. These users can rely on the assistance of LLMs to simplify this\\ntask through LangChain. Notably, LangChain offers pre-built modules for knowledge\\ngraph creation and facilitates seamless integration with graph databases, particularly\\nNeo4J. This integration is crucial for storing and analyzing the generated knowledge\\ngraphs.\\n2.5.1 LLMGraphTransformer\\nConverting unstructured text data into structured knowledge graphs offers a powerful\\napproach to gaining deeper insights and navigating complex relationships. The LLM-\\nGraphTransformer module [9] within LangChain plays a crucial role in this process. It\\nutilizes LLMs to analyze text documents and extract key entities (nodes) and the con-\\nnections between them (relationships). These extracted elements are then organized into\\na structured graph format, facilitating efficient analysis and exploration of the informa-\\ntion. It is important to remark that this module is part of the LangChain Experimental\\ncode, and, therefore, it may lack extensive testing, and is designed for research and\\nexperimental use only.\\nTo achieve its goal, it uses a predefined prompt specifically designed to guide the LLM\\ntoward extracting relevant entities and relationships suitable for constructing general-\\npurpose Knowledge Graphs (KG). In addition, it has an end-to-end data processing logic\\nthat handles the entire data process pipeline, transforming raw text into a KG document.\\nThe input of this module is a Document [9], which serves as the fundamental unit\\nof information. It comprises two key components: page content and metadata. The\\npage content is the core textual content that interacts with the language model, in the\\ncontext of this thesis, the page content specifically represents the user story input. The\\nmetadata is an optional field, which provides additional information about the unit of\\ninformation.\\nThe module interacts with the LLM using its predefined prompt through a chain and\\nreceives a response in string format. Then, it employs techniques to convert the Nodes\\nand Relationships received in raw string format into objects of classes Node and Rela-\\ntionship respectively, and lastly, the KG document is created. The end-to-end processing\\ncapability simplifies the knowledge graph construction process by eliminating the need\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 22}, page_content='1\\nfrom\\nlangchain_community .graphs\\nimport\\nNeo4jGraph\\n2\\nimport os\\n3\\n4\\nos.getenv(\"NEO4J_URI\")\\n5\\nos.getenv(\"NEO4J_USER\")\\n6\\nos.getenv(\" NEO4J_PASSWORD\")\\n7\\n8\\ngraph = Neo4jGraph ()\\nListing 2.2: Neo4j and LangChain integration\\nfor manual intervention in data transformation steps.\\nHowever, it is important to note that the effectiveness of LLMGraphTransformer de-\\npends heavily on the chosen LLM. Since LLMs are statistical models, the accuracy and\\nnuance of the extracted graph data can vary depending on their specific capabilities.\\nWhile LLMGraphTransformer is designed to convert text into knowledge graphs, it\\noften prioritizes high-level entities like names, places, and companies during extraction.\\nThis focus can lead to missing crucial details and entities specific to user stories, such as\\nactions, processes, or domain-specific concepts. These entities, even if not proper names\\nor locations, are vital for a comprehensive user story KG, and therefore this limitation\\nwill be addressed in the proposed solution design and implementation.\\n2.5.2 Neo4j integration\\nThe final step in building a knowledge graph is integrating it with a graph database\\nfor storage, with Neo4j being the chosen database in this study. LangChain simplifies\\nthis process through its Neo4jGraph module, which provides seamless integration with\\nNeo4j.\\nListing 2.2 illustrates a basic example of connecting to a Neo4j instance, using en-\\nvironment variables for secure authentication. Through the Neo4jGraph module, users\\ncan easily create a graph instance and populate a knowledge graph within Neo4j, by pro-\\nviding a specific data type, the Graph Document. This module significantly streamlines\\nthe interaction between the LangChain framework and the graph database, reducing\\ncomplexity and enhancing automation.\\nThe integration with Neo4j allows for not only the storage but also the further analysis\\nof the generated knowledge graph. By using a graph database, users can run queries\\nand perform analysis on the knowledge graph, enhancing its value and utility in the\\nrequirements engineering process. This is a core advantage of the proposed approach in\\ncomparison to previous researches.\\n2.6 Dataset Description\\nThe dataset used in this research, named Ace-design/qualified-user-stories-dataset, was\\ncurated by Sathurshan Arulmohan, S´ebastien Mosser, and Marie-Jean Meurs [49]. It\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 23}, page_content='builds upon the original dataset published by Dalpiaz et al. [16], which consists of 22\\nproduct backlogs containing 1679 user stories. The Ace-design dataset is an annotated\\nversion of the original, enriched with additional metadata and annotations for enhanced\\nanalysis and modeling purposes.\\nBacklog file name\\nContext\\ng02\\nReporting\\ng03\\nManagement application\\ng04\\nManagement application\\ng05\\nReporting\\ng08\\nDevelopment\\ng10\\nWeb\\ng11\\nWeb\\ng12\\nManagement application\\ng13\\nWeb\\ng14\\nContent management\\ng16\\nUniversity\\ng17\\nDevelopment\\ng18\\nContent management\\ng19\\nInternet of Things\\ng21\\nManagement application\\ng22\\nContent management\\ng23\\nContent management\\ng24\\nContent management\\ng25\\nContent management\\ng26\\nContent management\\ng27\\nContent management\\ng28\\nManagement application\\nTable 2.3: Overview of dataset content.\\nThe user stories were meticulously annotated by a systematic process that involved\\nusing Doccano [41], an open-source text annotation tool, to recognize and label relevant\\nentities (i.e., Persona, Action, Entity, Benefit). This process was followed by rigorous\\nquality assurance measures, including calibration on a subset of stories, regular meetings\\nto discuss annotations, and manual review of randomly selected stories [1].\\nEach backlog is structured in a distinct JSON file format, which file name and context\\ncan be seen in Table 2.3, where each file contains a list of dictionaries. Each dictionary\\nrepresents a user story along with its annotated components, as seen in Listing 2.3.\\nAs a result of this annotation process, the level of consistency between the different\\nevaluators achieved an impressive 94 % [1], which significantly endorsed the selection\\nof this dataset for use in this thesis. This high level of reliability assured the quality\\nand consistency of the annotated user stories, validating their suitability for the research\\nobjectives and ensuring robust and dependable results.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 24}, page_content='1\\n{\\n2\\n\"PID\": \"#G02#\",\\n3\\n\"Text\": \"#G02# As a Website user , I want to access\\npublished\\nFABS\\nfiles , so that I can see the new files as they come in.\",\\n4\\n\"Persona\": [\\n5\\n\"Website\\nuser\"\\n6\\n],\\n7\\n\"Action\": {\\n8\\n\"Primary\\nAction\": [\\n9\\n\"access\"\\n10\\n],\\n11\\n\"Secondary\\nAction\": [\\n12\\n\"see\"\\n13\\n]\\n14\\n},\\n15\\n\"Entity\": {\\n16\\n\"Primary\\nEntity\": [\\n17\\n\"published\\nFABS\\nfiles\"\\n18\\n],\\n19\\n\"Secondary\\nEntity\": [\\n20\\n\"new files\"\\n21\\n]\\n22\\n},\\n23\\n\"Benefit\": \"I can see the new files as they come in\",\\n24\\n\"Triggers\": [\\n25\\n[\\n26\\n\"Website\\nuser\",\\n27\\n\"access\"\\n28\\n]\\n29\\n],\\n30\\n\"Targets\": [\\n31\\n[\\n32\\n\"access\",\\n33\\n\"published\\nFABS\\nfiles\"\\n34\\n],\\n35\\n[\\n36\\n\"see\",\\n37\\n\"new files\"\\n38\\n]\\n39\\n],\\n40\\n\"Contains\": []\\n41\\n}\\nListing 2.3: Example of annotated user story in JSON format from g02 backlog.\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 25}, page_content='3 Prior Research as a Comparative\\nFoundation\\nThis section presents an overview of the research work by Sathursan Arulmohan, Marie-\\nJean Meurs, and S´ebastien Mosser, titled Extracting Domain Models from Textual Re-\\nquirements in the Era of Large Language Models [1] that forms the foundation of the\\napproach proposed in this thesis. Besides the outstanding contribution of their research\\nin annotating a comprehensive user stories dataset [49], their work is pioneering in em-\\nploying a distinct technique from traditional NLP methods to model user stories by\\napplying LLMs, specifically Chat-GPT [45] by OpenAI.\\n3.1 Research Goal\\nThis research addresses a key challenge in the context intersection between RE and Agile\\nmethodologies, which is to extract domain models from user stories. The idea is that\\nby extracting domain models, the ambiguity and conflicts of the requirements can be\\nminimized.\\nWhen performed manually, this process has a high chance of errors and takes a long\\ntime to complete. To address these challenges, the authors explore the potential of LLMs\\nand compare their capabilities to both a state-of-practice requirements extraction tool\\nand a dedicated NLP approach.\\nThis research represents an initial step toward the use of LLMs to automate require-\\nments engineering tasks. By examining the performance of these tools on a real-world\\ndataset, the paper lays the groundwork for future advancements in integrating artifi-\\ncial intelligence into the RE process, particularly in Agile environments where textual\\nrequirements dominate.\\n3.2 Background\\nThis solution proposes a specific ontology to structure the domain model (which was\\npreviously presented in Figure 2.1). It applies the concepts of agile methodology [4] that\\nelicits requirements as a set of prioritized user stories, also known as backlog to structure\\nthis ontology.\\nTo standardize and make the API’s output useful for further processing and evalu-\\nation, the solution uses function calls, a feature introduced by OpenAI that extends\\nthe capabilities of their language models by controlling the model’s output. Instead of\\nasking the system to generate free-form text, developers can provide a specific structure\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 26}, page_content='1\\nrecord_extracted_concepts = {\\n2\\n\"name\": \" record_elements \",\\n3\\n\"description\": \"Record the\\nelements\\nextracted\\nfrom a story\",\\n4\\n\"parameters\": {\\n5\\n\"type\": \"object\",\\n6\\n\"properties\": {\\n7\\n\"personas\": {\\n8\\n\"type\": \"array\",\\n9\\n\"description\": \"The list of personas\\nextracted\\nfrom\\nthe story\",\\n10\\n\"items\": { \"type\": \"string\" }\\n11\\n},\\n12\\n\"entities\": {\\n13\\n\"type\": \"array\",\\n14\\n\"description\": \"The list of entities\\nextracted\\nfrom\\nthe story\",\\n15\\n\"items\": { \"type\": \"string\" }\\n16\\n},\\n17\\n\"actions\": {\\n18\\n\"type\": \"array\",\\n19\\n\"description\": \"The list of actions\\nextracted\\nfrom\\nthe story\",\\n20\\n\"items\": { \"type\": \"string\" }\\n21\\n},\\n22\\n\"benefit\": {\\n23\\n\"type\": \"string\",\\n24\\n\"description\": \"A single\\nstring\\ncontaining\\nthe\\nbenefit\\nexpected\\nfrom the story\",\\n25\\n},\\n26\\n},\\n27\\n\"required\": [\"personas\", \"entities\", \"actions\", \"benefit\"],\\n28\\n}\\n29\\n}\\nListing 3.1: function call Example [1].\\nfor the output. This structure, defined using a JSON schema, acts as a template and\\ncan be seen in Listing 3.1. The system then fills in the template with appropriate data,\\nensuring the response matches the exact format needed.\\nWhen the API uses a function call to produce an output, the interaction is immedi-\\nately terminated, not allowing the sequence of prompts necessary to complete the three\\nnecessary steps to extract user stories information: extract concepts, categorize them,\\nand extract relations. To overcome this limitation, the approach engaged in a conver-\\nsation with the model to collect each answer in a dedicated schema, an example can be\\nseen in Listing 3.2.\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 27}, page_content='1\\nimport\\nopenai\\n2\\n3\\n# prompt\\ndefinition\\n4\\nconversation = {’role ’: ’system ’, ’content ’:’The\\nelements\\nyou are asked\\nto extract\\nfrom the\\nstories\\nare the\\nfollowing: Persona , Action ,\\nEntity , Benefit. A Story can\\ncontain\\nmultiple\\nelements in each\\ncategory.’}\\n5\\n6\\n# API call\\n7\\nresponse = openai.ChatCompletion .create(\\n8\\nmodel\\n= model ,\\n9\\nfunctions\\n= [ record_extracted_concepts ],\\n10\\nmessages\\n= conversation ,\\n11\\ntemperature = 0.0\\n12\\n)\\nListing 3.2: Prompt Definition and API Call Example [1].\\n3.3 Prompt Engineering and API Usage\\nThe solution proposed is based on prompt engineering to induce the LLM to produce\\nthe desired result, in this case, to extract concepts and relationships within a user story.\\nFour different prompts were created to tackle the distinct steps: system setup, extract\\nconcepts, categorize them, and extract relations:\\n1. System setup: the system role is impersonated, instructing the language model to\\ntake on a specific character. The overview of the task is presented.\\n2. Extract concepts: continuing in the system role, the task is refined to involve\\ndetailed extraction of personas, entities, actions, and benefits from a user story. A\\nconcrete example of this extraction process is provided, referencing a previously\\nannotated story. Subsequently, the user role is assumed, presenting the user story\\nto be processed.\\n3. Categorize concepts: given the model’s inability to retain information across inter-\\nactions, the previously derived concepts must be explicitly reintroduced into the\\ndialogue. Adopting the assistant role, a conversational entry is added, detailing\\nthe extracted concepts. Subsequently, the system role is impersonated to specify\\nthe task of classifying primary and secondary actions and entities, accompanied by\\nan example.\\n4. Extract relations: in the last step, the assistant role introduces the categories.\\nThen, the system role task is described and an example is given.\\nThen, user stories were fed one by one to the API to avoid hallucinations and overcome\\nthe tokens limitation in OpenAI’s API.\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 28}, page_content='3.4 Experimental Setup\\nIn their experiment, they compare their F-measure (harmonic mean of precision and\\nrecall) results with two other approaches: Visual Narrator, and Conditional Random\\nFields.\\nThe F-measure metric is chosen because it accounts for both correctly and\\nincorrectly classified observations. An F-measure score of 1.0 indicates a perfect match,\\nwhile a score of 0.0 signifies that either precision or recall is completely lacking.\\nVisual Narrator [48] is open-source software that uses NLP-based techniques, such as\\nPOS tagging and rule-based extraction to extract conceptual models from user stories.\\nConditional Random Fields (CRF) [33] is a Markov Random Fields class, a statistical\\nmodel that can be used to predict patterns based on their context. To apply CRF for\\nthis specific task, 20% of the dataset was used to train the model, by applying POS\\ntagging to each word in a user story to label the data. Once trained, the model could\\npredict the semantic role of unseen user stories.\\nTo ensure a fair comparison across the three tools, the annotated dataset [49] was\\ncleaned to include only the user stories that all tools could process. This resulted in a\\nsubset of 1,459 user stories, representing 87% of the complete dataset. In addition, since\\nVisual Narrator is not able to identify relations, the trigger and target relations were\\nnot evaluated.\\nTo accurately compare results, a criterion was determined, resulting in the following\\nthree comparison modes:\\n• Strict: In the strictest sense, a result is considered perfect when the AI produces\\nelements that exactly match those in the ground truth. An example of this com-\\nparison mode can be seen in Table 3.1.\\n• Inclusive: To introduce some flexibility, a scenario where the AI output is a superset\\nof the ground truth was considered. Here, we check that the baseline elements are\\nincluded within the AI-generated result. This approach allows for a broader range\\nof correct outputs. An example of this comparison mode can be seen in Table 3.2.\\n• Relaxed: Further relaxing the constraints, the Relaxed comparison mode allows\\nfor equivalencies such as treating plurals and singulars as the same or ignoring\\nadjective qualifiers. This approach accounts for variations in how concepts might\\nbe expressed. An example of this comparison mode can be seen in Table 3.3.\\nGround truth\\nExperiment results\\nComparison results\\nwebpage\\nwebpages\\nFalse\\nall webpages\\nwebpages\\nFalse\\nuser’s webpage\\n[user, webpage]\\nFalse\\nwebpage\\nwebpage\\nTrue\\nTable 3.1: Strict comparison example.\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 29}, page_content='Ground truth\\nExperiment results\\nComparison results\\nwebpage\\nwebpages\\nTrue\\nall webpages\\nwebpages\\nFalse\\nuser’s webpage\\n[user, webpage]\\nFalse\\nwebpage\\nwebpage\\nTrue\\nTable 3.2: Inclusive comparison example.\\nGround truth\\nExperiment results\\nComparison results\\nwebpage\\nwebpages\\nFalse\\nall webpages\\nwebpages\\nTrue\\nuser’s webpage\\n[user, webpage]\\nFalse\\nwebpage\\nwebpage\\nTrue\\nTable 3.3: Relaxed comparison example.\\nThe results showed that while Visual Narrator performed reasonably well in identifying\\npersonas and actions due to their predictable positions within the text, it struggled with\\nentity extraction, often misidentifying or omitting them.\\nThe proposed solution demonstrated a substantial improvement over Visual Narrator\\nin all categories. It required significantly less development effort and achieved superior\\nresults, particularly in identifying actions.\\nHowever, despite their overall strength, their solution was outperformed by a CRF-\\nbased model, which was trained specifically for this task.\\nStill, the ChatGPT-based\\nsolution was considered a promising approach for rapid prototyping.\\n3.5 Discussion\\nThe work of Arulmohan et al. serves as a crucial foundation for this research due to its\\npioneering application of LLMs to the domain of user story modeling. By demonstrating\\nthe feasibility of extracting domain models from textual requirements using LLMs, their\\nresearch provides a strong starting point for our exploration.\\nWhile their work lays the essential groundwork, this thesis aims to extend their findings\\nby addressing the limitation of their approach, which is heavily reliant on a specific LLM\\nprovider (OpenAI) and therefore raises concerns about model stability, potential biases,\\nand the impact of model updates on the overall solution. By exploring a model-agnostic\\napproach, this thesis intends to enhance the scalability and robustness of user story\\nmodeling.\\nAnother limitation of their work lies in its exclusive focus on concept extraction from\\nuser stories. While their experiment results in components of a KG in a JSON format,\\nit does not provide a graph representation from a practical user’s perspective.\\nThis\\nmeans that additional implementation steps are required to enable graphical visualiza-\\ntion and querying of the information. By neglecting the implementation of knowledge\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 30}, page_content='graphs which would allow users to visualize, query, and analyze the data, the research\\nfails to capitalize on the full potential of user stories for enhancing the software devel-\\nopment process. This thesis addresses this limitation by introducing a methodology for\\nautomatically generating and implementing knowledge graphs from user stories, thereby\\nproviding a more comprehensive understanding of system requirements.\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 31}, page_content='4 Design and Implementation\\nThe previous chapters established the motivation, theoretical foundation, and basis for\\nthis research by exploring user stories, knowledge graphs, and large language models.\\nBuilding upon this groundwork, this chapter delves into the practical aspects of the\\nresearch by detailing the design and implementation and paves the way for the evaluation\\nand analysis of its accuracy in the following chapter.\\nThis chapter begins in Section 4.1 by describing a preliminary experiment using\\nLangChain’s default module LLMGraphTransformer to extract a KG from user sto-\\nries, highlighting its limitations and the need for a customized approach. Section 4.2\\nintroduces the tailored module, the USGT, explaining how its architecture aligns with\\nthe defined ontology (Subsection 4.2.1) and the role of prompt design in its functional-\\nity (Subsection 4.2.2). The subsequent Section 4.3 outlines the design of the complete\\nsolution, from processing a product backlog to utilizing the USGT module and stor-\\ning the resulting knowledge graph in a Neo4j database. Lastly, Section 4.4 covers the\\nimplementation details of the USGT module.\\n4.1 Preliminary Experiment\\nIn the initial phase, an experiment was set up using the default LLMGraphTransformer\\nmodule [10] to explore the feasibility of using this module as a solution for extracting\\na Graph Document from user stories. Even though this module is quite versatile, it\\nwas designed for general-purpose knowledge graphs, and therefore it struggled to fully\\ncapture user stories nuances.\\nTo exemplify the difficulty of the default LLMGraphTransformer, an example is given\\nin Listing 4.1 and the results can be seen in Figure 4.1. The LLM was able to identify only\\none node correctly (UI designer as Persona) and two nodes partially correctly (button as\\nEntity, but should be new button, and user experience as Benefit, but should be improve\\nthe user experience), missing the Action nodes (create and improve). The relationships\\ncreated were also wrong.\\nThe preliminary experiment revealed significant limitations of the default LLMGraph-\\nTransformer in accurately mapping user stories into knowledge graphs. These limita-\\ntions stem from the design of the prompt behind the module, which is optimized for\\ngeneral-purpose knowledge graphs. The prompt defines nodes to represent entities and\\nconcepts, but this assumption does not always hold in the context of this research. For\\ninstance, a user story node is composed of an entire sentence, as well as the benefit\\nnode, making them more complex than simple entities or concepts. The treatment of\\nrelationships follows the same pattern: the prompt defines relationships as connections\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 32}, page_content='1\\nfrom\\nlangchain_experimental . graph_transformers\\nimport\\nLLMGraphTransformer\\n2\\nfrom\\nlangchain_core.documents\\nimport\\nDocument\\n3\\n4\\nllm_transformer = LLMGraphTransformer (\\n5\\nllm=llm ,\\n6\\nallowed_nodes =[’Persona ’, ’Action ’, ’Entity ’, ’Benefit ’],\\n7\\nallowed_relationships =[’TRIGGERS ’, ’TARGETS ’],\\n8\\n)\\n9\\n10\\nuser_story = \"As a UI designer , I want to create a new button so that I\\ncan improve\\nthe user\\nexperience.\"\\n11\\n12\\ndocument = [Document(page_content=user_story)]\\n13\\ngraph_document = llm_transformer . convert_to_graph_documents (document)\\n14\\n15\\ngraph. add_graph_documents (graph_document )\\nListing 4.1: Default LLMGraphTransformer to extract KG from user stories.\\nFigure 4.1: Neo4j visualization of the graph created in Listing 4.1.\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 33}, page_content='between entities or concepts, using general terms. Although the module offers two op-\\ntional parameters—allowed nodes and allowed relationships—intended to constrain the\\ngraph to a specific ontology, this approach does not produce the desired results.\\nThese shortcomings indicate that the default module is not sufficiently tailored to\\nhandle the intricacies of user stories, leading to incomplete and inaccurate graph repre-\\nsentations.\\nGiven these results, it became clear that a more specialized approach is necessary\\nto overcome these challenges. In response, this thesis proposes the development of a\\ncustom LLMGraphTransformer tailored specifically for user stories, which will be better\\nequipped to accurately identify and map the relevant nodes and relationships.\\nThis\\ncustom solution aims to address the gaps identified in the preliminary experiment, as well\\nas to address the limitation of the solution proposed by Arulmohan et al. and described\\nin Chapter 3, enabling a model-agnostic solution that improves upon the baseline.\\n4.2 Custom module: the UserStoryGraphTransformer\\nThis section introduces the USGT module, a specialized module to extract knowledge\\ngraphs from user stories using Large Language Models (LLMs).\\nThe following Sub-\\nsections 4.2.1 and 4.2.2 describe the module’s components and explain the design of\\nthe prompt, respectively. While LangChain’s LLMGraphTransformer module excels at\\nautomating the construction of general-purpose KG from text data using LLM, to effec-\\ntively capture the specific structure and relationships within user stories, it had to be\\ncustomized. For clarity, this customized version will be referred to as the USGT.\\nFigure 4.2: Component diagram of the UserStoryGraphTransformer module.\\nThe USGT module, as illustrated in the component diagram in Figure 4.2, comprises\\ntwo main components: the LLM Connector and the Graph Transformer.\\nThe LLM Connector receives as input a user story and an LLM configuration. The\\nuser story is a textual input in a Document format to be transformed into a KG, while the\\nLLM configuration, defined by the user, specifies the LLM provider and the model to be\\nused. This configuration enables communication with the LLM API by sending requests\\nand receiving responses. Upon receiving these inputs, the LLM Connector uses Prompt\\nTemplates to define prompts that guide the LLM in identifying and extracting the desired\\nnodes and relationships from the user story, and LangChain chains are employed to\\ninteract with the LLM. As an output, the LLM Connector delivers the LLM-derived\\ncomponents of a KG, composed of nodes and relationships.\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 34}, page_content='The Graph Transformer processes the LLM-derived components, enriching it with\\nadditional information, and converting nodes and relationships into a specific format\\nsuitable for ingestion into the graph database. This component ensures that the KG\\ncomponents extracted using the LLM Connector adhere to the ontology’s constraints\\nand formats. As output, the Graph Transformer delivers the Graph Document, which\\nis a consistent and accurate representation of the knowledge graph.\\nTo maintain the integrity of the extracted knowledge graph, each user story is pro-\\ncessed independently. This decision aligns with the ”Independent” principle from the\\nINVEST acronym.\\nThis decision is further supported by the experiment detailed in\\nChapter 3. In that study [1], researchers found that when the LLM was provided with\\na list of user stories, it tended to confuse nodes between different stories and even gen-\\nerated hallucinations. To mitigate these issues, processing each story separately was\\nnecessary.\\nIn summary, the USGT module provides a robust solution for automating the extrac-\\ntion of knowledge graphs from user stories. By integrating prompt engineering with data\\nprocessing aligned with the chosen ontology, it aims to ensure both the accuracy and\\nconsistency of the extracted knowledge graph. And, in the next subsections, we will dive\\ninto how each component is adapted to align with the knowledge graph architecture and\\nhow prompt engineering techniques were applied.\\n4.2.1 Aligning the LLM Connector and the Graph Transformer\\nThis section presents how both components of the USGT module, the LLM Connector\\nand the Graph Transformer, are aligned with the knowledge graph architecture (as pre-\\nsented in Subsection 2.2.1) to extract a KG from a user story, mapping the components\\nof a user story to specific nodes and relationships within the graph.\\nThe output of the USGT module is a Graph Document, which is in essence a knowledge\\ngraph with a specific object format. Part of the customization of this module is to make\\nsure that it adheres to the defined ontology (illustrated in Figure 2.1), which is also an\\nimportant part of being able to validate the quality of the KG extracted.\\nFrom a user story taken from the product backlog, the following information is ex-\\ntracted and represented as nodes in the knowledge graph:\\n• Userstory (1:1): each real-world user story has a node representation.\\n• Persona (1:1): each user story is directly associated with one persona entity.\\n• Actions (1:N): a user story can involve multiple actions, and each action is repre-\\nsented by a separate entity.\\n• Entities (1:N): similar to actions, a user story can involve multiple entities, each\\nrepresented by a separate entity.\\n• Benefit (0:1): optionally, a user story may describe a benefit, also represented as\\nan entity.\\n35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 35}, page_content='The relationships between these entities capture the interactions within the user story:\\n• Triggers: A directed relationship from the Persona entity to the main Action entity,\\nindicating the persona triggers the main action within the user story.\\n• Targets: Directed relationships from each Action entity to its related Entity enti-\\nties, signifying the actions target specific entities within the system.\\n• Has persona: A relationship between the user story and persona nodes.\\n• Has action: A relationship between the user story and action nodes.\\n• Has entity: A relationship between the user story and entity nodes.\\n• Has benefit: A relationship between the user story and benefit nodes.\\nWhen analyzing the KG architecture, it is possible to identify opportunities to stream-\\nline the LLM’s workload, which can save up resources, such as API costs, and processing\\ntime, while also enhancing accuracy.\\nThe userstory node represents the input itself and, therefore, does not require further\\nprocessing by the LLM to be represented as a node in the KG. Instead, it can be efficiently\\nhandled by the Graph Transformer component of the USGT module, by enriching the\\nLLM’s extracted nodes with this userstory node.\\nAdditionally, some relationships can be logically inferred from the existing nodes. For\\ninstance, if the LLM is able to extract a persona node, the has persona relationship\\ncan be established with certainty.\\nThe same logic applies to has action, has entity,\\nand has benefit relationships. The logical inference of relationships, allows the LLM to\\nconcentrate its resources on extracting more complex relationships—specifically triggers\\nand targets—that demand a deeper semantic understanding of the user story content.\\nThe LLM Connector relies on this architecture to make sure that its prompt design\\ncan guide the LLM to extract the persona, action, entity, and benefit nodes and the\\nrelationships triggers and targets. In addition, the Graph Transformer is responsible\\nfor enriching the KG with the userstory node and for deriving the logically inferred\\nrelationships, which are based on the existence of the extracted nodes from the LLM\\nConnector.\\n4.2.2 Prompt Design\\nThis subsection specifies the prompt template definition as part of the LLM Connector\\ncomponent of the USGT module. The purpose of the prompt in this scenario is to guide\\nthe LLM to the desired output, by extracting nodes and relationships from a user story\\naccording to the knowledge graph architecture.\\nWhen interacting with LLMs via APIs, the level of structure in the output is signifi-\\ncantly influenced by whether the model supports function calls. Models like ChatGPT\\n3.5 from OpenAI support this functionality and provide structured outputs in a prede-\\nfined JSON format, simplifying data processing and integration. However, models like\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 36}, page_content='Llama 3 by Meta do not have this capability, requiring more explicit prompts and exam-\\nples to guide the LLM toward a desired output structure. While the LLM may attempt\\nto adhere to the specified format, there’s no guarantee of perfect consistency without\\nfunction calls.\\nLangChain’s default LLMGraphTransformer module addresses these differences by\\nimplementing two distinct prompts—one for LLMs that support function calls and an-\\nother for those that do not. While these prompts vary in format, their core instructions\\nare similar, and they share the same objective. In this direction, in USGT, the first idea\\nwas to modify these prompts to extract the persona, action, entity, and benefit nodes\\nand create the triggers and targets relationships based on them.\\nHowever, this approach proved to be not ideal because the presence of a benefit node\\nin a user story typically implies the existence of corresponding action and entity nodes.\\nThe LLM struggled to extract all three node types simultaneously, often identifying only\\nthe benefit node.\\nTo address this limitation, a new strategy was adopted: two separate prompts were\\ncreated—one for extracting the persona, action, and entity nodes and their relationships,\\nand another solely for extracting the benefit node. This separation was made because\\nthe first three node types are always present in any given user story, ensuring that their\\nrelationships are consistently extracted. In contrast, the benefit node is optional and does\\nnot participate in the relationships to be extracted by the LLM (triggers and targets).\\nBy isolating the benefit node extraction into its own prompt, the extraction performance\\nand consistency were improved.\\n1\\nsystem_prompt = (\\n2\\n\"\"\"\\n3\\nKnowledge\\nGraph\\nConstructor\\nInstructions \\\\n\\n4\\n## 1. Overview \\\\n\\n5\\nYou are a specialized\\nrequirements\\nengineer , who\\nunderstands\\nabout\\nscrum\\nframework. Your task is to analyze\\nand\\nextract\\nnodes and\\nrelationships\\nfrom user\\nstories to build a knowledge\\ngraph.\\n6\\nYou have to extract as much\\ninformation as possible\\nwithout\\nsacrificing\\naccuracy. Do not add any\\ninformation\\nthat is not\\nexplicitly in the\\nmentioned\\nuser\\nstory. \\\\n\\n7\\n## 2. Nodes \\\\n\\n8\\nNodes\\nrepresent\\nconcepts in a user\\nstory. Given a user story , you need\\nto extract: \\\\n\\n9\\n- Persona: there is only one\\npersona\\nnode per user story , introduced as\\n’As a *persona *,’. \\\\n\\n10\\n- Actions: are all verbs in the user\\nstory\\nthat\\ndescribe\\nwhat the\\npersona\\ndesires to do (e.g. move on , access , have). Extract\\nthe verb\\nonly , without\\nmodifiers .\\\\n\\n11\\n- Entities: are nouns and each noun must be extracted as a separate\\nentity , even if they seem\\nrelated or grouped. Include\\nany\\nmodifiers\\nthat\\nclarify\\nthe entity (e.g. library\\ndatabase , domain).\\n\\\\n\\n12\\n** Consistency **: Ensure you use\\navailable\\ntypes for node labels , you\\nnecessarily\\nextract at least 4 nodes: persona , action , entity .\\\\n\\n13\\n** Node IDs **: Never\\nutilize\\nintegers as node IDs. Node IDs should be\\nnames or human -readable\\nidentifiers\\nextracted as found in the user\\nstory .\\\\n\\n37'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 37}, page_content='14\\n** Extract\\nall\\nactions\\nand\\nentities **:\\ncapture\\nevery\\naction and its\\ncorresponding\\nentity .\\\\n\\n15\\n** Separate\\nverbs **:\\nconsider\\neach verb as a distinct\\naction and its\\nobjects as related\\nentities .\\\\n\\n16\\n## 3. Relationships\\\\n\\n17\\nRelationships\\nrepresent\\nconnections\\nbetween\\nnodes. The only\\npossible\\nrelationships\\nare:\\\\n\\n18\\n- Persona ->main\\naction (triggers). \\\\n\\n19\\n- Action ->entity (targets). \\\\n\\n20\\nNo other\\nrelationships\\nare\\nallowed\\nexcept for the ones above , make sure\\nto create all the\\npossible\\nrelationships. \\\\n\\n21\\n## 4. Coreference\\nResolution \\\\n\\n22\\n** Maintain\\nEntity\\nConsistency **: When\\nextracting\\nentities , it’s vital to\\nensure\\nconsistency .\\\\n\\n23\\nIf an entity , such as \"John Doe\", is mentioned\\nmultiple\\ntimes in the\\ntext but is referred to by different\\nnames or pronouns (e.g., \"Joe\",\\n\"he\"), always use the most\\ncomplete\\nidentifier\\nfor that\\nentity\\nthroughout\\nthe\\nknowledge\\ngraph. In this example , use \"John Doe\" as\\nthe entity ID.\\\\n’\\n24\\nRemember , the\\nknowledge\\ngraph\\nshould be coherent\\nand easily\\nunderstandable , so maintaining\\nconsistency in entity\\nreferences is\\ncrucial .\\\\n\\n25\\n## 5. Strict\\nCompliance\\\\n\\n26\\nAdhere to the rules\\nstrictly. Non -compliance\\nwill\\nresult in termination.\\n27\\n## 6. Example \\\\n\\n28\\n’As a user , I want to sync my data so that I can access my information\\nfrom\\nanywhere.’ \\\\n\\n29\\nExtracted\\nNodes: \\\\n\\n30\\nPersona: [’user ’] \\\\n\\n31\\nAction: [’sync ’, ’access ’] \\\\n\\n32\\nEntity: [’data ’, ’current\\ninformation ’, ’anywhere ’] \\\\n\\n33\\nRelationships: \\\\n\\n34\\nTRIGGERS: [[’user ’, ’sync ’]] \\\\n\\n35\\nTARGETS: [[’sync ’, ’data ’], [’access ’, ’current\\ninformation ’]] \\\\n\"\"\"\\n36\\n)\\n37\\ndefault_prompt = ChatPromptTemplate . from_messages (\\n38\\n[\\n39\\n( \"system\", system_prompt , ),\\n40\\n(\\n41\\n\"human\",\\n42\\n(\\n43\\n\"Tip: Make sure to answer in the\\ncorrect\\nformat and do \"\\n44\\n\"not\\ninclude\\nany\\nexplanations. \"\\n45\\n\"Use the given\\nformat to extract\\ninformation\\nfrom the \"\\n46\\n\"following\\ninput: {input}\"\\n47\\n),\\n48\\n),\\n49\\n]\\n50\\n)\\nListing 4.2: Main Prompt Design.\\nThe first prompt (Listing 4.2), also referred to as the main prompt is the most complex\\nbecause it involves multiple tasks. It uses two different roles, the system role to set the\\n38'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 38}, page_content='context, and the human role to provide the input, in this case, the user story.\\nThe\\ncontext part is structured into six different parts:\\n1. Overview: Directs the model to assume the role of a requirements engineer and\\nestablishes the task context.\\n2. Nodes: Introduces the concept of a node, describes the three possible node types\\n(Persona, Action, and Entity), and provides detailed instructions for extracting all\\nrelevant nodes from the user story.\\n3. Relationships: Specifies what constitutes a relationship, presents the two possible\\nrelationship types (TRIGGERS between Persona and the main Action, and TAR-\\nGETS between Action and Entity), and emphasizes that no other relationships\\nshould be extracted.\\n4. Coreference Resolution: Reinforces the importance of maintaining consistency and\\ncoherence across extracted nodes and relationships.\\n5. Strict Compliance: Stresses the necessity for the model to adhere strictly to the\\ngiven instructions.\\n6. Example: Provides an example of a user story, illustrating the extracted nodes and\\nrelationships to guide the model’s output.\\nIn addition to the main prompt, a set of five examples was included to guide the\\nLLM in cases where function call support is not available. These examples, shown in\\nListing 4.3, illustrate the expected output format. In this format, text represents the\\ninput user story, head and tail correspond to the extracted nodes, head type and tail type\\nindicate the type of each node, and relation specifies the relationship between them.\\n1\\n{\\n2\\n\"text\": \"As a business owner , I want to give my inputs on the\\nproduct\\ndevelopment.\",\\n3\\n\"head\": \"business\\nowner\",\\n4\\n\"head_type\": \"Persona\",\\n5\\n\"relation\": \"TRIGGERS\",\\n6\\n\"tail\": \"give\",\\n7\\n\"tail_type\": \"Action\",\\n8\\n}\\nListing 4.3: Example of expected output when function call is not supported.\\nThe second prompt (Listing 4.4), referred to as the benefit prompt, is much simpler,\\nas it has a single objective: to extract the benefit node, if present. It is structured into\\nthe following three components:\\n1. Overview: Instructs the model to assume the role of a requirements engineer and\\nprovides context for the task.\\n2. Benefit: Defines what constitutes a benefit sentence within a user story and directs\\nthe model to extract it only if it is explicitly stated.\\n39'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 39}, page_content='3. Examples: Provides two examples of user stories—one where the benefit node is\\npresent and can be extracted, and another where it is absent, prompting the LLM\\nto return an empty response.\\n1\\nbenefit_prompt = (\\n2\\n\"\"\"\\n3\\nYou are a specialized\\nrequirements\\nengineer , who\\nunderstands\\nabout\\nscrum\\nframework .\\\\n\\n4\\nYou have to extract as much\\ninformation as possible\\nwithout\\nsacrificing\\naccuracy.\\n5\\nDo not add any\\ninformation\\nthat is not\\nexplicitly in the\\nmentioned\\nuser\\nstory .\\\\n\\n6\\n## Benefit\\\\n\\n7\\nExtract\\nthe\\nbenefit\\nsentence of the user story , if it exists.\\n8\\nThe\\nbenefit\\nsentence is a sentence\\ntypically\\nintroduced as ’so that\\n*benefit*’, ’in order to *benefit *’.\\n9\\n## Examples\\\\n\\n10\\nif benefit\\nsentence\\nexists: \\\\n\\n11\\ninput: ’As a user , I want to sync my data , so that I can access my\\ninformation\\nfrom\\nanywhere .’\\\\n\\n12\\nanswer: Node(id=’I can access my information\\nfrom\\nanywhere ’,\\ntype=’Benefit ’)\\\\n\\n13\\nif benefit\\nsentence\\ndoes not exist: \\\\n\\n14\\ninput: ’As a customer , I want to pay by cash.’ \\\\n\\n15\\nanswer: ’’ \\\\n\\n16\\n\"\"\"\\n17\\n)\\n18\\n19\\n20\\nbenefit_prompt = ChatPromptTemplate . from_messages (\\n21\\n[\\n22\\n(\\n23\\n\"system\",\\n24\\nbenefit_prompt ,\\n25\\n),\\n26\\n(\\n27\\n\"human\",\\n28\\n(\\n29\\n\"Tip: Make sure to answer in the\\ncorrect\\nformat and do \"\\n30\\n\"not\\ninclude\\nany\\nexplanations. \"\\n31\\n\"Use the given\\nformat to extract\\ninformation\\nfrom the \"\\n32\\n\"following\\ninput: {input}\"\\n33\\n),\\n34\\n),\\n35\\n]\\n36\\n)\\nListing 4.4: Benefit Prompt Design.\\nTo guide both prompts, the 6 Strategies for getting better results by OpenAI [44] were\\nused as a reference (accessed in November 2024), as described in Table 4.1. This guide\\nis composed of high-level principles, strategies, and specific methods, and tactics, that\\ncan be used to implement those strategies. The purpose of applying these methods is\\n40'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 40}, page_content='Strategy\\nTactics\\nStatus\\nWrite clear\\ninstructions\\nInclude details in your query to get more relevant an-\\nswers\\n✓\\nAsk the model to adopt a persona\\n✓\\nUse delimiters to clearly indicate distinct parts of the\\ninput\\n✓\\nSpecify the steps required to complete a task\\n✓\\nProvide examples\\n✓\\nSpecify the desired length of the output\\nNA\\nProvide reference text\\nInstruct the model to answer using a reference text\\n✓\\nInstruct the model to answer with citations from a ref-\\nerence text\\nNA\\nSplit complex tasks\\ninto simpler\\nsubtasks\\nUse intent classification to identify the most relevant\\ninstructions for a user query\\n✓\\nFor dialogue applications that require very long conver-\\nsations, summarize or filter previous dialogue\\nNA\\nSummarize long documents piecewise and construct a\\nfull summary recursively\\nNA\\nGive the model\\ntime to ”think”\\nInstruct the model to work out its own solution before\\nrushing to a conclusion\\nNA\\nUse inner monologue or a sequence of queries to hide\\nthe model’s reasoning process\\nNA\\nAsk the model if it missed anything on previous passes\\nNA\\nUse external tools\\nUse embeddings-based search to implement efficient\\nknowledge retrieval\\nNA\\nUse code execution to perform more accurate calcula-\\ntions or call external APIs\\nNA\\nGive the model access to specific functions\\n✓\\nTest changes\\nsystematically\\nEvaluate model outputs with reference to gold-standard\\nanswers\\n✓–\\nTable 4.1: OpenAI prompt guidelines.\\n41'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 41}, page_content='to improve the chances of the LLM to produce the desired outcome. Nine of the tactics\\nwere fully implemented (✓), one was partially implemented (✓–), and nine were not\\nrelevant or could not be applied in this solution.\\nIn conclusion, effective prompt engineering is crucial to obtain expected outputs within\\nthe USGT module. By employing a dual-prompt strategy—one designed to extract the\\npersona, action, and entity nodes along with their relationships, and another focused\\non isolating the optional Benefit node—the extraction accuracy was improved.\\nThis\\napproach not only leverages the unique capabilities of different LLMs but also aligns\\nwith best practices for prompt generation.\\n4.3 Design of the Automated Knowledge Graph Extraction\\nThis section delves into the design of the fully automated solution of Knowledge Graph\\nextraction from user stories. The USGT module uses its LLM Connector component\\nto interact with an LLM via LangChain, extracting nodes and relationships from user\\nstories, also named LLM-derived components, and its Graph Transformer component to\\nprocess the data into a graph structure. However, to ensure an automated process, the\\ncomplete automated solution also benefits from other ready-made modules within the\\nLangChain framework which will be further explained.\\nFigure 4.3: Overview of automated KG extraction using the USGT module.\\nTo illustrate the behavioral architecture, refer to Figure 4.3, which presents an activity\\ndiagram of the end-to-end process, from receiving a backlog to visualizing the knowledge\\ngraph in Neo4j. In this diagram, activities are color-coded to differentiate between pre-\\nbuilt and custom-developed activities: orange indicates LangChain modules, while white\\nrepresents components developed as part of this thesis.\\n42'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 42}, page_content='The first activity starts when a backlog is received (Figure 4.4) and it involves the\\nconfiguration of the LLM, a required input of the USGT module. The LLM configuration\\nspecifies the connection to the LLM API via the LangChain framework, which offers the\\npossibility to connect with several LLM providers.\\nFigure 4.4: Sub-diagram of activity 1: Configure LLM.\\nActivity 2, transform user story into Document format, applies LangChain ready-\\nmade function to convert the user story into a Document object type that is required\\nfor LLM interaction.\\nActivity 3 (Figure 4.5) is part of the USGT module and describes how the LLM\\nConnector works. It begins once the LLM configuration and the Document user story\\nare received, it then assesses if this language model supports or does not support function\\ncalls and proceeds to define the main and the benefit prompt templates as depicted in\\nSubsection 4.2.2.\\nTwo prompts are defined using a prompt template: the main and benefit prompts, in\\nwhich a user story parameter can be dynamically populated with each user story input.\\nThe prompt definition approach differs if the language model supports or not function\\ncalls. If it does not support this functionality, five additional examples and detailed\\ninstructions to guide the LLM toward responding in a specific JSON format are added\\nto the prompt.\\nFor models that support function calls, an output schema template is defined, con-\\ntaining nodes and relationship keys, whose values will be filled out by the LLM response.\\nThen two chains are constructed for the main and benefit prompts. They define the\\nsequence of operations performed on the language model, varying based on whether it\\nsupports function calls or does not. Once the chains are defined, the Document user\\nstory is then added to the prompt templates contained in them, resulting in chains. The\\nnext step is to send two requests (one for each chain) to the LLM API, and then, the\\nactivity is finished once it obtains the LLM responses, or LLM-derived components.\\nActivity 4, shown in Figure 4.3, uses the Graph Transformer component of the USGT\\nmodule. The objective is to transform the LLM-derived KG components into a Graph\\nDocument. This step can be seen in more detail in Figure 4.6.\\n43'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 43}, page_content='Figure 4.5: Sub-diagram of activity 3: Extract LLM-derived components using LLM\\nConnector.\\n44'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 44}, page_content='Figure 4.6: Sub-diagram of activity 4: Transform LLM-derived components into a Graph\\nDocuments using Graph Transformer.\\n45'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 45}, page_content='Once the LLM-derived KG components are received, their processing depends on the\\nLLM’s support for function calls.\\nIf the LLM supports function calls, the LLM response is already structured into JSON\\nformat, and the nodes and relationships can be directly accessed. In the other case, the\\nLLM response cannot be structured into a JSON format. It is a string, and if the LLM\\nfollowed all the instructions provided, it is possible to parse this string into a JSON\\nformat. Both scenarios result in a KG components object, which contains the nodes and\\nrelationships extracted by the LLM.\\nThen the nodes’ list within KG components object is enriched by adding the input\\nuser story as a node. This ensures that all nodes can be traced back to their original\\nuser story, aligning with the defined ontology. Subsequently, the complete nodes’ list\\nis converted into a list of Graph Node objects, a specific data structure required for\\ncreating a Graph Document.\\nAs discussed previously, certain relationships can be logically inferred from existing\\nnodes: has benefit, has persona, has entity and has action. Based on the presence of\\nspecific node types, these inferred relationships are derived and combined with the ex-\\nplicitly extracted relationships from the KG components. These combined relationships\\nare then converted into Graph Relationships.\\nWith the nodes and relationships properly converted into Graph Nodes and Graph\\nRelationships, respectively, a Graph Document can be generated. The Graph Document\\nis a special data structure to represent a Knowledge Graph that is required to be ingested\\nby the Neo4j database.\\nFigure 4.7: Sub-diagram of activity 5: Store Graph Document into Neo4j.\\nThe final activity, which can be visualized in Figure 4.7, has the objective of storing\\nthe Graph Document in Neo4j. LangChain’s Neo4j integration is used to establish a\\nconnection with the database and ingest the extracted KG.\\nAn example of the KG extracted from the user story ”As a student, I want to learn\\n46'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 46}, page_content='Figure 4.8: Neo4j snapshot of knowledge graph extracted from example user story using\\nUSGT.\\nhow to code, so that I can build my own projects.” can be seen in Figure 4.8, the user\\nstory represented by the grey node, the persona by the blue one, the actions by the red\\nnodes, the entities by the green nodes, and the benefit by the pink node.\\nThis section has presented a comprehensive solution for automated knowledge graph\\nextraction from user stories. The USGT module, and therefore its components, were\\nused to interact with an LLM, extract relevant information, and generate a Graph Doc-\\nument. Pre-built LangChain modules played an important role in guaranteeing an end-\\nto-end automated solution. The Graph Document, compatible with the Neo4j database,\\nprovides a valuable resource for analysis, visualization, and informed decision-making\\nthroughout the requirements engineering life-cycle.\\n4.4 UserStoryGraphTransformer Implementation\\nAfter understanding the importance and behavior of the USGT module within the au-\\ntomated process of knowledge graph extraction from user stories, in this section the\\nimplementation details of the module are explained and the repository can be found\\non [15].\\n1\\nfrom\\nlangchain_ollama\\nimport\\nOllamaLLM\\n2\\nfrom\\nlangchain_openai\\nimport\\nChatOpenAI\\n3\\n4\\nllm_ollama = OllamaLLM(model=\"llama3\")\\n5\\nllm_openai = ChatOpenAI(model_name=’gpt -4o-mini ’)\\nListing 4.5: Example of two different LLM configurations with LangChain.\\nThe module requires two parameters as input to function: an LLM configuration,\\nand a user story to be processed. The LLM configuration is facilitated by LangChain’s\\nframework, which offers a wide range of LLM providers to connect with (as seen in\\nListing 4.5).\\nThe user story is then converted into a Document format using the\\nlangchain core.documents module.\\nNext, the LLM Connector comes into action to interact with the LLM via API. For\\nit, two chains using LCEL are defined. Each chain is composed of a prompt template,\\n47'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 47}, page_content='1\\nstructured_llm = llm. with_structured_output (schema , include_raw=True)\\n2\\nmain_chain = prompt | structured_llm\\n3\\nbenefit_chain = benefit_prompt | structured_llm\\nListing 4.6: Chain definitions when function call is supported.\\n1\\nmain_chain = prompt | llm\\n2\\nbenefit_chain = benefit_prompt | llm\\nListing 4.7: Chain definitions when function call is not supported.\\ncontaining a dynamic placeholder to be filled with each user story, the LLM configuration,\\nwhich was received as input, and in the case of an LLM that supports function calls, a\\nstructured output parser, to ensure that the LLM’s output adheres to the specific schema\\ndefined (see Listings 4.6 and 4.7 to exemplify).\\n1\\nllm_main_response = main_chain.invoke ({\"input\": user_story })\\n2\\nllm_benefit_response = benefit_chain.invoke ({\"input\": user_story })\\nListing 4.8: Sending requests to the LLM API.\\nTo send the prompt to the API, the chain’s invoke method is used. This method fills\\nthe prompt’s dynamic placeholder with the user story (see Listing 4.8).\\n1\\nclass\\nNode(Serializable):\\n2\\n\"\"\" Represents a node in a graph\\nwith\\nassociated\\nproperties.\\n3\\n4\\nAttributes:\\n5\\nid (Union[str , int]): A unique\\nidentifier\\nfor the node.\\n6\\ntype (str): The type or label of the node , default is \"Node \".\\n7\\nproperties (dict): Additional\\nproperties\\nand\\nmetadata\\nassociated\\nwith the node.\\n8\\n\"\"\"\\n9\\n10\\nid: Union[str , int]\\n11\\ntype: str = \"Node\"\\n12\\nproperties: dict = Field( default_factory =dict)\\nListing 4.9: Definition of a Graph Node.\\nOnce the responses are received, the goal is to transform the knowledge graph com-\\nponents into a Graph Document. This process uses the Graph Transformer component,\\nand begins by enriching the components with the user story itself, ensuring that the\\nextracted nodes can be traced back to their source. This enriched data is then converted\\ninto Graph Nodes (see Listing 4.9).\\nThen, logically inferred relationships are derived to ensure that all extracted nodes\\nare properly connected within the knowledge graph. These relationships are determined\\nbased on the existence of specific node types and their inherent connections to the user\\n48'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 48}, page_content='1\\ndef\\ncreate_logical_rel (nodes):\\n2\\nrels = []\\n3\\n4\\nfor node in nodes:\\n5\\nif node.type == ’Userstory ’:\\n6\\nuser_story_node = node\\n7\\n8\\nfor node in nodes:\\n9\\nif node.type == ’Persona ’:\\n10\\nrels.append( create_relationship (user_story_node , node ,\\n’HAS_PERSONA ’))\\n11\\nelif node.type == ’Action ’:\\n12\\nrels.append( create_relationship (user_story_node , node ,\\n’HAS_ACTION ’))\\n13\\nelif node.type == ’Entity ’:\\n14\\nrels.append( create_relationship (user_story_node , node ,\\n’HAS_ENTITY ’))\\n15\\nelif node.type == ’Benefit ’:\\n16\\nrels.append( create_relationship (user_story_node , node ,\\n’HAS_BENEFIT ’))\\n17\\nreturn\\nrels\\nListing 4.10: Deriving logically inferred relationships based on extracted nodes.\\nstory.\\nFor instance, each user story is expected to have one associated Persona, at\\nleast one Action, Entity, and optionally a Benefit. The function create logical rel (see\\nListing 4.10) iterates through the extracted nodes, identifies their types, and generates\\nrelationships accordingly. By logically inferring these relationships, the module mini-\\nmizes reliance on the LLM for direct extraction, streamlining the process and reducing\\nthe API costs.\\nThese inferred relationships, along with the extracted knowledge graph components,\\nare further transformed into Graph Relationships (see Listing 4.11). Finally, with both\\nGraph Nodes and Graph Relationships in place, the Graph Document is constructed\\n(see Listing 4.12). This document represents the complete knowledge graph in a specific\\nformat suitable for ingestion into the Neo4j database.\\nGiven the Graph Document, it can be uploaded to the Neo4j database with the support\\nof the class Neo4jGraph from langchain community.graphs module. This class facilitates\\nthe connection to the database and allows for the addition of the document via the\\npre-built add graph documents function.\\n49'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 49}, page_content='1\\nclass\\nRelationship(Serializable):\\n2\\n\"\"\" Represents a directed\\nrelationship\\nbetween two nodes in a graph.\\n3\\n4\\nAttributes:\\n5\\nsource (Node): The source\\nnode of the\\nrelationship.\\n6\\ntarget (Node): The target\\nnode of the\\nrelationship.\\n7\\ntype (str): The type of the\\nrelationship.\\n8\\nproperties (dict): Additional\\nproperties\\nassociated\\nwith the\\nrelationship.\\n9\\n\"\"\"\\n10\\n11\\nsource: Node\\n12\\ntarget: Node\\n13\\ntype: str\\n14\\nproperties: dict = Field( default_factory =dict)\\nListing 4.11: Definition of a Graph Relationship.\\n1\\nclass\\nGraphDocument(Serializable):\\n2\\n\"\"\" Represents a graph\\ndocument\\nconsisting of nodes and\\nrelationships .\\n3\\n4\\nAttributes:\\n5\\nnodes (List[Node ]): A list of nodes in the graph.\\n6\\nrelationships (List[Relationship ]): A list of relationships in\\nthe graph.\\n7\\nsource (Document): The\\ndocument\\nfrom\\nwhich the graph\\ninformation\\nis derived.\\n8\\n\"\"\"\\n9\\n10\\nnodes: List[Node]\\n11\\nrelationships: List[Relationship]\\n12\\nsource: Document\\nListing 4.12: Definition of a Graph Document.\\n50'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 50}, page_content='5 Evaluation\\nThe evaluation phase is a critical component of this research, offering insights into the\\naccuracy, and in consequence, the reliability of the proposed solution.\\nThis chapter\\naddresses the evaluation methodology, along with the description of the metrics to be\\nevaluated (Section 5.1), provides details about how the experiments were conducted\\n(Section 5.2), presents the results 5.3, and concludes with a comparison to the benchmark\\nand an analysis of the work (Section 5.4).\\n5.1 Evaluation Methodology\\nOne contribution of this thesis is the development of an evaluation script (which can\\nbe found in [15]) that is able to automatically compare the extracted KG nodes and\\nrelationships against the annotated dataset [49] considered as the ground truth and\\ncalculate evaluation metrics. In this section, the specific metrics used on the evaluation\\nscript and their methods are detailed.\\nTo evaluate the knowledge graph extraction, a combination of Multiple-Classification\\n(MC) and Token-Similarity (TS) metrics as outlined by Taojun Hu and Xiao-Hua Zhou\\n[29] was employed. MC metrics, such as Accuracy, Recall, Precision, and F-measure,\\nevaluate the LLM’s ability to categorize texts into various groups, each representing a\\nlabel. TS metrics, including Perplexity, BLEU, ROUGE (in various forms), BERTScore,\\nand METEOR, measure the semantic similarity between the LLM’s generated text and\\na reference.\\nTo calculate the MC metrics, it is essential to define what qualifies as a correct output\\nfrom the LLM. For this, the criteria outlined by Arulmohan et al. [1] was adopted,\\nutilizing three comparison modes: strict, inclusive, and relaxed. The strict comparison\\nconsider a result as correct when the LLM produces the exact same response elements as\\nthe ground truth, the inclusive adds some flexibility and considers the LLM’s output as a\\nsuperset of the ground truth, and the relaxed comparison ignores adjective qualifiers and\\nconsiders plurals as singulars as the same (further examples can be seen in Chapter 3).\\nBased on this criteria, each knowledge graph (KG) component generated by the LLM\\ncan be classified as follows:\\n• True Positive (TP): An element that is considered equivalent as the ground truth.\\n• False Positive (FP): An element that was incorrectly identified by the LLM, as it\\nshould not have been included.\\n• False Negative (FN): An element that should have been identified by the LLM but\\nwas not, indicating a missing component.\\n51'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 51}, page_content='Once the TP, FP, and FN elements are identified and counted for each user story, it is\\npossible to calculate the Recall (Equation 5.1), Precision (Equation 5.2), and F-measure\\n(Equation 5.3) metrics for each comparison mode.\\nRecall =\\nTP\\nTP + FN\\n(5.1)\\nPrecision =\\nTP\\nTP + FP\\n(5.2)\\nF-measure = 2 · Precision · Recall\\nPrecision + Recall\\n(5.3)\\nA new addition in comparison to the evaluation of the experiment by Arulmohan et al.\\n[1] was the inclusion of the evaluation of the benefit node as well. The recall, precision\\nand F-measure were calculated for the strict and inclusive modes, but not to the relaxed\\nmode because of missing POS annotations on the ground-truth dataset.\\nAnother contribution of this thesis is the inclusion of BERTScore as an additional\\nTS metric to assess the quality of the extracted nodes. While other token similarity\\nmetrics, such as Perplexity, BLEU, ROUGE, and METEOR, exist, BERTScore was\\nchosen exclusively due to the specific characteristics of the data being analyzed. Many\\nactions and entities in the dataset consist of only a single token. Metrics like BLEU,\\nROUGE, and METEOR, which rely heavily on token overlaps or n-gram matches, are\\nless effective in such cases because they do not fully capture semantic relationships when\\nlimited lexical information is available. BERTScore, in contrast, provides a more relaxed\\nevaluation by assessing semantic similarity through contextual embeddings, allowing it\\nto capture deeper meaning in the generated content and compare it to the ground truth\\nmore effectively.\\nBERTScore [58], a metric to evaluate text generation which uses BERT [19] embed-\\ndings, was chosen due to its ability to measure the quality of a machine-generated text\\nin comparison to a reference text [29]. This metric uses token embeddings to capture\\nsemantic similarity, providing a nuanced measure of textual coherence and relevance [58].\\nBERTScore [29] starts by loading the pre-trained BERT embeddings. Each token gen-\\nerated by the LLM and from the ground truth is mapped to a corresponding embedding,\\nresulting in two sequences of fixed-length vectors:\\nxemb\\ni\\n= BERT(xi),\\ni = 1, . . . , N\\n(5.4)\\nyemb\\nj\\n= BERT(yj),\\nj = 1, . . . , M\\n(5.5)\\nThen, the similarity between each pair of tokens across the two texts is computed\\nusing cosine similarity:\\nsim(xemb\\ni\\n, yemb\\nj\\n) =\\nxemb\\ni\\n· yemb\\nj\\n|xemb\\ni\\n||yemb\\nj\\n|\\n(5.6)\\n52'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 52}, page_content='To aggregate the similarity scores, element-wise matching is performed, and the aver-\\nage similarity is calculated as follows:\\nPrecision = 1\\nN\\nN\\nX\\ni=1\\nmax\\nj=1,...,M sim(xemb\\ni\\n, yemb\\nj\\n)\\n(5.7)\\nRecall = 1\\nM\\nM\\nX\\nj=1\\nmax\\ni=1,...,N sim(yemb\\nj\\n, xemb\\ni\\n)\\n(5.8)\\nFinally, the F-measure score is computed by combining precision and recall (Equa-\\ntion 5.3).\\nThe metrics are calculated at the user story level but are averaged across the backlog\\nto provide a broader perspective. This approach ensures that the evaluation considers\\nvariations in context and patterns across different backlogs.\\nIn conclusion, the evaluation script provides a comprehensive framework for assessing\\nthe accuracy of extracted knowledge graph components. It not only enables a systematic\\ncomparison of different approaches but also establishes a foundation for future evalua-\\ntions.\\n5.2 Experimental Setup\\nTo assess the accuracy of the USGT module, two experiments were set up. The difference\\nbetween the experiments lies on the LLM provider and its model chosen, the first is Llama\\n3 by Meta, an open-source, cost-free for research purposes and highly powerful model,\\nand the second is GPT-4o mini, the most cost-efficient small model by OpenAI. These\\nmodels were also chosen because Llama 3 does not support function calls, while GPT-\\n4o mini does support this functionality, therefore demonstrating the versatility of the\\nsolution to work with different LLM providers.\\nThe dataset employed for experimentation is the same as in the experiment of Arul-\\nmohan et al. [1] and presented in Chapter 3. Which means, a cleaned version of the\\nannotated dataset [49], representing 87% of the complete version, to ensure the results\\nof this thesis can be compared to their results.\\nFor the LLM configuration, both models, Llama 3 and GPT-4o mini, were set to a\\nzero temperature. This configuration parameter is necessary to control the stochasticity\\nof the output, making it more consistent and predictable, since the output of the LLM\\nis non-deterministic.\\nTo enable reproducibility and facilitate further experimentation with the USGT mod-\\nule, the accompanying repository is organized as follows:\\n• us_graph_transformer.py: A Python file containing the USGT module, includ-\\ning all its associated functions and components.\\n• pos_baseline/: A folder containing the annotated backlog dataset. This dataset\\nserves as the ground truth for evaluation.\\n53'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 53}, page_content='• extractor.py: The automated script for running the USGT. By default, it pro-\\ncesses the backlog files from the pos_baseline/ folder but can be configured to\\nextract data from alternative sources.\\n• extracted-user-stories/: A folder to store the user stories extracted by the\\nextractor.py. Each subfolder corresponds to a specific experiment, with results\\nsaved in JSON format.\\n• template.json: A template JSON file demonstrating how the extracted user sto-\\nries should be formatted for evaluation.\\n• evaluation.py: The automated script to evaluate the results of an experiment by\\ncomparing the extracted user stories to the pos_baseline (ground truth) dataset\\nusing the metrics defined on the previous section.\\n• evaluation/: A folder to store evaluation results, including detailed comparison\\nmetrics for each experiment.\\nTo replicate the experiments, readers can follow these steps:\\n1. Setup the environment: Clone the repository and install the required dependencies\\nlisted in requirements.txt.\\n2. Create a .env file: Create this file to store the LLM API key, if it is required by\\nthe LLM provider you chose, and the Neo4j connection parameters.\\n3. Prepare the experiment: In the extractor.py file, define the variables such as\\nexperiment name, and the LLM configuration. The two configurations explored in\\nthis thesis are pre-defined and can be uncommented.\\n4. Run the experiment: Execute the extractor.py script. By default, the script\\nprocesses data in pos_baseline/ folder. Modify the source in the script if using\\nother datasets.\\n5. Evaluate the results: Compare the extracted user stories with the ground truth\\nusing the evaluation.py script. Evaluation metrics are saved in the evaluation/\\nfolder.\\nThe experiments involved configuring the LLMs and running the extractor.py script\\nto apply the USGT module to extract knowledge graphs from all the user stories within\\nthe 22 product backlogs, and then evaluating the results against the ground truth.\\n5.3 Results\\nThis section presents the outcomes of the experiments conducted to evaluate the perfor-\\nmance of the USGT module in extracting nodes according to the ontology defined. The\\nresults are analyzed to assess the accuracy, consistency, and adaptability of the module\\n54'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 54}, page_content='Backlog Name\\nPersona F-Measure\\nEntity F-Measure\\nAction F-Measure\\nBenefit F-Measure\\nGPT-4o-mini Llama 3 GPT-4o-mini Llama 3 GPT-4o-mini Llama 3 GPT-4o-mini Llama 3\\ng02\\n1.00\\n0.99\\n0.82\\n0.59\\n0.73\\n0.69\\n0.78\\n0.43\\ng03\\n1.00\\n0.96\\n0.82\\n0.53\\n0.81\\n0.43\\n0.93\\n0.95\\ng04\\n0.98\\n0.93\\n0.76\\n0.49\\n0.74\\n0.61\\n0.88\\n0.94\\ng05\\n1.00\\n1.00\\n0.79\\n0.36\\n0.82\\n0.51\\n0.85\\n0.75\\ng08\\n1.00\\n0.96\\n0.85\\n0.54\\n0.57\\n0.43\\n0.80\\n0.97\\ng10\\n1.00\\n0.97\\n0.74\\n0.47\\n0.73\\n0.60\\n0.80\\n0.81\\ng11\\n1.00\\n0.99\\n0.81\\n0.53\\n0.72\\n0.58\\n0.90\\n0.94\\ng12\\n1.00\\n0.95\\n0.73\\n0.42\\n0.76\\n0.66\\n0.73\\n0.82\\ng13\\n1.00\\n0.98\\n0.65\\n0.44\\n0.57\\n0.51\\n0.78\\n0.71\\ng14\\n1.00\\n0.98\\n0.73\\n0.48\\n0.73\\n0.62\\n0.80\\n0.78\\ng16\\n1.00\\n1.00\\n0.95\\n0.67\\n0.31\\n0.60\\n1.00\\n1.00\\ng17\\n1.00\\n1.00\\n0.72\\n0.36\\n0.70\\n0.72\\n0.98\\n0.98\\ng18\\n1.00\\n0.99\\n0.81\\n0.57\\n0.74\\n0.67\\n0.89\\n0.91\\ng19\\n1.00\\n1.00\\n0.83\\n0.55\\n0.80\\n0.66\\n0.93\\n0.77\\ng21\\n1.00\\n0.78\\n0.79\\n0.41\\n0.72\\n0.51\\n0.71\\n0.78\\ng22\\n1.00\\n0.97\\n0.72\\n0.32\\n0.70\\n0.53\\n0.91\\n0.99\\ng23\\n1.00\\n0.99\\n0.85\\n0.75\\n0.91\\n0.90\\n1.00\\n0.98\\ng24\\n0.98\\n0.88\\n0.82\\n0.50\\n0.68\\n0.55\\n0.63\\n0.87\\ng25\\n1.00\\n0.99\\n0.81\\n0.49\\n0.87\\n0.85\\n0.96\\n0.93\\ng26\\n1.00\\n0.97\\n0.78\\n0.49\\n0.78\\n0.63\\n0.82\\n0.82\\ng27\\n1.00\\n0.93\\n0.70\\n0.43\\n0.69\\n0.55\\n0.70\\n0.80\\ng28\\n1.00\\n1.00\\n0.81\\n0.61\\n0.86\\n0.74\\n0.98\\n0.90\\nAverage\\n1.00\\n0.96\\n0.79\\n0.50\\n0.73\\n0.62\\n0.85\\n0.85\\nTable 5.1: Strict Comparison of F-Measures for GPT-4o-mini and Llama 3.\\nwhen working with different LLM providers. Specifically, the outputs of Llama 3 and\\nGPT-4o mini are compared.\\nBased on the previously described evaluation methods, Table 5.1 presents results in\\nthe strict mode, Table 5.2 evaluates the inclusive mode, Table 5.3 evaluates the relaxed\\nmode, and, lastly, Table 5.4 presents a comparison using BertScore.\\nIn the strict mode comparison (Table 5.1), GPT-4o-mini outperforms Llama 3 in\\nextracting all node types except for benefit, where Llama 3 shows a slightly higher per-\\nformance. On average, Llama 3 struggles primarily with entity and action extraction in\\nthis mode. A detailed analysis of the data reveals that Llama 3 frequently mishandles\\nnoun qualifiers and verb complements, both of which are critical in this strict mode.\\nAdditionally, a significant performance gap is observed in backlog g02 for benefit extrac-\\ntion. Upon closer inspection, many user stories in this backlog did not have a benefit to\\nextract, yet Llama 3 either attempted to extract other parts from the story or produced\\nhallucinations as the benefit node.\\nMoving to the inclusive and (Table 5.2) relaxed (Table 5.3) modes, both models show\\nimproved scores across all categories as expected. However, GPT-4o-mini continues to\\nlead with a more consistent performance across all categories, maintaining its edge in\\nbenefit extraction in the inclusive mode.\\nThe BertScore (Table 5.4) comparison mode brings the semantic alignment perspec-\\n55'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 55}, page_content='Backlog Name\\nPersona F-Measure\\nEntity F-Measure\\nAction F-Measure\\nBenefit F-Measure\\nGPT-4o-mini Llama 3 GPT-4o-mini Llama 3 GPT-4o-mini Llama 3 GPT-4o-mini Llama 3\\ng02\\n1.00\\n0.99\\n0.82\\n0.59\\n0.74\\n0.69\\n0.78\\n0.43\\ng03\\n1.00\\n0.96\\n0.82\\n0.53\\n0.81\\n0.43\\n0.93\\n0.95\\ng04\\n0.98\\n0.93\\n0.77\\n0.49\\n0.75\\n0.61\\n0.88\\n0.94\\ng05\\n1.00\\n1.00\\n0.79\\n0.37\\n0.82\\n0.52\\n0.85\\n0.75\\ng08\\n1.00\\n0.96\\n0.86\\n0.55\\n0.57\\n0.43\\n0.80\\n0.97\\ng10\\n1.00\\n0.97\\n0.75\\n0.47\\n0.75\\n0.60\\n0.80\\n0.81\\ng11\\n1.00\\n0.99\\n0.81\\n0.53\\n0.73\\n0.59\\n0.90\\n0.94\\ng12\\n1.00\\n0.95\\n0.75\\n0.45\\n0.77\\n0.66\\n0.76\\n0.84\\ng13\\n1.00\\n0.98\\n0.65\\n0.44\\n0.63\\n0.53\\n0.78\\n0.71\\ng14\\n1.00\\n0.98\\n0.74\\n0.48\\n0.74\\n0.63\\n0.80\\n0.78\\ng16\\n1.00\\n1.00\\n0.95\\n0.67\\n0.46\\n0.60\\n1.00\\n1.00\\ng17\\n1.00\\n1.00\\n0.73\\n0.37\\n0.77\\n0.72\\n0.98\\n0.98\\ng18\\n1.00\\n0.99\\n0.81\\n0.58\\n0.79\\n0.67\\n0.89\\n0.92\\ng19\\n1.00\\n1.00\\n0.83\\n0.55\\n0.81\\n0.66\\n0.93\\n0.77\\ng21\\n1.00\\n0.78\\n0.79\\n0.42\\n0.73\\n0.51\\n0.72\\n0.78\\ng22\\n1.00\\n0.97\\n0.72\\n0.32\\n0.75\\n0.54\\n0.92\\n0.99\\ng23\\n1.00\\n0.99\\n0.86\\n0.77\\n0.91\\n0.90\\n1.00\\n0.98\\ng24\\n0.98\\n0.88\\n0.82\\n0.50\\n0.74\\n0.55\\n0.63\\n0.88\\ng25\\n1.00\\n0.99\\n0.81\\n0.49\\n0.88\\n0.85\\n0.96\\n0.93\\ng26\\n1.00\\n0.97\\n0.79\\n0.49\\n0.80\\n0.65\\n0.84\\n0.82\\ng27\\n1.00\\n0.93\\n0.74\\n0.46\\n0.72\\n0.56\\n0.70\\n0.80\\ng28\\n1.00\\n1.00\\n0.81\\n0.62\\n0.87\\n0.75\\n0.98\\n0.90\\nAverage\\n1.00\\n0.96\\n0.79\\n0.51\\n0.75\\n0.62\\n0.86\\n0.86\\nTable 5.2: Inclusive Comparison of F-Measures for GPT-4o-mini and Llama 3.\\n56'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 56}, page_content='Backlog Name\\nPersona F-Measure\\nEntity F-Measure\\nAction F-Measure\\nGPT-4o-mini\\nLlama 3\\nGPT-4o-mini\\nLlama 3\\nGPT-4o-mini\\nLlama 3\\ng02\\n1.00\\n0.99\\n0.89\\n0.64\\n0.79\\n0.73\\ng03\\n1.00\\n0.96\\n0.86\\n0.60\\n0.85\\n0.53\\ng04\\n0.98\\n0.93\\n0.84\\n0.55\\n0.80\\n0.64\\ng05\\n1.00\\n1.00\\n0.86\\n0.40\\n0.85\\n0.59\\ng08\\n1.00\\n0.96\\n0.89\\n0.56\\n0.62\\n0.49\\ng10\\n1.00\\n0.97\\n0.82\\n0.53\\n0.81\\n0.68\\ng11\\n1.00\\n0.99\\n0.87\\n0.58\\n0.77\\n0.64\\ng12\\n1.00\\n0.95\\n0.80\\n0.48\\n0.84\\n0.74\\ng13\\n1.00\\n0.98\\n0.73\\n0.49\\n0.67\\n0.60\\ng14\\n1.00\\n0.98\\n0.82\\n0.55\\n0.76\\n0.68\\ng16\\n1.00\\n1.00\\n0.95\\n0.67\\n0.31\\n0.60\\ng17\\n1.00\\n1.00\\n0.81\\n0.41\\n0.72\\n0.72\\ng18\\n1.00\\n0.99\\n0.85\\n0.60\\n0.80\\n0.71\\ng19\\n1.00\\n1.00\\n0.88\\n0.58\\n0.82\\n0.69\\ng21\\n1.00\\n0.78\\n0.84\\n0.47\\n0.79\\n0.58\\ng22\\n1.00\\n0.98\\n0.83\\n0.36\\n0.73\\n0.65\\ng23\\n1.00\\n0.99\\n0.89\\n0.79\\n0.96\\n0.92\\ng24\\n0.98\\n0.88\\n0.86\\n0.52\\n0.74\\n0.61\\ng25\\n1.00\\n0.99\\n0.92\\n0.58\\n0.87\\n0.85\\ng26\\n1.00\\n0.98\\n0.84\\n0.53\\n0.85\\n0.73\\ng27\\n1.00\\n0.93\\n0.76\\n0.47\\n0.76\\n0.61\\ng28\\n1.00\\n1.00\\n0.88\\n0.63\\n0.89\\n0.76\\nAverage\\n1.00\\n0.97\\n0.85\\n0.54\\n0.77\\n0.67\\nTable 5.3: Relaxed Comparison of F-Measures for GPT-4o-mini and Llama 3.\\n57'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 57}, page_content='Backlog Name\\nPersona F-Measure\\nEntity F-Measure\\nAction F-Measure\\nBenefit F-Measure\\nGPT-4o-mini Llama 3 GPT-4o-mini Llama 3 GPT-4o-mini Llama 3 GPT-4o-mini Llama 3\\ng02\\n1.00\\n0.99\\n0.95\\n0.87\\n0.85\\n0.79\\n0.92\\n0.55\\ng03\\n1.00\\n0.99\\n0.93\\n0.79\\n0.88\\n0.74\\n0.98\\n1.00\\ng04\\n1.00\\n0.99\\n0.94\\n0.84\\n0.89\\n0.82\\n0.98\\n1.00\\ng05\\n1.00\\n1.00\\n0.94\\n0.75\\n0.90\\n0.78\\n0.96\\n1.00\\ng08\\n1.00\\n0.97\\n0.94\\n0.77\\n0.83\\n0.82\\n0.94\\n1.00\\ng10\\n1.00\\n0.98\\n0.92\\n0.85\\n0.88\\n0.79\\n0.97\\n1.00\\ng11\\n1.00\\n0.99\\n0.93\\n0.84\\n0.88\\n0.72\\n0.97\\n1.00\\ng12\\n1.00\\n0.97\\n0.93\\n0.86\\n0.86\\n0.84\\n0.98\\n0.96\\ng13\\n1.00\\n0.99\\n0.88\\n0.86\\n0.81\\n0.74\\n1.00\\n1.00\\ng14\\n1.00\\n0.98\\n0.91\\n0.86\\n0.87\\n0.79\\n0.97\\n1.00\\ng16\\n1.00\\n1.00\\n0.93\\n0.87\\n0.72\\n0.67\\n1.00\\n1.00\\ng17\\n1.00\\n1.00\\n0.91\\n0.79\\n0.86\\n0.81\\n1.00\\n0.98\\ng18\\n1.00\\n1.00\\n0.93\\n0.83\\n0.89\\n0.79\\n0.97\\n0.93\\ng19\\n1.00\\n1.00\\n0.93\\n0.87\\n0.87\\n0.78\\n0.96\\n0.78\\ng21\\n1.00\\n0.98\\n0.91\\n0.81\\n0.84\\n0.77\\n0.96\\n1.00\\ng22\\n1.00\\n0.99\\n0.94\\n0.81\\n0.82\\n0.76\\n0.99\\n1.00\\ng23\\n1.00\\n0.99\\n0.97\\n0.96\\n0.97\\n0.93\\n1.00\\n0.98\\ng24\\n1.00\\n0.99\\n0.92\\n0.78\\n0.87\\n0.81\\n0.96\\n1.00\\ng25\\n1.00\\n1.00\\n0.95\\n0.86\\n0.92\\n0.90\\n0.96\\n0.93\\ng26\\n1.00\\n0.99\\n0.92\\n0.87\\n0.92\\n0.85\\n0.99\\n1.00\\ng27\\n1.00\\n0.99\\n0.91\\n0.79\\n0.85\\n0.78\\n0.84\\n0.83\\ng28\\n1.00\\n1.00\\n0.94\\n0.86\\n0.93\\n0.85\\n0.98\\n0.88\\nAverage\\n1.00\\n0.99\\n0.93\\n0.84\\n0.87\\n0.80\\n0.97\\n0.95\\nTable 5.4: BertScore Comparison of F-Measures for GPT-4o-mini and Llama 3.\\n58'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 58}, page_content='Figure 5.1: Average comparison of node types extraction using GPT-4o-mini model.\\nFigure 5.2: Average comparison of node types extraction using LLama3.\\ntive, and in this case both models show a good performance.\\nOverall, GPT-4o-mini (Figure 5.1) demonstrates superior performance across all eval-\\nuated categories, with consistently higher F-Measure scores. Even though it doesn’t have\\na perfect F-measure when comparing exact string matching, it proves capable in captur-\\ning semantic alignment, particularly in the benefit node, which poses a great challenge\\nsince it has many elements to be extracted. Llama 3 (Figure 5.2), while competitive,\\nexhibits greater variability.\\n5.4 Discussion\\nThis study aimed to investigate the effectiveness of the USGT module in extracting\\nknowledge graphs from user stories. Specifically, to compare the performance of the\\nproposed approach to existing methods, explore the feasibility of a generalized approach\\nacross different LLMs, and assess the potential for a fully automated solution. In this sec-\\n59'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 59}, page_content='tion, the results will be discussed, a comparison to the previous experiment is presented\\nand the research questions are answered.\\nRQ1.How does the accuracy of nodes and relationships extraction of the pro-\\nposed solution compare to the existing Large Language Model-based method [1]?\\nTo answer the first research question it is necessary to perform an equivalent experi-\\nment to the one presented by Arulmohan et al. [1]. The existing solution applied GPT-3.5\\nmodel by OpenAI to extract the nodes and relationships of user stories. However, as of\\nNovember 2024, this model had been deprecated, and is no longer available for use in\\nthe experiment proposed by this thesis.\\nFortunately, the results from the same research group as in [1] can be found on their\\nonline repository [39]. These results applied the same techniques described in the pub-\\nlished paper [1], but with different GPT models, including GPT-4-turbo. This access\\nenabled a comparison between the solution proposed in this thesis and the existing ap-\\nproach using GPT-4-0125-preview (also referred to as GPT-4-turbo), as well as the CRF\\nmethod, as summarized in Table 5.5. The experiment was conducted using the same\\ndataset and a zero temperature configuration on the LLM.\\nBacklog Name\\nBenchmark GPT-4-turbo\\nGPT-4-turbo\\nCRF\\nP\\nE\\nA\\nP\\nE\\nA\\nP\\nE\\nA\\ng02\\n1.00\\n0.67\\n0.66\\n1.00\\n0.84\\n0.76\\n1.00\\n0.77\\n0.83\\ng03\\n1.00\\n0.76\\n0.81\\n0.84\\n0.85\\n0.89\\n1.00\\n0.82\\n0.90\\ng04\\n0.94\\n0.69\\n0.71\\n1.00\\n0.80\\n0.82\\n1.00\\n0.77\\n0.83\\ng05\\n1.00\\n0.69\\n0.66\\n1.00\\n0.83\\n0.82\\n1.00\\n0.85\\n0.86\\ng08\\n1.00\\n0.73\\n0.74\\n1.00\\n0.84\\n0.64\\n1.00\\n0.97\\n0.68\\ng10\\n0.98\\n0.62\\n0.71\\n1.00\\n0.73\\n0.75\\n1.00\\n0.81\\n0.76\\ng11\\n1.00\\n0.69\\n0.79\\n1.00\\n0.81\\n0.78\\n1.00\\n0.84\\n0.81\\ng12\\n1.00\\n0.64\\n0.69\\n1.00\\n0.73\\n0.75\\n1.00\\n0.72\\n0.73\\ng13\\n1.00\\n0.56\\n0.60\\n1.00\\n0.70\\n0.65\\n1.00\\n0.63\\n0.60\\ng14\\n1.00\\n0.63\\n0.71\\n1.00\\n0.76\\n0.76\\n1.00\\n0.76\\n0.84\\ng17\\n1.00\\n0.68\\n0.78\\n1.00\\n0.78\\n0.83\\n1.00\\n0.83\\n0.80\\ng18\\n1.00\\n0.73\\n0.69\\n1.00\\n0.82\\n0.78\\n1.00\\n0.73\\n0.84\\ng19\\n1.00\\n0.79\\n0.74\\n1.00\\n0.85\\n0.86\\n1.00\\n0.88\\n0.88\\ng21\\n0.84\\n0.65\\n0.67\\n1.00\\n0.73\\n0.73\\n1.00\\n0.82\\n0.77\\ng22\\n1.00\\n0.59\\n0.78\\n0.99\\n0.74\\n0.80\\n1.00\\n0.78\\n0.88\\ng23\\n1.00\\n0.81\\n0.87\\n1.00\\n0.86\\n0.89\\n1.00\\n0.88\\n0.97\\ng24\\n0.94\\n0.64\\n0.65\\n1.00\\n0.83\\n0.73\\n1.00\\n0.82\\n0.79\\ng25\\n1.00\\n0.79\\n0.84\\n1.00\\n0.85\\n0.91\\n1.00\\n0.91\\n0.94\\ng26\\n1.00\\n0.64\\n0.71\\n1.00\\n0.77\\n0.79\\n1.00\\n0.81\\n0.91\\ng27\\n0.98\\n0.58\\n0.67\\n0.99\\n0.72\\n0.73\\n1.00\\n0.76\\n0.82\\ng28\\n1.00\\n0.81\\n0.84\\n1.00\\n0.87\\n0.86\\n1.00\\n0.75\\n0.88\\nAverage\\n0.98\\n0.69\\n0.73\\n0.99\\n0.80\\n0.79\\n1.00\\n0.81\\n0.82\\nTable 5.5: Strict Comparison of F-Measures for GPT-4-turbo from Benchmark, GPT-4-\\nturbo using USGT module, and CRF.\\nWhen comparing results, the USGT module demonstrated superior performance on\\naverage across all node types. Notably, entity extraction improved by 16% on average\\n60'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 60}, page_content='compared to the benchmark. However, there were exceptions, such as persona extraction\\nfor backlog g03, which performed poorly compared to the benchmark. This discrepancy\\narose from the complexity of the persona in this backlog, which included multiple ele-\\nments such as role and specific context or function (e.g., Planning Staff Member).\\nThe differences in results are partly attributable to internal changes in the LLMs over\\ntime, as the experiments were conducted at different moments. However, a significant\\nfactor was the variation in prompt designs. While both experiments followed OpenAI’s\\nprompt guidelines [44], the implementation of the guideline’s strategies differed substan-\\ntially, the wording, phrase construction and instructions to the LLM are different and\\nthis is a common challenge when working with this technology.\\nThe earlier study also employed CRF, a tailored machine-learning approach trained\\non 20% of the dataset.\\nThis method outperformed all GPT models and the Visual\\nNarrator technique (details in Chapter 3). When comparing CRF to GPT-4-turbo using\\nUSGT, CRF still leads in performance; however, the gap has narrowed under the same\\nevaluation metrics, making LLM-based solutions a competitive alternative. To determine\\nthe most assertive approach, it is necessary to consider specific use case and context. For\\ntasks that require constant updates, flexibility, or are part of larger, dynamic systems,\\nLLMs may offer advantages. On the other hand, for applications where efficiency, quick\\ndeployment, and low computational cost are essential, CRF might be a better choice.\\nThe previous work also mentioned the challenges when depending on a specific hosting\\nprovider, including technical changes affecting API interaction and data processing, as\\nwell as pricing, and model availability.\\nIn contrast, this thesis proposes a versatile, model-agnostic solution, as demonstrated\\nby experiments using models from two different providers.\\nThis approach addresses\\nprior limitations by enabling the selection of models and providers based on specific\\ncontexts and constraints. For instance, in a budget-constrained scenario, an open-source\\nLLM provider could be used, while in experimental contexts, the solution facilitates\\ncomparison across different models and providers.\\nThis study aimed to investigate the effectiveness of the USGT module in extracting\\nknowledge graphs from user stories. Specifically, to compare the performance of the\\nproposed approach to existing methods, explore the feasibility of a generalized approach\\nacross different LLMs, and assess the potential for a fully automated solution.\\nThe results previously reported show that the solution is able to successfully extract\\nnodes and relationships from user stories according to the specific ontology proposed,\\nhowever the adherence to the ontology, and therefore, the quality of the results depends\\non the chosen large language model.\\nRQ2.\\nHow can a knowledge graph be automatically generated from user\\nstories using large language models without being dependent on a specific\\nprovider?\\nThe second research question directly addresses the versatility of the solution. By\\nusing the LangChain framework, a unified module is created, that can integrate with\\nvarious LLM providers. This abstraction layer allows the module to easily configure and\\n61'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 61}, page_content='use a new language model, without modifying the core logic of it. This flexibility not\\nonly accelerates development but also enables us to explore the strengths and weaknesses\\nof different models to optimize performance.\\nRQ3. What strategies can be employed to build a fully automated solution to\\nextract knowledge graph from user stories?\\nPrevious research [1] demonstrated that large language models (LLMs), despite certain\\nlimitations in accuracy, have the capability to extract the core components of a knowledge\\ngraph—namely, nodes and relationships—from user stories. These extracted components\\nhold significant potential as tools to enhance the requirements engineering life cycle.\\nHowever, the prior work primarily focused on the extraction process and did not address\\nthe practical implementation of a fully realized knowledge graph. Traditional methods\\nfor constructing knowledge graphs often involve substantial complexity and effort. To\\nbridge this gap, this thesis proposes a comprehensive solution that not only extracts the\\nknowledge graph components but also implements them in an automated and streamlined\\nmanner.\\nTo achieve a complete degree of automation, the solution proposed relied on LangChain\\nin these aspects:\\n• Defining a prompt template: the prompt template is defined in advance and has a\\nplaceholder that dynamically adds the input user story to it, without the necessity\\nof manual adjustments each time a new user story is received.\\n• Interacting with API via chains: through LCEL it is possible to define chains to\\nstandardize the API interaction. This eliminates the need for manual API calls and\\ndata formatting at each step, automating the process of feeding inputs to LLMs\\nand processing the outputs.\\n• Using pre-built modules: LangChain offers a rich set of ready-to-use modules that\\nfacilitate various tasks such as configuring LLMs, performing data transformations,\\nand interacting with external databases (e.g., Neo4j). These modules help stream-\\nline the entire process, from model configuration to data storage, and therefore,\\nsignificantly reduce implementation complexity.\\nThis approach not only automates the extraction and storage of knowledge graphs but\\nalso enables the system to be easily extended or modified by simply adjusting the relevant\\nLangChain components, making it highly adaptable for future use cases or updates.\\n62'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 62}, page_content='6 State of the Art\\nWhile user stories are a popular tool in Requirements Engineering (RE) for capturing\\nuser needs, extracting and structuring the information they contain can be a challenge.\\nThis chapter explores how recent advancements in two key areas - Knowledge Graphs\\n(KGs) and Large Language Models (LLMs) - can offer new solutions for RE, particularly\\nwhen focused on user stories, as well as how LangChain is being used in research. We\\nwill delve into existing research on these three pillars and their potential intersections,\\nexamining how they can be leveraged to enhance the clarity, structure, and overall utility\\nof modeled user requirements.\\n6.1 Modeling User Stories using NLP\\nModeling user stories can be understood as part of domain modeling [6], which involves\\nidentifying essential business and application entities and their relationships. NLP is\\nthe most common technique to model user stories. The study by Raharjana et al. [47]\\nidentifies eight approaches centered on modeling, which can be broadly categorized into\\nthree groups based on their methodologies and objectives [38]:\\n• Conceptual Modelling with Visual Narrator Tool: Three of the identified ap-\\nproaches utilize the Visual Narrator tool [48], developed by the Requirements En-\\ngineering Lab at Utrecht University, to extract conceptual models, a textual tem-\\nplate to define a structured representation of user stories, from backlogs. This tool,\\nwhich creates ontology-like structures, is used differently to analyze user stories.\\nThe first approach employs POS tagging to parse the user stories and construct\\nthe conceptual models Lucassen et al. [34]. Despite its utility, this method suffers\\nfrom low precision in accurately identifying parts of speech, which can lead to er-\\nrors in the resulting models. Similarly, the second approach by Lucassen et al. [35]\\nrelies on POS tagging but encounters additional difficulties with the correct identi-\\nfication of compound terms, impacting the overall results. The third approach by\\nDalpiaz et al. [17] diverges by utilizing a vector space model in conjunction with\\nmanual data tagging.\\n• Generation of UML Artifacts: Another subset of three approaches focuses on\\ntransforming user stories into various UML artifacts using POS tagging technique.\\nThis includes the creation of class diagrams by Nasiri et al. [42], sequence diagrams\\nby Elallaoui et al. [20], and use case diagrams [21], although useful for generating a\\nbasic overview of system interactions, this technique fails in handling the nuances\\nof more sophisticated systems, limiting its applicability in complex environments.\\n63'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 63}, page_content='• Ontology-Based Approaches: The final two approaches adopt an ontology-based\\nperspective [26] [3]. The first method by Genaid et al. [26] involves constructing an\\nontology directly from the source code and using NER in user stories with specific\\ncode locations. This approach automates traceability, ensuring that user stories\\nare accurately linked to their corresponding code segments, however, it presents\\na low precision. The second method by Athiththan et al. [3] models the backlog\\nitself as an ontology using POS tagging, which is then used to generate boilerplate\\ncode, thus speeding up the development process. Despite their innovative use of\\nontologies, these approaches do not fully utilize the graph structure inherent in\\nontologies.\\nDespite numerous studies using NLP, empirical evaluations demonstrating concrete\\neffectiveness are scarce [59]. In addition, these approaches are limited by their precision\\nand recall [47], therefore this thesis aims to improve these metrics.\\n6.1.1 Ontologies\\nA key aspect of domain modeling is ontology, a formal representation of domain model-\\ning. A user story modeling ontology was proposed by Mancuso et al. [36] which created\\na domain-specific modeling language and integrated it into the modeling tool AOAME\\nthat resulted in a visual user story. Ladeinde et al. [32] also proposed the use of KGs to\\nmodel user stories, by applying NLP techniques, and its ontology is based on role, goal,\\nand benefit.\\nWhile the ontologies proposed by Mancuso et al. and Ladeinde et al. offer valuable\\napproaches, Arulmohan et al.’s ontology is considered a more comprehensive and flexible\\nframework for this research, because the first [36] doesn’t have general node types which\\ndifficult the querying, while the second [32] extracts only three node types and doesn’t\\nstandardize the relationships types.\\n6.2 Large Language Models and Knowledge Graphs\\nLarge Language Models (LLMs) have shown potential for building knowledge graphs\\n(KGs) due to their natural language understanding capabilities. They excel at under-\\nstanding natural language, making them well-suited for two main tasks required for the\\nconstruction of a KG: concept extraction and relation extraction [31]. Concept extrac-\\ntion involves identifying key ideas and topics within the text. Relation extraction focuses\\non how these concepts are connected. To improve this process, techniques like NER and\\nadditional context should be added to the LLM.\\nPrevious work relied on prompt engineering to extract entities, such as PromptNER\\nby Ashok et al. [2] that proposes a solution based on four vital components: an LLM,\\na prompt to define and restrict the set of entity types, a concise collection of domain-\\nspecific examples, and instructions to specify the intended output format.\\nAn alternative approach by Khorashadizadeh et al. [31] is based on instruction-tuning,\\nin which the language models receive natural language instructions to guide the model’s\\n64'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 64}, page_content='response. In this direction, Zhou et al. [60] presented a target distillation approach with\\nmission-focused instruction tuning.\\nWhile prompt engineering and instruction tuning offer advancements, they struggle\\nwith user stories due to limited ontology awareness. These approaches propose a generic\\nentity extraction but lack the ability to understand the specific structure and constraints\\ndefined within a user story ontology. This leads to inaccurate or incomplete knowledge\\ngraphs, as extracted entities might not correspond to the ontology’s specifications. Ad-\\nditionally, current LLM-based methods often lack scalability, hindering their transition\\nto production-ready systems. The approach proposed in this thesis is tailored to capture\\nuser story nuances and benefits from the usage of a robust framework that supports\\nLLM systems scalability.\\n6.3 Large Language Models and Requirements Engineering\\nThere is a wide range of applications of LLMs in the domain of RE [28]: anaphoric am-\\nbiguity treatment, requirements classification, coreference detection, requirements elici-\\ntation, and software traceability.\\nResearch by White et al. [56] proposes prompt patterns that leverage LLMs, such as\\nChatGPT, to facilitate the elicitation and identification of missing requirements [2]. This\\napproach can help capture user needs more comprehensively during the initial stages of\\nsoftware development.\\nEndres et al. [22] explore the use of LLMs to formalize requirements from natural\\nlanguage intent. This holds promise for streamlining the transition from user stories to\\nformal specifications, improving clarity, and reducing ambiguity.\\nArulmohan et al. [1] investigated GPT-3.5’s potential for extracting domain models\\nfrom user stories. While they proposed a framework for transforming these concepts into\\na comprehensive domain representation, they did not address the specific challenge of\\nknowledge graph creation.\\nSubsequently, Bragilovski et al. [5] compared human, rule-based, GPT, and ML-based\\napproaches for deriving domain models. Their findings highlighted that human perfor-\\nmance still surpasses AI-based methods, indicating a need for further advancements in\\nAI techniques.\\nCheng et al. [11] explored the application of generative AI in requirements engineering,\\nnoting the widespread use of GPT models in this field.\\nThis thesis solution further\\ncontributes to this domain by offering a versatile model solution that expands upon\\nexisting research.\\nThere remains a notable absence of research on employing LLMs for requirements engi-\\nneering and design purposes. The literature review by Hou et al. [28] spanning 2017-2024\\ndemonstrated that only 3.9% of the research in the application of LLMs in software en-\\ngineering was focused on requirements engineering. Additionally, Fan et al.[23] highlight\\nthat practitioners are hesitant to rely on LLMs for higher-level design goals. This hesi-\\ntancy underscores the need for further research to build trust and demonstrate the value\\nproposition of LLMs in RE.\\n65'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 65}, page_content='6.4 LangChain Applications\\nWhile LangChain has proven its value in various applications, its direct application to\\nuser story modeling and knowledge graph generation within the realm of requirements\\nengineering remains relatively unexplored. A comprehensive literature review revealed a\\nlack of research specifically addressing the integration of LangChain into these domains.\\nThis scarcity presents a promising path for original research and innovation.\\nLangChain has been used to build sophisticated question-answering systems that can\\nprovide informative and accurate responses to user queries. Jeong’s work [30] developed\\nan advanced Retrieval-Augmented Generation (RAG) system based on graph technology\\nusing LangChain, which resulted in increasing accuracy and relevance of the responses\\nprovided by the system to the users.\\nAnother application of this framework is in the integration of chatbots and virtual\\nassistants. Singh et al. [51] explored the application of LLMs through LangChain to in-\\ntroduce MindGuide, a chatbot to assist individuals looking for guidance regarding mental\\nhealth. They incorporated features such as ChatPrompt Template and LLMChain to\\nrapidly prototype and streamline the orchestration of the LLM in their application.\\nHowever, to fully leverage LangChain’s potential, it is essential to extend its applica-\\ntion to the critical domain of requirements engineering. By exploring its utility in user\\nstory modeling and knowledge graph generation, this research aims to contribute to the\\nadvancement of the field and provide novel solutions to existing challenges.\\n66'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 66}, page_content='7 Conclusion\\nIn this chapter, we conclude the work presented in this thesis.\\nSection 7.1 restates\\nthe research objectives and summarizes the key findings.\\nSection 7.2 highlights the\\ncontributions and the impact of the results. Section 7.3 discusses the limitations of the\\nstudy, and finally, Section 7.4 explores potential paths for future research.\\n7.1 Summary\\nThis thesis, motivated to improve the requirements engineering life-cycle, aimed to inves-\\ntigate the effectiveness of the USGT module in extracting knowledge graphs from user\\nstories. Specifically, we sought to compare the performance of our approach to existing\\nmethods, explore the feasibility of a generalized approach across different LLMs, and\\nassess the potential for a fully automated solution.\\nThe potential of the LangChain framework was explored to simplify the development of\\nLLM-based solutions for knowledge graph generation. And, to facilitate reproducibility\\nand comparison with existing methods, a reusable evaluation script was developed to\\nassess the performance of the USGT module against a predefined ground truth, which\\ncan be used in the future for evaluation of new methods.\\nThrough a comprehensive evaluation, the USGT module demonstrated superior per-\\nformance compared to the existing LLM-based method, however still not better than\\na pre-trained method, such as CRF particularly in terms of F-measure. The flexibil-\\nity of the LangChain framework was also highlighted, enabling the seamless integration\\nof different LLMs and facilitating the automation of the extraction process, therefore\\naddressing a previous limitation.\\n7.2 Contributions\\nThis thesis makes significant contributions to the field of automated knowledge graph\\ngeneration and its application within requirements engineering. First, it introduces the\\nUSGT Python module, a novel tool designed to extract nodes and relationships from\\nuser stories and transform them into a knowledge graph format. This module builds\\non existing methodologies, particularly benefiting from the strengths of LLMs and the\\nLangChain framework, to provide an automated approach for knowledge graph creation.\\nAdditionally, this research proposes a practical solution tailored to the requirements\\nengineering domain, offering practitioners a method to integrate the USGT module to\\nautomate the generation of knowledge graphs and store it in a graph database.\\n67'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 67}, page_content='A further contribution lies in the development of a robust evaluation script, which\\nextends the evaluation criteria established by Arulmohan et al. [1]. This script introduces\\na new evaluation metric, BERTScore, to assess semantic similarity between extracted\\nand ground truth components and expands the evaluation to include an additional node\\ntype, the benefit node. The evaluation script not only supports reproducibility of the\\nexperiments presented in this study but also serves as a foundation for future research\\nand experimentation in the field.\\nMoreover, this thesis conducts a comprehensive evaluation of the USGT module’s\\nperformance. A comparative assessment with prior work highlights improvements and\\nareas for further refinement, demonstrating the value of the proposed approach.\\nThe research also contributes to the exploration of the development of LLM-based\\napplications, specifically in the context of knowledge graph construction. It showcases\\nthe potential of the LangChain framework in simplifying the integration of LLMs into\\nworkflows.\\nThrough these contributions, the thesis aims to enhance the requirements engineer-\\ning life-cycle by promoting transparency among stakeholders and development teams.\\nBy maintaining a manageable product backlog stored in a knowledge graph format, it\\ncan enable the identification and analysis of dependencies between requirements, sup-\\nporting continuous improvement and fostering a more assertive and structured software\\ndevelopment process.\\n7.3 Limitations and Threats to Validity\\nWhile the proposed USGT module has shown promising results, it is important to ac-\\nknowledge its limitations.\\nThe limitations categorized based on their impact on the\\ndifferent types of threats to validity, including construct, internal, external, and conclu-\\nsion validity.\\nOne limitation concerns the evaluation against the ground truth. Although the dataset\\nwas annotated through a rigorous process, the exact annotation of node elements remains\\nsomewhat subjective. Different interpretations by scientists, requirements analysts, or\\nengineers could lead to variations in the annotations, which may affect the accuracy of\\nthe evaluation. This subjectivity poses a threat to construct validity, as the evaluation\\nmight not consistently reflect the true quality or correctness of the extracted knowledge\\ngraph components.\\nAnother limitation is the non-deterministic nature of LLM outputs. Even with model\\nparameters such as temperature set to zero, responses from the LLM may vary across\\nexecutions. This variability impacts internal validity, as it can lead to inconsistent ex-\\nperimental results, making it difficult to attribute outcomes solely to the factors being\\ntested. Moreover, it affects external validity, as the same approach might yield inconsis-\\ntent results when applied in different settings or with alternative datasets.\\nAlthough the USGT module allows experimentation with LLMs from different providers,\\nthese models may exhibit biases or lack sufficient domain understanding. This limita-\\ntion poses a threat to external validity, as the findings may not generalize well to diverse\\n68'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 68}, page_content='domains or user story formats.\\nThe flexibility of the module to support multiple LLM providers is made possible\\nthrough LangChain, which simplifies implementation complexity. However, this reliance\\nintroduces a dependency on the framework itself, which could become problematic if\\nLangChain undergoes significant changes, becomes obsolete, or imposes constraints that\\nare incompatible with future requirements.\\nThe definition of the prompt plays a critical role in the performance of the model.\\nWhile the prompt is a key factor, its formulation remains highly subjective. Even though\\nguidelines exist [44], they are constantly changing, and it is difficult to determine with\\ncertainty whether a prompt is “right” or “wrong” which brings an element of uncertainty\\ninto the process. This introduces a threat to construct validity, as prompt design choices\\nmight inadvertently influence what is being measured.\\nAdditionally, the reliance on\\nsubjective prompt formulations may also pose a threat to internal validity.\\nFinally, while the proposed solution successfully automates the extraction and storage\\nof knowledge graphs in Neo4j, the study does not evaluate the degree of automation\\nin detail or assess the usability of the solution from a user perspective. This limitation\\nimpacts conclusion validity, as it leaves questions about the practicality and effectiveness\\nof the solution for end-users unanswered.\\nThe limitations outlined in this study may impact the interpretation and generaliz-\\nability of the results. However, acknowledging these limitations provides a transparent\\nevaluation of the work and highlights opportunities for future research, which will be\\ndiscussed in the following section.\\n7.4 Future Avenues\\nThis section presents potential paths for future research based on the findings and limi-\\ntations of this study.\\nAs presented in this study and in the state of the art, there are many methods and tools\\nto extract the core components of a knowledge graph-nodes and relationships. These\\ninclude methods such as the the Visual Narrator Tool, Conditional Random Fields,\\nChatGPT, and the USGT module. While these methods differ in performance, they\\noffer unique advantages depending on the specific context, such as implementation com-\\nplexity, reliability of results, dataset size, available resources, and long-term stability.\\nFuture research could involve a comprehensive comparative analysis of these methods,\\naccompanied by a practical guide to help practitioners choose the most suitable approach\\nbased on their specific requirements.\\nFocusing on LLMs methods for knowledge graph extraction, a recent work by Garbas et\\nal. [25] proposes a new tool, TransformerRanker, designed to evaluate language models\\nthat use a transformer architecture for classification tasks. This approach suggests a\\npromising direction for future research: integrating the language model versatility of\\nUSGT module with a component to identify the best language model to extract nodes\\nand relationships from a backlog dataset.\\nTo build trust among requirements analysts and engineers who could benefit from the\\n69'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 69}, page_content='proposed solution, future research could evaluate the degree of automation achieved by\\nthe USGT module. This includes quantifying the extent of automation across the entire\\nsolution and conducting usability testing to assess its practicality for end-users. Insights\\ngained from such evaluations could inform improvements in user experience and help\\nbridge the gap between academic research and real-world applications.\\n70'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 70}, page_content='Bibliography\\n[1] Sathurshan Arulmohan, Marie-Jean Meurs, and S´ebastien Mosser. Extracting do-\\nmain models from textual requirements in the era of large language models. In 2023\\nACM/IEEE International Conference on Model Driven Engineering Languages and\\nSystems Companion (MODELS-C), pages 580–587. IEEE, 2023.\\n[2] Dhananjay Ashok and Zachary C Lipton. Promptner: Prompting for named entity\\nrecognition. arXiv preprint arXiv:2305.15444, 2023.\\n[3] Kathirgamasegaran Athiththan, Selvaratnam Rovinsan, Srijeevahan Sathveegan,\\nNahanaa Gunasekaran, Kamila SAW Gunawardena, and Dharshana Kasthuri-\\nrathna. An ontology-based approach to automate the software development process.\\nIn 2018 ieee international conference on information and automation for sustain-\\nability (iciafs), pages 1–6. IEEE, 2018.\\n[4] Kent Beck, Mike Beedle, Arie Van Bennekum, Alistair Cockburn, Ward Cunning-\\nham, Martin Fowler, James Grenning, Jim Highsmith, Andrew Hunt, Ron Jeffries,\\net al. The agile manifesto, 2001.\\n[5] Maxim Bragilovski, Ashley T Van Can, Fabiano Dalpiaz, and Arnon Sturm. De-\\nriving domain models from user stories: Human vs. machines. In 2024 IEEE 32nd\\nInternational Requirements Engineering Conference (RE), pages 31–42. IEEE, 2024.\\n[6] Manfred Broy. Domain modeling and domain engineering: Key tasks in require-\\nments engineering. Perspectives on the Future of Software Engineering: Essays in\\nHonor of Dieter Rombach, pages 15–30, 2013.\\n[7] Erik Cambria and Bebo White. Jumping nlp curves: A review of natural language\\nprocessing research. IEEE Computational intelligence magazine, 9(2):48–57, 2014.\\n[8] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao\\nChen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of\\nlarge language models. ACM Transactions on Intelligent Systems and Technology,\\n2023.\\n[9] Harrison Chase. Langchain, 2022. https://github.com/langchain-ai/langchain. Ac-\\ncessed on 2024-10-10.\\n[10] Harrison Chase. Llmgraphtransformer, 2023.\\n71'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 71}, page_content='[11] Haowei Cheng, Jati H Husen, Sien Reeve Peralta, Bowen Jiang, Nobukazu Yosh-\\nioka, Naoyasu Ubayashi, and Hironori Washizaki. Generative ai for requirements\\nengineering: A systematic literature review. arXiv preprint arXiv:2409.06741, 2024.\\n[12] Clayton M Christensen, Taddy Hall, Karen Dillon, and David S Duncan. Know\\nyour customers’ jobs to be done. Harvard business review, 94(9):54–62, 2016.\\n[13] Mike Cohn. User stories applied: For agile software development. Addison-Wesley\\nProfessional, 2004.\\n[14] IEEE Computer Society. Software Engineering Technical Committee. IEEE Stan-\\ndard Glossary of Software Engineering Terminology, volume 729. IEEE, 1983.\\n[15] Thayn´a Camargo da Silva. Extracting knowledge graphs from user stories using\\nlangchain, 2024. https://zenodo.org/record/14254058.\\n[16] Fabiano Dalpiaz. Requirements data sets (user stories), July 2018. dataset.\\n[17] Fabiano Dalpiaz, Ivor Van Der Schalk, Sjaak Brinkkemper, Fatma Ba¸sak Aydemir,\\nand Garm Lucassen. Detecting terminological ambiguity in user stories: Tool and\\nexperimentation. Information and Software Technology, 110:3–16, 2019.\\n[18] Juan M Carrillo De Gea, Joaqu´ın Nicol´as, Jos´e L Fern´andez Alem´an, Ambrosio\\nToval, Christof Ebert, and Aurora Vizca´ıno. Requirements engineering tools: Capa-\\nbilities, survey and assessment. Information and Software Technology, 54(10):1142–\\n1157, 2012.\\n[19] Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language\\nunderstanding. arXiv preprint arXiv:1810.04805, 2018.\\n[20] Meryem Elallaoui, Khalid Nafil, and Raja Touahni. Automatic generation of uml se-\\nquence diagrams from user stories in scrum process. In 2015 10th international con-\\nference on intelligent systems: theories and applications (SITA), pages 1–6. IEEE,\\n2015.\\n[21] Meryem Elallaoui, Khalid Nafil, and Raja Touahni. Automatic transformation of\\nuser stories into uml use case diagrams using nlp techniques. Procedia computer\\nscience, 130:42–49, 2018.\\n[22] Madeline Endres, Sarah Fakhoury, Saikat Chakraborty, and Shuvendu K Lahiri.\\nFormalizing natural language intent into program specifications via large language\\nmodels. arXiv preprint arXiv:2310.01831, 2023.\\n[23] Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta,\\nShin Yoo, and Jie M Zhang. Large language models for software engineering: Survey\\nand open problems.\\nIn 2023 IEEE/ACM International Conference on Software\\nEngineering: Future of Software Engineering (ICSE-FoSE), pages 31–53. IEEE,\\n2023.\\n72'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 72}, page_content='[24] D Fensel, U Simsek, K Angele, E Huaman, E K¨arle, O Panasiuk, I Toma, J Umbrich,\\nand A Wahler. Knowledge graphs methodology, tools and selected use cases (2020).\\n[25] Lukas Garbas, Max Ploner, and Alan Akbik. Transformerranker: A tool for effi-\\nciently finding the best-suited language models for downstream classification tasks.\\narXiv preprint arXiv:2409.05997, 2024.\\n[26] Adrian Genaid et al. Connecting user stories and code for test development. In 2012\\nthird international workshop on recommendation systems for software engineering\\n(rsse), pages 33–37. IEEE, 2012.\\n[27] Robert L Glass. Facts and fallacies of software engineering. Addison-Wesley Pro-\\nfessional, 2002.\\n[28] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo,\\nDavid Lo, John Grundy, and Haoyu Wang. Large language models for software\\nengineering: A systematic literature review. arXiv preprint arXiv:2308.10620, 2023.\\n[29] Taojun Hu and Xiao-Hua Zhou. Unveiling llm evaluation focused on metrics: Chal-\\nlenges and solutions. arXiv preprint arXiv:2404.09135, 2024.\\n[30] Cheonsu Jeong. A study on the implementation method of an agent-based advanced\\nrag system using graph. arXiv preprint arXiv:2407.19994, 2024.\\n[31] Hanieh Khorashadizadeh, Fatima Zahra Amara, Morteza Ezzabady, Fr´ed´eric Ieng,\\nSanju Tiwari, Nandana Mihindukulasooriya, Jinghua Groppe, Soror Sahri, Farah\\nBenamara, and Sven Groppe. Research trends for the interplay between large lan-\\nguage models and knowledge graphs. arXiv preprint arXiv:2406.08223, 2024.\\n[32] Ayodeji Ladeinde, Chetan Arora, Hourieh Khalajzadeh, Tanjila Kanij, and John\\nGrundy. Extracting queryable knowledge graphs from user stories: An empirical\\nevaluation. In International Conference on Evaluation of Novel Approaches to Soft-\\nware Engineering 2023, pages 684–692. Scitepress, 2023.\\n[33] John Lafferty, Andrew McCallum, Fernando Pereira, et al. Conditional random\\nfields: Probabilistic models for segmenting and labeling sequence data. In Icml,\\nvolume 1, page 3. Williamstown, MA, 2001.\\n[34] Garm Lucassen, Fabiano Dalpiaz, Jan Martijn EM van der Werf, and Sjaak\\nBrinkkemper. Improving agile requirements: the quality user story framework and\\ntool. Requirements engineering, 21:383–403, 2016.\\n[35] Garm Lucassen, Marcel Robeer, Fabiano Dalpiaz, Jan Martijn EM Van Der Werf,\\nand Sjaak Brinkkemper. Extracting conceptual models from user stories with visual\\nnarrator. Requirements Engineering, 22:339–358, 2017.\\n[36] Marco Mancuso and Emanuele Laurenzi. An approach for knowledge graphs-based\\nuser stories in agile methodologies. In International Conference on Business Infor-\\nmatics Research, pages 133–141. Springer, 2023.\\n73'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 73}, page_content='[37] Michael McTear and Marina Ashurkina. Transforming conversational ai. 2024.\\n[38] S´ebastien Mosser, Corinne Pulgar, and Vladimir Reinhar. Modelling agile back-\\nlogs as composable artifacts to support developers and product owners. J. Object\\nTechnol., 21(3):3–1, 2022.\\n[39] S´ebastien Mosser and Sathurshan Arulmohan.\\nExtracting domain models from\\nuser stories- repository, 2023.\\nRepository available on: https://github.com/ace-\\ndesign/qualified-user-stories.\\n[40] Prakash M Nadkarni, Lucila Ohno-Machado, and Wendy W Chapman. Natural\\nlanguage processing: an introduction. Journal of the American Medical Informatics\\nAssociation, 18(5):544–551, 2011.\\n[41] Hiroki Nakayama, Takahiro Kubo, Junya Kamura, Yasufumi Taniguchi, and\\nXu Liang.\\ndoccano: Text annotation tool for human, 2018.\\nSoftware available\\nfrom https://github.com/doccano/doccano.\\n[42] Samia Nasiri, Yassine Rhazali, and Mohammed Lahmer. Towards a generation of\\nclass diagram from user stories in agile methods. In Advancements in Model-Driven\\nArchitecture in Software Engineering, pages 135–159. IGI Global, 2021.\\n[43] Neo4j.\\nNeo4j - the world’s leading graph database, 2024.\\nhttps://neo4j.com/.\\nAccessed on 2024-06-12.\\n[44] OpenAI. Prompt engineering, 2023. https://platform.openai.com/docs/guides/prompt-\\nengineering?ref=blef.fr. Accessed on 2024-09-07.\\n[45] OpenAI.\\nChatgpt:\\nOpenai\\nlanguage\\nmodel,\\n2024.\\nhttps://platform.openai.com/docs/models/gpt-4. Accessed on 2024-07-14.\\n[46] Klaus Pohl. Requirements engineering: An overview. Citeseer, 1996.\\n[47] Indra Kharisma Raharjana, Daniel Siahaan, and Chastine Fatichah. User stories\\nand natural language processing: A systematic literature review.\\nIEEE access,\\n9:53811–53826, 2021.\\n[48] Marcel Robeer, Garm Lucassen, Jan Martijn EM Van Der Werf, Fabiano Dalpiaz,\\nand Sjaak Brinkkemper.\\nAutomated extraction of conceptual models from user\\nstories via nlp. In 2016 IEEE 24th international requirements engineering conference\\n(RE), pages 196–205. IEEE, 2016.\\n[49] Sathurshan\\nArulmohan,\\nS´ebastien\\nMosser,\\nand\\nMarie-Jean\\nMeurs.\\nace-\\ndesign/qualified-user-stories: Version 1.0, July 2023. Annotated dataset.\\n[50] Ken Schwaber and Jeff Sutherland. The scrum guide. Scrum Alliance, 21(1):1–38,\\n2011.\\n74'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-16T00:01:09+00:00', 'source': 'data/pdf_files/langchain.pdf', 'file_path': 'data/pdf_files/langchain.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-16T00:01:09+00:00', 'trapped': '', 'modDate': 'D:20250616000109Z', 'creationDate': 'D:20250616000109Z', 'page': 74}, page_content='[51] Aditi Singh, Abul Ehtesham, Saifuddin Mahmud, and Jong-Hoon Kim. Revolu-\\ntionizing mental health care through langchain: A journey with a large language\\nmodel. In 2024 IEEE 14th Annual Computing and Communication Workshop and\\nConference (CCWC), pages 0073–0078. IEEE, 2024.\\n[52] Ian Sommerville.\\nSoftware engineering (ed.). America: Pearson Education Inc,\\n2011.\\n[53] Steffen Staab and Rudi Studer. Handbook on ontologies. Springer Science & Business\\nMedia, 2013.\\n[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N Gomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.\\nAdvances in neural information processing systems, 30, 2017.\\n[55] Mo Wang, Minjuan Wang, Xin Xu, Lanqing Yang, Dunbo Cai, and Minghao Yin.\\nUnleashing chatgpt’s power: A case study on optimizing information retrieval in\\nflipped classrooms via prompt engineering. IEEE Transactions on Learning Tech-\\nnologies, 17:629–641, 2024.\\n[56] Jules White, Sam Hays, Quchen Fu, Jesse Spencer-Smith, and Douglas C Schmidt.\\nChatgpt prompt patterns for improving code quality, refactoring, requirements elic-\\nitation, and software design. In Generative AI for Effective Software Development,\\npages 71–108. Springer, 2024.\\n[57] Stephen B Wicker and Saejoon Kim. Fundamentals of codes, graphs, and iterative\\ndecoding, volume 714. Springer Science & Business Media, 2002.\\n[58] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi.\\nBertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675,\\n2019.\\n[59] Liping Zhao, Waad Alhoshan, Alessio Ferrari, Keletso J Letsholo, Muideen A\\nAjagbe, Erol-Valeriu Chioasca, and Riza T Batista-Navarro. Natural language pro-\\ncessing for requirements engineering: A systematic mapping study. ACM Computing\\nSurveys (CSUR), 54(3):1–41, 2021.\\n[60] Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, and Hoifung Poon. Univer-\\nsalner: Targeted distillation from large language models for open named entity\\nrecognition. arXiv preprint arXiv:2308.03279, 2023.\\n75'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='arXiv:2506.04761v2  [cs.LG]  25 Jun 2025\\nLog-Linear Attention\\nHan Guo1∗\\nSonglin Yang1∗\\nTarushii Goel1\\nEric P. Xing3\\nTri Dao2\\nYoon Kim1\\n1Massachusetts Institute of Technology\\n2Princeton University, Together AI\\n3Carnegie Mellon University, Mohamed bin Zayed University of AI, GenBio AI\\nhanguo@mit.edu\\nAbstract\\nThe attention mechanism in Transformers is an important primitive for accurate\\nand scalable sequence modeling. Its quadratic-compute and linear-memory com-\\nplexity however remain significant bottlenecks. Linear attention and state-space\\nmodels enable linear-time, constant-memory sequence modeling and can moreover\\nbe trained efficiently through matmul-rich parallelization across sequence length.\\nHowever, at their core these models are still RNNs, and thus their use of a fixed-size\\nhidden state to model the context is a fundamental limitation. This paper develops\\nlog-linear attention, an attention mechanism that balances linear attention’s effi-\\nciency and the expressiveness of softmax attention. Log-linear attention replaces\\nthe fixed-size hidden state with a logarithmically growing set of hidden states. We\\nshow that with a particular growth function, log-linear attention admits a similarly\\nmatmul-rich parallel form whose compute cost is log-linear in sequence length.\\nLog-linear attention is a general framework and can be applied on top of existing\\nlinear attention variants. As case studies, we instantiate log-linear variants of two\\nrecent architectures—Mamba-2 and Gated DeltaNet—and find they perform well\\ncompared to their linear-time variants.2\\n1\\nIntroduction\\nThe attention layer [4] is a core building block of modern deep learning architectures, most notably in\\nthe Transformer architecture [63]. For training, attention can be parallelized across sequence length\\nthrough reformulating the computation as a series of matrix-matrix multiplications (matmuls), which\\ncan enable efficient training on modern accelerators such as GPUs and TPUs. However, the compute\\ncost of attention grows quadratically and its memory cost grows linearly with respect to sequence\\nlength; despite the wallclock efficiency improvements obtained from hardware-optimized attention\\nimplementations [15, 11, 57, 35, 32], this quadratic-compute linear-memory cost is a fundamental\\nlimitation in enabling new applications and serves as a significant bottleneck in existing ones.\\nLinear attention [28] replaces the softmax kernel with a simple linear kernel (i.e., dot product) to\\nderive the “attention” scores. The use of a linear kernel makes it possible to reformulate linear\\nattention as a linear RNN with matrix-valued hidden states, and thus linear attention enables linear-\\ntime, constant-memory sequence modeling.3 For training, linear attention can be parallelized across\\nsequence length via a chunking mechanism where a sequence is split up into chunks and the\\ncomputations across chunks are performed in parallel [23, 60, 69, 12]. The complexity of this\\nchunkwise parallel algorithm is subquadratic in sequence length but still rich in matmuls,4 leading\\nto hardware-efficient implementations [66, 51, 6] that obtain practical wallclock improvements\\n∗Equal contribution.\\n2Code available at https://github.com/HanGuo97/log-linear-attention.\\n3Thus there are three senses in which linear attention is linear: the use of a linear kernel, its reformulation as\\na linear RNN where the hidden state is a linear function of the previous state, and its linear-time complexity.\\n4Unlike parallel scan [8] which can also parallelize linear attention across sequence length but consists mostly\\nof elementwise operations instead of matmuls.\\nPreprint. Under review.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='Model\\nA\\nM (Data Dependent?)\\nTraining Algorithm / Time\\nDecoding Time and Space\\nAttention\\nσ(QK⊤)\\nMask (✗)\\nFlashAttention O(T 2)\\nO(T )\\nO(T )\\nLinear Attention [28]\\nQK⊤\\nMask (✗)\\nChunk-recurrent O(T )\\nO(1)\\nO(1)\\nRetNet [60]\\nQK⊤\\nSemiseparable (✗)\\nChunk-recurrent O(T )\\nO(1)\\nO(1)\\nMamba-2 [12]\\nQK⊤\\nSemiseparable (✓)\\nChunk-recurrent O(T )\\nO(1)\\nO(1)\\nMulti-Hyena [38]\\nQK⊤\\nToeplitz (✗)\\nFFT O(T log T )\\nO(log2 T )\\nO(T )\\nDeltaNet [55, 70]\\nTK(QK⊤)\\nMask (✗)\\nChunk-recurrent O(T )\\nO(1)\\nO(1)\\nGated DeltaNet [68]\\nTK(QK⊤)\\nSemiseparable (✓)\\nChunk-recurrent O(T )\\nO(1)\\nO(1)\\nLog-Linear Mamba-2\\nQK⊤\\nHierarchical (✓)\\nChunk-scan O(T log T )\\nO(log T )\\nO(log T )\\nLog-Linear Gated DeltaNet\\nTK(QK⊤)\\nHierarchical (✓)\\nChunk-scan O(T log T )\\nO(log T )\\nO(log T )\\nTable 1: Summary of efficient attention mechanisms under the unified formulation: P = A ⊙M, O = PV.\\nM is a lower-triangle (causal) matrix. We use symbol TK (A) = (A ⊙L)\\n\\x00I + KK⊤⊙(I −L)\\n\\x01−1 for\\nnotational brevity, where L is a lower-triangular matrix of 1s. Here decoding time is the time per step, and\\ndecoding space refers to the overall memory complexity during generation.\\nover optimized implementations of softmax attention. While early versions of linear attention\\ngenerally underperformed softmax attention [27, 46, 37, 49, 60], modern variants with data-dependent\\nmultiplicative gates [69, 52, 44]—which include state-space models (SSMs) such as Mamba [19,\\n12]—and delta-rule-based structured transition matrices [55, 69, 68, 18, 59] have led to significant\\nimprovements. However, despite these improvements linear attention’s use of a fixed-sized hidden\\nstate is a fundamental limitations when it comes to certain capabilities such as associative recall\\nover a given context [2]. And empirically, although many recent linear RNNs claim to match or\\noutperform softmax attention, these results are often based on training and evaluation in short-context\\nsettings. In practice, their performance degrades when exposed to longer contexts [61, 34].\\nThis paper develops log-linear attention as a middle ground between linear attention and full softmax\\nattention. Instead of using a single hidden state matrix to represent the history (as in linear atten-\\ntion/SSMs), log-linear attention maintains a growing set of hidden states where the set size grows\\nlogarithmically with respect to sequence length. With a particular choice of the growth function,\\nwe show that log-linear attention admits a matmul-rich “parallel form” for training which involves\\nreplacing the lower-triangular causal mask in ordinary linear attention with a data-dependent hier-\\narchical matrix, which enables subquadratic training; in particular we show that the compute cost\\nof log-linear attention is log-linear in sequence length (hence the name), while its memory cost is\\nlogarithmic. Log-linear attention is a general framework for sequence modeling and can be used to\\ngeneralize existing linear attention models. As case studies, we use the framework on two popular\\nrecent models, Mamba-2 [12] and Gated DeltaNet [68] to derive log-linear variants of both models,\\nand find that these variants perform well compared to their original linear variants.\\n2\\nBackground: A Structured Matrix View of Efficient Attention Variants\\nGiven an input sequence of length T and the corresponding key, query, value matrices\\nK, Q, V\\n∈\\nRT ×d, softmax attention obtains the output O ∈RT ×d for all time steps via\\nO = softmax(QK⊤⊙M)V, where M ∈{−∞, 0}T ×T is a causal masking matrix. This in-\\ncurs O(T 2) compute and O(T) memory, which makes it costly to apply to long sequences. As a\\nresponse, there has been much recent work on efficient alternatives with sub-quadratic compute and\\nsub-linear memory, including linear attention, state-space models, and long convolution models.\\nDespite their differences, many of these approaches can be captured by the following equation:\\nP = A ⊙M,\\nO = PV,\\n(1)\\nwhere A ∈RT ×T is an attention-like matrix (e.g., QK⊤in the case of ordinary linear attention)\\nand M ∈RT ×T is a lower-triangular causal masking matrix (e.g., M ∈{0, 1}T ×T for linear\\nattention). By separating out the interaction terms A and the (potentially data-dependent) masking\\nmatrix M (which typically models the “decay factor” between two positions), this abstraction reveals\\ncommonalities across a broad class of models, as shown in Table 1. Moreover, different structures\\nimposed on M can lead to efficient training and inference algorithms. We now describe key models\\nthat fit within this framework.\\nLinear attention.\\nLinear attention [28] simply removes the softmax operation, resulting in the\\nfollowing parallel form5\\nO = (QK⊤⊙M) V,\\nMij = 1{i ≤j}.\\n5Here we work linear attention without any feature maps or normalization, since most recent works have\\nfound them to be unnecessary (although see [25, 9, 2]).\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Linear attention can be reparameterized into the following “recurrent form” for inference,\\nSt = St−1 + vtk⊤\\nt ,\\not = Stqt,\\nwhich enables linear-time constant-memory sequence modeling.\\nLinear attention with (data-dependent) gates.\\nVanilla linear attention lacks a forgetting mech-\\nanism, which has been shown to be crucial in ordinary RNNs. One way to incorporate such a\\nmechanism is through a scalar gate αt ∈(0, 1), which results in recurrence St = αtSt−1 + vtk⊤\\nt .\\nThis has the following corresponding parallel form:\\nO = (QK⊤⊙M)V,\\nMij =\\niY\\nk=j+1\\nαk.\\n(2)\\nOriginally introduced by Peng et al. [46], gated linear attention has enjoyed a resurgence in recent\\nyears [52, 44, 67, 29] and are an instance of time-varying SSMs [19, 12]. Well-known models\\nin this family include RetNet [60], which uses a data-independent gate αt = α, and Mamba-2\\n[12], which uses the above data-dependent gate. Dao and Gu [12] show that with a scalar gating\\nfactor, M has a 1-semiseparable structure where every submatrix in the lower triangular portion\\nhas rank at most 1. Some works such as Mamba [19] and GLA [69] work with data-dependent\\ngating matrices, i.e., St = Gt ⊙St−1 + vtk⊤\\nt where Gt ∈(0, 1)d×d. When Gt has rank-one\\nstructure (e.g., as in GLA and GateLoop [29]), it is still possible to have an “efficient” representation\\nvia O =\\n\\x10\\x10\\x10\\n(Q ⊙B) (K/B)⊤\\x11\\x11\\n⊙M) (V/D)\\n\\x11\\n⊙D where B, D ∈RL×d capture the per-step\\nlow-rank factorizations of Gt, and M ∈{0, 1}T ×T is now a simple causal masking matrix (see Yang\\net al. [69, §C] for the derivation). This type of representation is (to the best of our knowledge) not\\npossible with the full rank Gt used by Mamba.\\nLinear attention with the delta rule.\\nDeltaNet [55] is a type of linear attention layer which updates\\nthe hidden state via the delta rule [64],6 where the recurrent form is given by7\\nSt = St−1\\n\\x00I −ktk⊤\\nt\\n\\x01\\n+ vtk⊤\\nt ,\\not = Stqt.\\nWhile the original work used a purely recurrent form, Yang et al. [70] recently show that it is\\npossible to parallelize DeltaNet across sequence length through leveraging a compact representation\\nof Householder matrices [7, 24], resulting in the following parallel form (cf. [70, §3.2]):\\nO =\\n\\uf8eb\\n\\uf8ed\\x00QK⊤⊙L\\n\\x01 \\x00I + KK⊤⊙(L −I)\\n\\x01−1\\n|\\n{z\\n}\\nA\\n⊙M\\n\\uf8f6\\n\\uf8f8V\\nwhere L and M are lower-triangular matrices consisting of 1s. Since A itself is already lower-\\ntriangular, the causal masking matrix M is not strictly necessary in the above. However, by changing\\nM to have 1-semiseparable structure as in Mamba-2, we can recover Gated DeltaNet [68], whose\\nrecurrence is given by St = αtSt−1(I −ktk⊤\\nt ) + vtk⊤\\nt . Linear attention with such data-dependent\\nstructured transition matrices has been shown to be theoretically more expressive than linear attention\\nwith multiplicative gates when it comes to certain types of state-tracking tasks [40, 18, 59, 45], which\\nmake these layers attractive targets to generalize via our log-linear attention framework.\\nLong convolution models.\\nLong-convolution sequence models, where the convolution kernel size\\nequals the sequence length, can also be cast into this framework. For example Toeplitz neural network\\n[50] and MultiHyena [38] layers are given by O = (QK⊤⊙Th) V, where Th is a causal Toeplitz\\nmatrix generated by a long convolution kernel h ∈RT , i.e., Th[i, j] = h[i −j] for i ≥j and\\n0 otherwise. Other long convolutional variants like H3 [17] and Hyena [47] also admit a precise\\nattention-style formulation, which has already been shown in past work [47, 38]. While the decoding\\nspeed of long convolution models can be improved from O(T) to O(log2 T) per step [43], their\\nmemory cost remains linear, i.e., the same as in softmax attention. However, some long convolution\\nmodels such as S4 [20] admit a reparameterization into a time-invariant SSM and thus enjoy constant-\\nmemory inference. There has also been efforts to distill long convolution models into RNNs [38, 48],\\nbut these inherit the memory bottleneck of RNNs.\\n6Linear attention with the delta rule is also an instance of a fast-weight programmer [56].\\n7The actual DeltaNet recurrence is given by St = St−1(I −βtktk⊤\\nt ) + vtk⊤\\nt where βt is a data-dependent\\nscalar value in either (0, 1) or (0, 2), but we set βt = 1 here for notational brevity.\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='Relationship between masking structure and efficient algorithms.\\nUsing an unstructured M\\n(e.g., a random lower-triangular matrix) degrades both compute and memory complexity to softmax\\nattention-levels, despite the absence of softmax; i.e., the structure of M is essential for training/infer-\\nence efficiency, not just the removal of softmax. In linear attention where M is a lower-triangular\\nmatrix of 1’s, we can compute O chunkwise, leading to an O(T) algorithm.8 This algorithm gener-\\nalizes to the gated case where M has 1-semiseparable structure as shown in the state-space duality\\nframework [12]. Long convolution models can use FFT to bring down the cost to O(T log T).\\n3\\nLog-Linear Attention\\nLinear Attention\\nLog-Linear Attention\\n+\\n+\\n+\\nFigure 1: Standard linear attention (top) vs. log-\\nlinear attention (bottom). The input consists of\\nquery, key, and value vectors.\\nThe preceding section shows that the structure of the\\nmasking matrix M in O = (A⊙M)V plays a key role\\nin determining compute and memory costs. Our log-\\nlinear attention mechanism places a particular structure\\non M that enables the compute cost to be log-linear\\nin T (i.e., O(T log T)) and the memory cost to be log-\\narithmic (i.e., O(log T)). Log-linear attention only\\nmodifies the masking matrix M and therefore can be\\nused to generalize linear attention models whose A\\nmatrix can have different structure. As case studies, we\\nshow how to derive log-linear variants of Mamba-2 and\\nGated DeltaNet based on our framework.\\nLog-linear attention employs a Fenwick tree–based\\nscheme [16] to hierarchically partition the input into\\npower-of-two-sized segments. Each position summa-\\nrizes a range ending at that point, enabling queries to\\nattend to a logarithmic number of hidden states that\\ncapture past context at multiple temporal scales. This\\nstructure naturally emphasizes recent tokens through finer segmentation and supports O(log T) time\\nand space complexity during decoding. For training, we show that this formulation corresponds to a\\nstructured M that yields a parallel algorithm with O(T log T) time and O(T) space complexity.\\n3.1\\nFenwick Tree Partitioning for Linear Attention\\nWe begin with the simplest form of linear attention and show how log-linear attention\\ngeneralizes it by encoding distinct recurrent memories across different temporal segments.\\nt=0\\nt=1\\nt=2\\nt=3\\nt=4\\nt=5\\nt=6\\nt=7\\nFigure 2: Fenwick tree bucket assignments.\\nFor effective hierarchical segmentation, the method used to\\npartition the prefix [0, t) for a query qt at step t is critical.\\nA straightforward approach assigns each token s ∈[t]\\nto a level ℓ= ⌊log2 s⌋, based on its absolute position.\\nHowever, in autoregressive decoding, this leads to overly\\ncoarse granularity for the most recent tokens—precisely\\nthe ones most crucial for accurate prediction. Intuitively,\\nrecent context should be modeled with higher resolution.\\nTo address this, we adopt a partitioning scheme based on\\nthe Fenwick tree structure [54, 16], which divides the\\nprefix [0, t) into up to L = ⌈log2 t + 1⌉+ 1 disjoint\\nbuckets. This decomposition is guided by the function\\nlssb(t) = max {ℓ∈N | 2ℓdivides t}, which identifies\\nthe least significant set bit in the binary representation of t. Conceptually, the partitioning pro-\\nceeds greedily, at each step subtracting the largest power of two that fits within the remaining segment\\nof the prefix, asgiven below and shown in Fig. 2,\\nb(i)\\nt =\\n(t\\nif i = 0\\nb(i−1)\\nt\\n−2\\nlssb\\n\\x10\\nb(i−1)\\nt\\n\\x11\\notherwise\\n,\\nB(ℓ)\\nt =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\n{b(0)\\nt }\\nif ℓ= 0\\n{b(i+1)\\nt\\n, · · · , b(i)\\nt −1}\\nif ℓ= lssb\\n\\x10\\nb(i)\\nt\\n\\x11\\n+1\\n∅\\notherwise\\n8This algorithm depends on the chunk size C, but since C is a hyperparameter this is still linear in T.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='Each bucket B(ℓ)\\nt\\nhas (at most) a power-of-two length: |B(ℓ)\\nt | = 2ℓ−1 for ℓ≥1, with a sentinel\\nbucket of size |B(0)\\nt\\n| = 1. Then, to obtain the output ot, log-linear attention computes the recurrent\\nmemory separately for each bucket, and weight the output by a data-dependent scalar λ(ℓ)\\nt\\n≥0, which\\nmodulates the contribution of its corresponding bucket to the output. These weights are parameterized\\nas functions of the input xt via a linear projection, allowing the model to adaptively attend to different\\ntemporal scales. Concretely, the output is given by,\\not =\\nL−1\\nX\\nℓ=0\\nλ(ℓ)\\nt q⊤\\nt\\n\\uf8eb\\n\\uf8ec\\n\\uf8ed\\nX\\ns∈B(ℓ)\\nt\\nvsk⊤\\ns\\n\\uf8f6\\n\\uf8f7\\n\\uf8f8=\\nL−1\\nX\\nℓ=0\\nλ(ℓ)\\nt q⊤\\nt S(ℓ)\\nt ,\\n(3)\\nwhere S(ℓ)\\nt\\n∈Rd×d is hidden state that summarizes all the information in level ℓ. We observe that\\nwhen all λ(ℓ)\\nt\\nare the same (or more generally when the λ(ℓ)\\nt\\nand λ(ℓ′)\\nt\\nare linearly related across\\ntime) log-linear attention collapses to linear attention. Allowing distinct λ(ℓ)\\nt\\nis therefore essential for\\ncapturing multi-scale temporal structure.\\nParallel form.\\nWhile Eq. 3 is conceptually intuitive, it involves primarily matrix-vector products,\\nwhich are inefficient on modern hardware optimized for matrix-matrix operations. To better lever-\\nage hardware acceleration and enable parallel computation across time steps, we reformulate the\\ncomputation into a matmul-friendly form as in Sec. 2:\\nO =\\n\\x00QK⊤⊙MH\\x01\\n|\\n{z\\n}\\nP\\nV,\\nMH\\nts =\\n(\\nλℓ(t,s)\\nt\\nif s ≤t\\n0\\notherwise\\n(4)\\nHere, ℓ(t, s) denotes the level to which token s belongs at time t under the Fenwick tree partitioning.\\nFor brevity, we omit the explicit dependence on (t, s) when the context is clear. Notably, the matrix\\nA exhibits a structured low-rank pattern induced by the Fenwick tree partitioning, as shown below.\\nIn §3.2 we show how we can exploit this structure to derive a O(T log T) parallel training algorithm.\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nλ(0)\\n0\\nq⊤\\n0 k0\\nλ(1)\\n1\\nq⊤\\n1 k0\\nλ(0)\\n1\\nq⊤\\n1 k1\\nλ(0)\\n2\\nq⊤\\n2 k2\\nλ(1)\\n3\\nq⊤\\n3 k2\\nλ(0)\\n3\\nq⊤\\n3 k3\\nλ(0)\\n4\\nq⊤\\n4 k4\\nλ(1)\\n5\\nq⊤\\n5 k4\\nλ(0)\\n5\\nq⊤\\n5 k5\\nλ(0)\\n6\\nq⊤\\n6 k6\\nλ(1)\\n7\\nq⊤\\n7 k6\\nλ(0)\\n7\\nq⊤\\n7 k7\\n\"\\nλ(2)\\n2\\nq2\\nλ(2)\\n3\\nq3\\n# \\x14\\nk0\\nk1\\n\\x15⊤\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nλ(3)\\n4\\nq4\\nλ(3)\\n5\\nq5\\nλ(3)\\n6\\nq6\\nλ(3)\\n7\\nq7\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nk0\\nk2\\nk3\\nk1\\n\\uf8f9\\n\\uf8fa\\uf8fb\\n⊤\\n\"\\nλ(2)\\n6\\nq6\\nλ(2)\\n7\\nq7\\n# \\x14\\nk4\\nk5\\n\\x15⊤\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nMemory-efficient decoding.\\nIncremental token-by-token decoding proceeds as follows. Re-\\ncall that lssb(t) determines the index of the least significant set bit in the binary represen-\\ntation of t.\\nThe new set of hidden states{S(ℓ)\\nt }ℓare then given by the below equation.\\nS(ℓ)\\nt =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nvtk⊤\\nt\\nif ℓ=0\\n0\\nif 0<ℓ≤lssb(t)\\nPℓ−1\\nℓ′=0 S(ℓ′)\\nt−1\\nif ℓ= lssb(t)+1\\nS(ℓ)\\nt−1\\nif ℓ> lssb(t)+1\\nThis recurrence reflects the core structure of the Fenwick-\\npartitioned memory: at each timestep, the current memory\\nterm vtk⊤\\nt is inserted into the finest-resolution bucket\\n(ℓ= 0), while all buckets up to and including lssb(t)\\nare merged and promoted into a coarser-resolution bucket.\\nWhen t is a power of two, a new level is added. This main-\\ntains O(log T) memory during inference. This decoding\\nprocess follows the same principle as the online update\\nand query mechanism in Fenwick trees.\\nRemark.\\nThe matrix MH (and A) is a lower-triangular instance of a hierarchical (H) ma-\\ntrix—specifically, of the HODLR (Hierarchically Off-Diagonal Low-Rank) type. When constructed\\nusing schemes like the Fenwick tree, it inherits the recursive partitioning and low-rank off-diagonal\\nblocks that define H matrices. This establishes a direct connection between log-linear attention and\\nhierarchical matrices: the attention operator corresponds to structured matrix multiplication with an\\nH matrix. We refer to MH as a quasi-H matrix—a specialized class lying between general H and\\nsemiseparable matrices, designed to support O(log T)-space recurrence. See Section B.1 for details.\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='Level 1\\n(block low-rank)\\nLevel 3\\n(block low-rank)\\nLevel 2\\n(block low-rank)\\nLevel 0\\n(block diagonal)\\nFigure 3: Left: Decomposition of the matrix MH. Right: Chunkwise algorithm (Algorithm 1). Level 0 handles\\nintra-chunk computations using a quadratic (in chunk size) algorithm, which is efficient due to small chunk\\nsizes. Levels 1 and above perform inter-chunk computations by invoking existing inter-chunk primitives multiple\\ntimes, with overall complexity logarithmic in the number of chunks.\\n3.2\\nEfficient Algorithm for Training\\nThe chunkwise parallel algorithm for linear attention [60, 67, 12] splits a sequence into chunks of\\nlength C and performs the computations for all chunks in parallel, while passing information across\\nchunks when necessary. This offers a balance between the fully parallel and recurrent forms by\\nreducing the computational cost of global attention while enabling greater sequence-level parallelism\\nthan strict recurrent computations. We show how primitives for efficient chunkwise computation of\\nlinear attention can be adapted to the log-linear case. First observe that the matrix MH exhibits a\\nlow-rank structure in its off-diagonal blocks, enabling a decomposition of the form:\\nMH = D +\\nL−1\\nX\\nℓ=1\\nM(ℓ),\\nM(ℓ)\\nts =\\n(\\nλ(ℓ)\\nt ,\\nif s ∈B(ℓ)\\nt ,\\n0,\\notherwise.\\nHere, D is a block-diagonal matrix with T\\nC blocks {D[k]}\\nT\\nC\\nk=1, each encoding intra-chunk interactions.\\nEach block D[i] ∈RC×C is lower triangular, where D[i]\\nts = λ(ℓ)\\niC+t. M(ℓ) captures inter-chunk\\ndependencies at level ℓthrough a blockwise low-rank structure. See Fig. 3 (left) for an illustration.\\nBuilding on this structure, we develop a chunkwise log-linear attention algorithm (Algorithm 1). As\\nshown in Fig. 3 (right), this chunkwise strategy adds a logarithmic overhead on top of linear attention.\\nThis algorithm processes the interactions in two stages.\\nIntra-chunk computations (ℓ=0): For the block diagonal component D, each block is treated as\\ndense unstructured block, resulting in an ordinary O(C2) matrix multiplication for the T\\nC diagonal\\nblocks, thus incurring O(TC) cost in total.\\nInter-chunk computations (ℓ>0): For the blocks corresponding to {M(ℓ)}L−1\\nℓ=1 , the dependencies\\nbetween chunks are handled via a sequence of linear attention passes. Owing to the hierarchical\\nmatrix structure and its decomposition (Eq. 3.2), each level reduces to a computation involving a\\nsequentially semi-separable (SSS) matrix. When an efficient (linear-time) state-passing primitive is\\navailable—such as those used in Mamba-2 or Gated DeltaNet—inter-chunk computation requires\\nonly O(log T\\nC ) invocations of this primitive. Each invocation costs O(T) in both time and memory,9\\nand thus the total cost of these operations are O(T log T).\\nThis method extends the classical scan algorithm to the hierarchical domain, which we term a\\nchunkwise parallel scan. Unlike token-level scans—often hindered by memory bandwidth limitations\\nand high I/O overhead during training [67]—chunk scan restructures recurrent memory updates into\\nparallel operations across chunks. Specifically, it performs O(log T) independent scans, one per\\nmemory level, each implementable using standard parallel techniques such as the Blelloch scan [8].\\nLayer-specific weights (e.g., the λ(ℓ)\\nt\\nterms from MH) are directly embedded in these scans, enabling\\nefficient and scalable computation throughout the hierarchy.\\n9At level ℓ, the matrix M(ℓ) contains\\nT\\n2ℓ−1C chunks, each of size 2ℓ−1C. By skipping redundant operations,\\nthe total cost can be reduced by a constant factor of two.\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='4096\\n8192\\n16384\\n32768\\n65536\\n131072\\nSequence Length\\n5 × 103\\n1 × 104\\n2 × 104\\n4 × 104\\n1 × 105\\n2 × 105\\nThroughput (tokens/s)\\nFlashAttention-2\\nMamba-2\\nLog-Linear Mamba-2\\n4096\\n8192\\n16384\\n32768\\n65536\\n131072\\nSequence Length\\n10\\n1\\n100\\n101\\n102\\n103\\n104\\nRuntime (ms)\\nFlashAttention-2\\nMamba-2\\nLog-Linear Mamba-2 (naive)\\nLog-Linear Mamba-2\\nFigure 4: Training throughput (left; higher is better) and kernel runtime for a forward and backward pass (right;\\nlower is better) across varying sequence lengths. Log-Linear Mamba-2 (naive) denotes repeated application of\\nthe existing Mamba-2 primitives, while Log-Linear Mamba-2 uses a custom implementation with optimizations\\nsuch as level fusion. The throughput drop at sequence length 131K is due to gradient checkpointing to reduce\\nmemory usage. Experiments were run on an H100 GPU with batch size 2, 48 heads, head dimension 64, state\\ndimension 128, and chunk size 64. We use MVA for (Log-Linear) Mamba-2, and GQA for FlashAttention-2.\\n3.3\\nLog-Linear Variants of Mamba-2 and Gated DeltaNet\\nHaving introduced log-linear attention as a hierarchical extension of linear attention, we now discuss\\nhow this idea can be applied to two concrete architectures: Mamba-2 [12] and Gated DeltaNet [68].\\nAs discussed in Sec. 2, both models incorporate gating mechanisms, which induce a sequentially\\nsemi-separable (SSS) structure in the attention mask [12], which we denote as MS where Mij =\\nQi\\nk=j+1 αk (see Eq. 2). The main difference between the two lies in their respective parameterizations\\nof the transition matrix A. Our approach preserves the original form of A in each model while\\ncomposing the attention mask with its log-linear variant M = MS ⊙MH.10 We refer to the resulting\\nmodels as log-linear Mamba-2 and log-linear Gated DeltaNet. Their parallel forms are given by,\\nO =\\n\\x00QKT ⊙MS ⊙MH\\x01\\nV\\nLog-Linear Mamba-2\\nO =\\n\\x10\\x00QK⊤⊙L\\n\\x01 \\x00I + KK⊤⊙(L −I)\\n\\x01−1 ⊙MS ⊙MH\\x11\\nV\\nLog-Linear Gated DeltaNet\\nThis construction illustrates a general principle: any linear attention-style mechanism with structured\\nmemory and an efficient chunkwise-parallel primitive can be extended to a log-linear variant by\\ncomposing its attention mask with a log-linear counterpart.\\n3.4\\nImplementation\\nWe implemented the chunkwise parallel scan algorithm in Triton [62]. The custom kernel for log-\\nlinear Mamba-2 outperforms FlashAttention-2 [11] (forward + backward) at sequence lengths beyond\\n8K. In full training setups, throughput depends on model architecture. Notably, log-linear Mamba-\\n2 (with MLP) surpasses Transformer throughput at 32K, despite additional layers like depthwise\\nconvolutions absent in the Transformer. See Sec. C for details.\\n3.5\\nGeneralizations\\nMore expressive linear RNNs.\\nConsider general linear RNNs of the form St = St−1Ct + vtk⊤\\nt\\nwhere Ct is a data-dependent transition matrix. Without any structural assumptions on Ct (e.g., as in\\nDeltaNet where Ct has identity-plus-rank-one structure) this general model does not admit a “nice”\\nfactorization of A into key/query matrices. This makes the general formulation difficult to work with\\nin practice. However, it is still possible to derive log-linear versions of such expressive linear RNNs\\nif we generalize A and M to higher-order tensors. We give this generalization in §A.\\nFull use of levels.\\nAs shown in Fig. 1, although there are O(log T) states corresponding to different\\nhierarchical levels, roughly half of them are zero in practice. This sparsity arises from the specific\\nstructure of HODLR matrices, which belong to a broader class of H matrices known as weakly\\nadmissible matrices [21]. Strongly admissible H matrices allow for finer-grained partitioning of the\\nmatrix, and their corresponding recurrent forms utilize all hierarchical levels. However, this comes at\\na significant computational cost, and we found this strongly admissible variant of log-linear attention\\nto only marginally improve performance while incurring a significant slowdown. We thus adopt the\\nweakly admissible structure throughout this work. See §B.4 for further discussion.\\n10More precisely, the elementwise product of an SSS matrix and an H matrix remains an H matrix. We\\nseparate them here for clarity.\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='4\\nExperiments\\n4.1\\nSynthetic Benchmark\\n16\\n32\\n64\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\nMamba-2\\nLog-Linear Mamba-2\\n16\\n32\\n64\\nGated-DeltaNet\\nLog-Linear Gated-DeltaNet\\nFigure 5: MQAR experiments with early stopping at 99% accuracy.\\nWe begin by evaluating models on\\nthe multi-query associative recall\\n(MQAR) task [1], a standard di-\\nagnostic benchmark for testing in-\\ncontext recall. Our setup closely fol-\\nlows Arora et al. [2]: models are\\ntrained and evaluated on 256-token se-\\nquences containing 4 to 64 key-value\\npairs (excluding the length generaliza-\\ntion component), with tuned learning\\nrates. For log-linear models, we also tune the λ parameterization. We run each configuration with\\nfive seeds. Training was early stopped when accuracy exceeded 99%. Additional experimental details\\nare provided in §D.\\nResults.\\nAs shown in Fig. 5, log-linear models perform well—even when compared to strong linear\\nmodels like Gated DeltaNet, which is specifically optimized for associative recall. Note that softmax\\nattention achieves near-perfect accuracy across all settings.\\n4.2\\nLanguage Modeling\\nWe perform academic-scale language modeling pretraining from scratch using 50B tokens on the\\nLong-Data-Collections dataset,11 using a sequence length of 16K. All models have 21 layers and\\nuse a hidden size of 1536. We use a Transformer with 16 attention heads and a RoPE base of 500K,\\na modified Mamba-2 with 48 heads and MLP layers, and a Gated DeltaNet with 6 heads. The\\nTransformer, Mamba-2, and Gated DeltaNet models contain 693M, 802M, and 793M parameters,\\nrespectively. For the log-linear variants, we apply a linear layer on top of the hidden states to compute\\nthe per-head values λ(ℓ)\\nt . This adds less than 3% additional parameters for Mamba-2 (825M) and less\\nthan 0.4% for Gated DeltaNet (796M). Since Mamba-2 and Gated DeltaNet have more parameters\\nthan ordinary Transformers, we also include a (roughly) parameter-matched Transformer variant\\nwith 24 layers (778M parameters) for comparison. For our log-linaer variants, we use the default\\nhyperparameters from the baselines (§D).\\nModel\\nWiki.\\nLMB.\\nLMB.\\nPIQA\\nHella.\\nWino.\\nARC-e\\nARC-c\\nAvg.\\nppl ↓\\nppl ↓\\nacc ↑\\nacc ↑\\nacc_n ↑\\nacc ↑\\nacc ↑\\nacc_n ↑\\nTransformer\\n21.56\\n22.14\\n38.8\\n65.1\\n39.6\\n50.7\\n45.6\\n24.5\\n44.0\\nw/ 24 Layers\\n21.13\\n21.17\\n39.3\\n66.6\\n40.4\\n53.3\\n47.8\\n26.4\\n45.6\\nMamba-2\\n22.44\\n24.14\\n36.2\\n66.8\\n41.2\\n51.6\\n46.0\\n27.1\\n44.8\\nw/ Log-Linear\\n22.11\\n21.86\\n37.0\\n66.6\\n41.1\\n51.7\\n45.5\\n27.4\\n44.9\\nGated DeltaNet\\n21.73\\n19.71\\n39.3\\n65.8\\n40.9\\n52.2\\n47.1\\n24.6\\n45.0\\nw/ Log-Linear\\n21.44\\n18.08\\n40.5\\n66.1\\n41.4\\n53.9\\n46.9\\n24.9\\n45.6\\nTable 2: Performance comparison on language modeling and zero-shot commonsense reasoning.\\nStandard benchmarks.\\nFollowing prior work [12, 68], we evaluate models on WikiText perplexity\\nand several zero-shot commonsense reasoning benchmarks (Table 2). These are short-context tasks\\nand are therefore largely insensitive to model state size. As such, we generally expect the log-linear\\nvariants to perform comparably to their linear counterparts. Log-Linear Mamba-2 improves upon\\nits linear counterpart in perplexity and in half of the commonsense reasoning tasks. Log-Linear\\nGated DeltaNet shows stronger gains, outperforming its linear version in perplexity and in all but one\\nreasoning benchmark. Notably, it also outperforms a layer-matched Transformer across all metrics\\nand a parameter-matched Transformer on half of them.\\nPer-position loss.\\nFollowing Lin et al. [34], we report the model’s loss at each token position to\\nevaluate its ability to handle long contexts (Fig. 6). If the loss steadily decreases as the token position\\n11https://huggingface.co/datasets/togethercomputer/Long-Data-Collections.\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='0\\n2000\\n4000\\n6000\\n8000\\n10000\\n12000\\n14000\\n16000\\nPosition\\n2.56\\n2.58\\n2.60\\n2.62\\n2.64\\n2.66\\nLoss\\nTransformer\\nTransformer (24 Layers)\\nMamba-2\\nLog-Linear Mamba-2\\n0\\n2000\\n4000\\n6000\\n8000\\n10000\\n12000\\n14000\\n16000\\nPosition\\nTransformer\\nTransformer (24 Layers)\\nGated-DeltaNet\\nLog-Linear Gated-DeltaNet\\nFigure 6: Per-position loss on Book3 samples (about 39M tokens) with running average of window size 501.\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nsingle-1\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nsingle-2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\nsingle-3\\n4096\\n8192\\n16384\\n0.2\\n0.4\\n0.6\\n0.8\\nmulti-key-1\\n4096\\n8192\\n16384\\n0.0\\n0.2\\n0.4\\n0.6\\nmulti-query\\n4096\\n8192\\n16384\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nmulti-value\\nTransformer\\nTransformer (24 Layers)\\nMamba-2\\nLog-Linear Mamba-2\\nGated-DeltaNet\\nLog-Linear Gated-DeltaNet\\nFigure 7: Needle-In-A-Haystack experiments. See Table 7 for details.\\nincreases, it suggests the model is effectively using the full context. However, if the loss levels off\\nafter a certain point, it indicates the model struggles to make use of information that is too far back\\nin the sequence. For this analysis, we use 39M tokens from Book-3.12 To improve visualization,\\nwe apply a running average with a window size of 501. We observe that extending both Mamba-2\\nand Gated DeltaNet to their log-linear counterparts consistently reduces the (smoothed) loss across\\nvarious positions, indicating improved long-range context utilization. Log-Linear Gated DeltaNet\\nalso closely tracks the performance of the layer-matched Transformer, although a performance gap\\nremains when compared to the parameter-matched Transformer.\\nNeedle-In-A-Haystack.\\nWe use the Needle-In-A-Haystack (NIAH, Fig. 7) benchmark from\\nRULER [22], where the model must retrieve a value (the “needle”) based on a key hidden in a\\nlong context (the “haystack”). In the simpler single-needle tasks, the log-linear variant of Mamba-2\\noutperformed its linear counterpart on 8 out of 9 metrics. Gated DeltaNet, which already achieved\\nperfect accuracy in several cases, saw improvements in 3 metrics, with 3 remaining unchanged. For\\nthe more challenging multi-needle tasks, Log-Linear Mamba-2 again improved in 8 out of 9 metrics,\\nwhile Log-Linear Gated DeltaNet achieved improvements across all metrics.\\nIn-Context Retrieval.\\nFollowing Arora et al. [2, 1], we evaluate models on real-world, recall-\\nintensive tasks (Table 3). Since these benchmarks were originally designed for short sequences (≤2K\\ntokens), we report results at sequence lengths of 512, 1024, 2048, and (except NQ) 16K. We find that\\nLog-Linear Mamba-2 yields improvements on roughly half of the tasks (SQuAD, TriviaQA, and NQ).\\n12victor-wu/book3\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='SWDE\\nSQuAD\\nFDA\\nModel\\n512\\n1024 2048\\n16k\\n512\\n1024 2048\\n16k\\n512\\n1024 2048\\n16k\\nTransformer\\n47.3\\n44.6\\n45.2\\n45.4\\n34.0\\n34.5\\n34.5\\n34.5\\n72.2\\n70.8\\n72.9\\n72.2\\nw/ 24 Layers\\n53.8\\n50.9\\n50.3\\n50.8\\n30.7\\n31.2\\n31.2\\n30.9\\n73.8\\n76.0\\n74.4\\n73.8\\nMamba-2\\n42.5\\n37.7\\n30.7\\n30.6\\n21.6\\n21.7\\n21.9\\n22.0\\n53.7\\n38.0\\n23.8\\n21.3\\nw/ Log-Linear\\n41.9\\n35.6\\n28.4\\n28.5\\n25.8\\n25.9\\n25.9\\n26.1\\n53.0\\n37.5\\n20.5\\n16.6\\nGated DeltaNet\\n41.0\\n32.5\\n27.2\\n27.8\\n23.8\\n24.1\\n24.3\\n23.7\\n57.2\\n43.7\\n33.2\\n30.5\\nw/ Log-Linear\\n46.2\\n39.4\\n35.3\\n35.1\\n25.2\\n25.2\\n25.3\\n25.3\\n64.9\\n53.5\\n39.1\\n30.5\\nTriviaQA\\nDrop\\nNQ\\nModel\\n512\\n1024 2048\\n16k\\n512\\n1024 2048\\n16k\\n512\\n1024 2048\\nTransformer\\n48.5\\n49.6\\n48.5\\n48.5\\n22.8\\n22.8\\n22.5\\n22.3\\n24.5\\n24.3\\n24.6\\nw/ 24 Layers\\n46.9\\n47.0\\n46.8\\n46.8\\n22.7\\n22.4\\n22.7\\n23.0\\n24.0\\n24.4\\n24.5\\nMamba-2\\n43.7\\n43.2\\n43.2\\n43.2\\n22.2\\n22.1\\n22.2\\n22.1\\n18.5\\n16.5\\n16.5\\nw/ Log-Linear\\n44.9\\n45.0\\n45.5\\n45.5\\n20.2\\n20.6\\n20.3\\n19.9\\n20.0\\n19.9\\n20.4\\nGated DeltaNet\\n45.6\\n45.6\\n45.6\\n45.6\\n21.1\\n21.7\\n21.4\\n21.8\\n20.1\\n18.4\\n18.7\\nw/ Log-Linear\\n45.9\\n45.6\\n46.0\\n46.0\\n20.7\\n20.8\\n20.8\\n21.0\\n22.5\\n21.8\\n21.3\\nTable 3: Accuracy on retrieval tasks w/ input truncated to different lengths.\\nSingle-Doc QA\\nMulti-Doc QA\\nSummarization\\nFew-shot\\nCode\\nModel\\nNQA\\nQQA\\nMFQ\\nHQA\\n2WM\\nMus\\nGvR\\nQMS\\nMNs\\nTRC\\nTQA\\nSSM\\nLCC\\nRBP\\nTransformer\\n11.7\\n9.7\\n20.8\\n22.4 29.8\\n6.7\\n13.1\\n9.4\\n3.2\\n27.5 28.0 16.2\\n23.7 29.8\\nw/ 24 Layers\\n10.7 18.4 26.1\\n33.7 25.7 11.6\\n16.8\\n9.4\\n10.3\\n16.5 45.2 14.3\\n31.5 30.9\\nMamba-2\\n9.1\\n17.4 10.9\\n11.2 20.9\\n4.3\\n8.3\\n6.0\\n4.9\\n2.0\\n22.6\\n8.8\\n38.1 34.6\\nw/ Log-Linear\\n9.8\\n9.6\\n15.4\\n11.5 22.0\\n5.1\\n5.4\\n11.1\\n4.5\\n16.5 21.6 14.9\\n31.2 30.3\\nGated DeltaNet\\n8.5\\n11.9 16.4\\n14.4 24.5\\n6.6\\n9.2\\n11.7 11.6\\n36.5 25.3 23.1\\n31.1 31.1\\nw/ Log-Linear\\n9.9\\n6.1\\n17.6\\n17.7 25.2\\n7.5\\n5.5\\n11.9\\n1.9\\n8.0\\n41.1 23.2\\n28.3 29.6\\nTable 4: Accuracy on LongBench tasks [5]: Narrative QA, QasperQA, MultiField QA, HotpotQA, 2WikiMul-\\ntiQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SamSum, LCC, and RepoBench-P.\\nIn contrast, Log-Linear Gated DeltaNet shows more consistent gains, matching or outperforming\\nGated DeltaNet across all tasks except DROP.\\nLong context understanding.\\nFinally, we evaluated the models’ performance on LongBench [5]\\n(Table 4). We observe that both Log-Linear Mamba-2 and Gated DeltaNet outperforms the baseline\\nMamba-2 Gated DeltaNet in 8 out of 14 evaluation tasks.\\n5\\nDiscussion and Limitations\\nWhile log-linear attention improves upon linear attention in many cases, there are still quite a few\\ntasks where it did not improve upon the linear attention baselines. Due to compute resources we were\\nunable to experiment with different parameterizations of the λ terms (or hyperparameters in general),\\nand it is possible that optimal parameterization of λ could lead to improved results. We also still\\nobserve a significant performance gap compared to Transformers also persists across all benchmarks.\\nThe engineering complexity of log-linear attention is higher. Inter-chunk computations conceptually\\nresemble multiple applications of linear attention primitives, but intra-chunk operations require\\nbespoke implementations. These intra-chunk mechanisms are a primary factor behind the speed\\ndifferences. Additionally, the backward pass is more intricate, as it requires (manually) computing\\nthe gradients not only for the standard attention components but also for the additional λ terms.\\nFinally, the use of Fenwick-tree partitioning (§3.1) introduces an inductive bias: recent tokens are\\nallocated more fine-grained memory, while distant tokens are compressed more aggressively. This\\ndesign reflects a natural assumption rooted in hierarchical matrix which posits that interactions\\nbetween distant elements can be approximated in low-rank form. While intuitive and inspired by\\nphysical phenomena, this inductive bias may not be optimal for all applications. Future work could\\nexplore extensions that enable more flexible structures while preserving computational efficiency.\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='6\\nRelated Work\\nMatrix multiplication serves as the computational backbone of modern deep learning. Contemporary\\narchitectures typically consist of a token mixing layer and a channel mixing layer, both of which\\nheavily depend on matrix multiplications. A growing body of research has investigated replacing\\ndense matrices with structured matrices. For channel mixing, efforts include Butterfly matrices [13],\\nMonarch matrices [14], and more recently, Block Tensor-Train matrices [53]. Token mixing has been\\nexemplified by the family of linear attention models [28] and their various kernelizations [65]. Dao\\nand Gu [12] generalize these approaches by extending low-rank structures to semi-separable matrices,\\nenabling efficient recurrent inference and subsuming many recent recurrent models. Another line\\nof work employs sparse attention patterns such as sliding-window attention (SWA), and several\\nhybrid approaches have also emerged [42, 3, 41]. In this work, we introduce a hierarchical matrix\\nformulation that supports state expansion while maintaining hardware-efficient training and inference.\\nSeveral prior efforts have focused on reducing the quadratic cost of attention to log-linear time\\ncomplexity [30, 58, 10, 50, 17, 36, 72]. Approaches such as LogSparse Transformer [33] and\\nInformer [75] introduce sparse attention patterns to improve computational efficiency, particularly in\\ntime-series applications. Reformer [30] employs locality-sensitive hashing (LSH) to efficiently cluster\\nsimilar queries and keys. Multi-resolution attention [73] adopts a hierarchical approach, progressively\\nrefining attention scores from coarse to fine granularity, while Fast Multipole Attention [26] adapts\\nthe classical fast multipole method to efficiently model long-range interactions. In our work, we\\nleverage the Fenwick tree data structure—a specialized binary indexed tree that enables efficient\\nprefix sum calculations and updates in logarithmic time—to design an efficient attention layer during\\nboth training and decoding phases. While Zhu and Soricut [76] also employ hierarchical matrices\\nfor attention, their formulation is fully parallel and targeted at modest sequence lengths. In contrast,\\nour approach adopts a chunkwise-parallel strategy with a custom Triton implementation optimized\\nfor long-sequence training. Derived from linear attention, our design imposes a structured H-matrix\\nconstraint that ensures O(log T) inference and O(T log T) training complexity. Concurrently, Yau\\net al. [71] propose a related architecture with O(log T) memory, using a relaxed prefix-scan algorithm\\nfor state aggregation that accommodates arbitrary (potentially non-associative) functions.\\n7\\nConclusion\\nWe introduced Log-Linear Attention, a general framework that extends a broad class of linear attention\\nand state-space models to their log-linear counterparts—models with logarithmically growing state\\nsize. This framework offers both theoretical insights and practical benefits, linking structured matrix\\ntheory with hardware-efficient computation. As a case study, we applied this approach to two recent\\narchitectures: Mamba-2 and Gated DeltaNet.\\nAcknowledgments\\nWe thank Tianyuan Zhang, Jyothish Pari, Adam Zweiger, and Yu Zhang for helpful discussion. This\\nstudy was supported by the MIT-Google Program for Computing Innovation, MIT-IBM Watson AI\\nLab, and the AI2050 program at Schmidt Sciences (Grant G-25-67980). HG was supported by a\\nMicrosoft PhD Fellowship.\\nReferences\\n[1] S. Arora, S. Eyuboglu, A. Timalsina, I. Johnson, M. Poli, J. Zou, A. Rudra, and C. Ré. Zoology:\\nMeasuring and improving recall in efficient language models. arXiv preprint arXiv:2312.04927,\\n2023.\\n[2] S. Arora, S. Eyuboglu, M. Zhang, A. Timalsina, S. Alberti, D. Zinsley, J. Zou, A. Rudra, and\\nC. Ré. Simple linear attention language models balance the recall-throughput tradeoff. In\\nProceedings of ICML, 2024.\\n[3] S. Arora, S. Eyuboglu, M. Zhang, A. Timalsina, S. Alberti, D. Zinsley, J. Zou, A. Rudra, and\\nC. Ré. Simple linear attention language models balance the recall-throughput tradeoff, 2025.\\nURL https://arxiv.org/abs/2402.18668.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='[4] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align\\nand translate. In Proceedings of ICLR, 2014.\\n[5] Y. Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou, et al.\\nLongbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint\\narXiv:2308.14508, 2023.\\n[6] M. Beck, K. Pöppel, P. Lippe, and S. Hochreiter. Tiled flash linear attention: More efficient\\nlinear rnn and xlstm kernels. arXiv preprint arXiv:2503.14376, 2025.\\n[7] C. H. Bischof and C. V. Loan. The WY representation for products of householder matrices.\\nIn SIAM Conference on Parallel Processing for Scientific Computing, 1985. URL https:\\n//api.semanticscholar.org/CorpusID:36094006.\\n[8] G. E. Blelloch. Prefix sums and their applications. 1990.\\n[9] J. Buckman, C. Gelada, and S. Zhang. Symmetric Power Transformers.\\n[10] H. J. Cunningham, G. Giannone, M. Zhang, and M. P. Deisenroth. Reparameterized multi-\\nresolution convolutions for long sequence modelling. In The Thirty-eighth Annual Conference\\non Neural Information Processing Systems, 2024. URL https://openreview.net/forum?\\nid=RwgNbIpCpk.\\n[11] T. Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In\\nProceedings of ICLR, 2024.\\n[12] T. Dao and A. Gu. Transformers are SSMs: Generalized models and efficient algorithms through\\nstructured state space duality. In Proceedings of ICML, 2024.\\n[13] T. Dao, A. Gu, M. Eichhorn, A. Rudra, and C. Ré. Learning fast algorithms for linear transforms\\nusing butterfly factorizations, 2020. URL https://arxiv.org/abs/1903.05895.\\n[14] T. Dao, B. Chen, N. S. Sohoni, A. D. Desai, M. Poli, J. Grogan, A. Liu, A. Rao, A. Rudra,\\nand C. Ré. Monarch: Expressive structured matrices for efficient and accurate training. In\\nK. Chaudhuri, S. Jegelka, L. Song, C. Szepesvári, G. Niu, and S. Sabato, editors, International\\nConference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA,\\nvolume 162 of Proceedings of Machine Learning Research, pages 4690–4721. PMLR, 2022.\\nURL https://proceedings.mlr.press/v162/dao22a.html.\\n[15] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Ré. FlashAttention: Fast and memory-efficient\\nexact attention with IO-awareness. In Proceedings of NeurIPS, 2022.\\n[16] P. M. Fenwick. A new data structure for cumulative frequency tables. Software: Practice and\\nExperience, 24, 1994. URL https://api.semanticscholar.org/CorpusID:7519761.\\n[17] D. Y. Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. Re. Hungry hungry hippos: To-\\nwards language modeling with state space models. In The Eleventh International Conference on\\nLearning Representations, 2023. URL https://openreview.net/forum?id=COZDy0WYGg.\\n[18] R. Grazzi, J. Siems, J. K. Franke, A. Zela, F. Hutter, and M. Pontil. Unlocking state-tracking in\\nlinear RNNs through negative eigenvalues. In Proceedings of ICLR, 2025.\\n[19] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. In\\nProceedings of CoLM, 2024.\\n[20] A. Gu, K. Goel, and C. Ré. Efficiently modeling long sequences with structured state spaces. In\\nProceedings of ICLR, 2022.\\n[21] W. Hackbusch, B. N. Khoromskij, and R. Kriemann. Hierarchical matrices based on a weak\\nadmissibility criterion. Computing, 73:207–243, 2004.\\n[22] C.-P. Hsieh, S. Sun, S. Kriman, S. Acharya, D. Rekesh, F. Jia, Y. Zhang, and B. Ginsburg.\\nRuler: What’s the real context size of your long-context language models? arXiv preprint\\narXiv:2404.06654, 2024.\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='[23] W. Hua, Z. Dai, H. Liu, and Q. Le. Transformer quality in linear time. In International\\nconference on machine learning, pages 9099–9117. PMLR, 2022.\\n[24] T. Joffrain, T. M. Low, E. S. Quintana-Ortí, R. A. van de Geijn, and F. G. V. Zee. Accumulating\\nhouseholder transformations, revisited. ACM Trans. Math. Softw., 32:169–179, 2006. URL\\nhttps://api.semanticscholar.org/CorpusID:15723171.\\n[25] P. Kacham, V. Mirrokni, and P. Zhong. Polysketchformer: Fast transformers via sketching\\npolynomial kernels. arXiv preprint arXiv:2310.01655, 2023.\\n[26] Y. Kang, G. Tran, and H. D. Sterck. Fast multipole attention: A divide-and-conquer attention\\nmechanism for long sequences, 2024. URL https://arxiv.org/abs/2310.11960.\\n[27] J. Kasai, H. Peng, Y. Zhang, D. Yogatama, G. Ilharco, N. Pappas, Y. Mao, W. Chen, and N. A.\\nSmith. Finetuning pretrained transformers into rnns. In Proceedings of EMNLP, 2021.\\n[28] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive\\ntransformers with linear attention. In Proceedings of ICML, 2020.\\n[29] T. Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling. arXiv\\npreprint arXiv:2311.01927, 2023.\\n[30] N. Kitaev, Ł. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In Proceedings of\\nICLR, 2020.\\n[31] D. Kressner, S. Massei, and L. Robol. Low-rank updates and a divide-and-conquer method for\\nlinear matrix equations. SIAM Journal on Scientific Computing, 41(2):A848–A876, 2019.\\n[32] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica.\\nEfficient memory management for large language model serving with pagedattention. In\\nProceedings of SOSP, 2023.\\n[33] S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y.-X. Wang, and X. Yan. Enhancing the locality and\\nbreaking the memory bottleneck of transformer on time series forecasting. Advances in neural\\ninformation processing systems, 32, 2019.\\n[34] Z. Lin, E. Nikishin, X. He, and A. Courville. Forgetting transformer: Softmax attention with a\\nforget gate. In The Thirteenth International Conference on Learning Representations, 2025.\\nURL https://openreview.net/forum?id=q2Lnyegkr8.\\n[35] H. Liu, M. Zaharia, and P. Abbeel. Ring attention with blockwise transformers for near-infinite\\ncontext. In Proceedings of ICLR, 2024.\\n[36] L. Madaan, S. Bhojanapalli, H. Jain, and P. Jain. Treeformer: Dense gradient trees for efficient\\nattention computation. In The Eleventh International Conference on Learning Representations.\\n[37] H. H. Mao. Fine-Tuning Pre-trained Transformers into Decaying Fast Weights. In Proceedings\\nof EMNLP, pages 10236–10242.\\n[38] S. Massaroli, M. Poli, D. Y. Fu, H. Kumbong, R. N. Parnichkun, D. W. Romero, A. Timalsina,\\nQ. McIntyre, B. Chen, A. Rudra, C. Zhang, C. Re, S. Ermon, and Y. Bengio. Laughing hyena\\ndistillery: Extracting compact recurrences from convolutions. In Thirty-seventh Conference on\\nNeural Information Processing Systems, 2023. URL https://openreview.net/forum?id=\\nOWELckerm6.\\n[39] S. Massei, L. Robol, and D. Kressner. hm-toolbox: Matlab software for hodlr and hss matrices.\\nSIAM Journal on Scientific Computing, 42(2):C43–C68, 2020.\\n[40] W. Merrill, J. Petty, and A. Sabharwal. The illusion of state in state-space models. In Proceedings\\nof ICML, 2024.\\n[41] T. Munkhdalai, M. Faruqui, and S. Gopal. Leave no context behind: Efficient infinite context\\ntransformers with infini-attention, 2024. URL https://arxiv.org/abs/2404.07143.\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='[42] T. Nguyen, V. Suliafu, S. Osher, L. Chen, and B. Wang. Fmmformer: Efficient and flexible\\ntransformer via decomposed near-field and far-field attention. In Proceedings of NeurIPS, 2021.\\n[43] C.-A. Oncescu, S. Purandare, S. Idreos, and S. M. Kakade. Flash inference: Near linear time\\ninference for long convolution sequence models and beyond. In The Thirteenth International\\nConference on Learning Representations, 2025. URL https://openreview.net/forum?\\nid=cZWCjan02B.\\n[44] B. Peng, D. Goldstein, Q. Anthony, A. Albalak, E. Alcaide, S. Biderman, E. Cheah, T. Ferdinan,\\nH. Hou, P. Kazienko, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic\\nrecurrence. arXiv preprint arXiv:2404.05892, 3, 2024.\\n[45] B. Peng, R. Zhang, D. Goldstein, E. Alcaide, X. Du, H. Hou, J. Lin, J. Liu, J. Lu, W. Merrill, et al.\\nRwkv-7\" goose\" with expressive dynamic state evolution. arXiv preprint arXiv:2503.14456,\\n2025.\\n[46] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong. In Proceedings of\\nICLR, 2021.\\n[47] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and\\nC. Ré.\\nHyena hierarchy: Towards larger convolutional language models.\\nIn A. Krause,\\nE. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, International Conference\\non Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202\\nof Proceedings of Machine Learning Research, pages 28043–28078. PMLR, 2023. URL\\nhttps://proceedings.mlr.press/v202/poli23a.html.\\n[48] Z. Qin and Y. Zhong. Accelerating toeplitz neural network with constant-time inference\\ncomplexity. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference\\non Empirical Methods in Natural Language Processing, pages 12206–12215, Singapore, Dec.\\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.750.\\nURL https://aclanthology.org/2023.emnlp-main.750/.\\n[49] Z. Qin, W. Sun, H. Deng, D. Li, Y. Wei, B. Lv, J. Yan, L. Kong, and Y. Zhong. cosformer:\\nRethinking softmax in attention. In Proceedings of ICLR, 2022.\\n[50] Z. Qin, X. Han, W. Sun, B. He, D. Li, D. Li, Y. Dai, L. Kong, and Y. Zhong. Toeplitz neural\\nnetwork for sequence modeling. In Proceedings of ICLR, 2023.\\n[51] Z. Qin, W. Sun, D. Li, X. Shen, W. Sun, and Y. Zhong. Lightning attention-2: A free lunch for\\nhandling unlimited sequence lengths in large language models. arXiv preprint arXiv:2401.04658,\\n2024.\\n[52] Z. Qin, S. Yang, W. Sun, X. Shen, D. Li, W. Sun, and Y. Zhong. HGRN2: Gated Linear RNNs\\nwith State Expansion. In Proceedings of CoLM, 2024.\\n[53] S. Qiu, A. Potapczynski, M. Finzi, M. Goldblum, and A. G. Wilson. Compute better spent:\\nReplacing dense layers with structured matrices. ArXiv, abs/2406.06248, 2024. URL https:\\n//api.semanticscholar.org/CorpusID:270371652.\\n[54] B. Y. Ryabko. A fast on-line adaptive code. IEEE Trans. Inf. Theory, 38:1400–1404, 1992.\\nURL https://api.semanticscholar.org/CorpusID:206392294.\\n[55] I. Schlag, K. Irie, and J. Schmidhuber. Linear Transformers Are Secretly Fast Weight Program-\\nmers. In Proceedings of ICML, 2021.\\n[56] J. Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent\\nnetworks. Neural Computation, 4(1):131–139, 1992.\\n[57] J. Shah, G. Bikshandi, Y. Zhang, V. Thakkar, P. Ramani, and T. Dao. FlashAttention-3: Fast\\nand accurate attention with asynchrony and low-precision. In Proceedings of NeurIPS, 2024.\\n[58] J. Shi, K. A. Wang, and E. B. Fox. Sequence modeling with multiresolution convolutional\\nmemory, 2023. URL https://arxiv.org/abs/2305.01638.\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='[59] J. Siems, T. Carstensen, A. Zela, F. Hutter, M. Pontil, and R. Grazzi. Deltaproduct: Improving\\nstate-tracking in linear rnns via householder products. arXiv preprint arXiv:2502.10297, 2025.\\n[60] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei. Retentive network: A\\nsuccessor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023.\\n[61] Y. Sun, X. Li, K. Dalal, J. Xu, A. Vikram, G. Zhang, Y. Dubois, X. Chen, X. Wang, S. Koyejo,\\nT. Hashimoto, and C. Guestrin. Learning to (learn at test time): Rnns with expressive hidden\\nstates, 2025. URL https://arxiv.org/abs/2407.04620.\\n[62] P. Tillet, H.-T. Kung, and D. Cox. Triton: an intermediate language and compiler for tiled neural\\nnetwork computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on\\nMachine Learning and Programming Languages, pages 10–19, 2019.\\n[63] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and\\nI. Polosukhin. Attention is all you need. In Proceedings of NeurIPS, 2017.\\n[64] B. Widrow, M. E. Hoff, et al. Adaptive switching circuits. In IRE WESCON convention record,\\nvolume 4, pages 96–104. New York, 1960.\\n[65] Y. Xiong, Z. Zeng, R. Chakraborty, M. Tan, G. Fung, Y. Li, and V. Singh. Nyströmformer: A\\nnyström-based algorithm for approximating self-attention. In Proceedings of AAAI, 2021.\\n[66] S. Yang and Y. Zhang.\\nFla: A triton-based library for hardware-efficient implementa-\\ntions of linear attention mechanism, Jan. 2024.\\nURL https://github.com/fla-org/\\nflash-linear-attention.\\n[67] S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim. Gated linear attention transformers with\\nhardware-efficient training. arXiv preprint arXiv:2312.06635, 2023.\\n[68] S. Yang, J. Kautz, and A. Hatamizadeh. Gated delta networks: Improving mamba2 with delta\\nrule. In Proceedings of ICLR, 2024.\\n[69] S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim. In Proceedings of ICML, 2024.\\n[70] S. Yang, B. Wang, Y. Zhang, Y. Shen, and Y. Kim. Parallelizing linear transformers with the\\ndelta rule over sequence length. In Proceedings of NeurIPS, 2024.\\n[71] M. Yau, S. Gupta, V. Engelmayer, K. Irie, S. Jegelka, and J. Andreas. Sequential-parallel duality\\nin prefix scannable models. arXiv preprint arXiv:2506.10918, 2025.\\n[72] Z. Ye, Q. Guo, Q. Gan, X. Qiu, and Z. Zhang. Bp-transformer: Modelling long-range context\\nvia binary partitioning. arXiv preprint arXiv:1911.04070, 2019.\\n[73] Z. Zeng, S. Pal, J. Kline, G. M. Fung, and V. Singh. Multi resolution analysis (mra) for\\napproximate self-attention, 2022. URL https://arxiv.org/abs/2207.10284.\\n[74] Y. Zhang and S. Yang. Flame: Flash language modeling made easy, Jan. 2025. URL https:\\n//github.com/fla-org/flame.\\n[75] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang. Informer: Beyond\\nefficient transformer for long sequence time-series forecasting. In Proceedings of AAAI, 2021.\\n[76] Z. Zhu and R. Soricut. H-transformer-1d: Fast one-dimensional hierarchical attention for\\nsequences, 2021. URL https://arxiv.org/abs/2107.11906.\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='Model\\nTemporal Structure\\nHidden Size Structure\\nMamba-2\\nSemiseparable\\nScaled Identity\\nGated DeltaNet\\nSemiseparable\\nIdentity plus Low-Rank\\nLog-Linear Mamba-2\\nHierarchical\\nScaled Identity\\nLog-Linear Gated DeltaNet\\nHierarchical\\nIdentity plus Low-Rank\\nTable 5: Structural comparison of different attention variants.\\nA\\nGeneralizing Log-Linear Attention to More Expressive Linear RNNs\\nThe main paper adopts the following unified view of efficient attention (Eq. 1):\\nP = A ⊙M,\\nO = PV,\\nThis formulation reveals that the key difference between linear and log-linear attention lies in the\\nstructure of the mask matrix M ∈RT ×T . Variations among linear attention models—such as\\nMamba-2 and Gated DeltaNet—stem from different parameterizations of A. While this perspective\\noffers a unifying and intuitive framework that captures a wide range of attention mechanisms, it\\ncomes with an important limitation: the state-transition terms are restricted to be scalars (in the case\\nof Mamba-2) or identity-plus-rank-one matrices (in the case of Gated DeltaNet).\\nIn this section, we introduce a more general framework that relaxes this scalar constraint by allowing\\nstate-transition terms (including the thus λ(ℓ)\\nt\\nterms) to be matrix-valued. This extension enables\\nricher and more expressive attention mechanisms while preserving computational efficiency.\\nLinear Attention as an SSS Tensor.\\nConsider the standard linear attention mechanism with\\ndata-dependent gating and an SSS (sequentially semiseparable) mask MS:\\nP = QK⊤⊙MS,\\nO = PV.\\nIn the main paper, we extend the SSS mask MS to a hierarchical form MH. Notice that in Mamba-2,\\nthe resulting matrix P also inherits the same structural property, with its SSS-rank governed by the\\nhidden dimension d:\\nPt,s = Qt (Ct · · · Cs+1) K⊤\\ns ,\\nwhere Ct = αtI.\\nWe now define a 4D tensor MS ∈R(T ×T )×(d×d) such that:\\nPt,s = QtMt,sK⊤\\ns ,\\nwhere Mt,s = Ct · · · Cs+1.\\nEach entry Mt,s ∈Rd×d is a matrix, making MS a 4D tensor. We refer to this as an SSS tensor due\\nto its sequentially semiseparable-like structure along the temporal dimension, though this term is not\\nyet formalized in the literature.\\nThis tensor-centric view naturally accommodates matrix-valued state transitions Ct ∈Rd×d with\\narbitrary structure, offering a richer representation than scalar- or identity-plus-rank-one-based\\napproaches. In particular, models such as Mamba-2 and Gated DeltaNet can be interpreted as\\noperating on 4D tensors with different hidden-dimension structures, while still preserving temporal\\nsemiseparability.13\\nMamba-2:\\nMS\\nt,s =\\ns+1\\nY\\nt′=t\\nαt′I,\\nGated DeltaNet:\\nMS\\nt,s =\\ns+1\\nY\\nt′=t\\nαt′ \\x00I −βt′kt′k⊤\\nt′\\n\\x01\\nLog-Linear Attention as an H Tensor.\\nWe can apply our log-linear attention to these more flexible\\n(linear) RNNs by incorporating matrix-valued, level- and data-dependent terms Λ(ℓ)\\nt\\n∈Rd×d:\\nMamba-2:\\nMH\\nt,s = Λ(ℓ)\\nt\\ns+1\\nY\\nt′=t\\nαt′I,\\nGated DeltaNet:\\nMH\\nt,s = Λ(ℓ)\\nt\\ns+1\\nY\\nt′=t\\nαt′ \\x00I −βt′kt′k⊤\\nt′\\n\\x01\\nThis formulation highlights a key insight: both Mamba-2 and Gated DeltaNet share a common\\nsemiseparable structure in the temporal dimension, but differ in how they structure the hidden\\ndimension. Mamba-2 relies on scaled identities, while Gated DeltaNet applies identity-minus-rank-\\none modifications. Table 5 summarizes these distinctions.\\n13Strictly speaking, Gated DeltaNet also need to include a term βt from βtvtk⊤\\nt . For clarity, we omit it here,\\nas it can be absorbed into other terms.\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='I = {1, 2, 3, 4, 5, 6, 7, 8}\\nI(3)\\n1\\n= {1, 2, 3, 4}\\nI(3)\\n2\\n= {5, 6, 7, 8}\\nI(2)\\n1\\n= {1, 2}\\nI(2)\\n2\\n= {3, 4}\\nI(2)\\n3\\n= {5, 6}\\nI(2)\\n4\\n= {7, 8}\\nI(1)\\n1\\n= {1} I(1)\\n2\\n= {2} I(1)\\n3\\n= {3} I(1)\\n4\\n= {4} I(1)\\n5\\n= {5} I(1)\\n6\\n= {6} I(1)\\n7\\n= {7} I(1)\\n8\\n= {8}\\nℓ= 3\\nℓ= 2\\nℓ= 1\\nℓ= 0\\nFigure 8: Visualization adapted from [39, 31]: This example illustrates a cluster tree of depth 3 along with the\\ncorresponding block partitions at each level. Blocks marked with stripes are stored as low-rank matrices in the\\nHODLR format, while those filled with solid color represent dense matrices.\\nB\\nLog-Linear Attention as H Matrices\\nWe begin by introducing two classes of Hierarchical matrices (H matrices) following Massei et al.\\n[39]: HODLR (Hierarchically Off-Diagonal Low-Rank) matrices and HSS (Hierarchically Semi-\\nSeparable) matrices. We then show how Log-Linear Attention corresponds to a specific subclass of\\nH matrices that occupies an intermediate position between these two. Finally, we discuss a further\\nvariant of H matrices that, in principle, allows for more refined partitioning—potentially enhancing\\napproximation quality at the cost of increased (though constant-factor) computational complexity.\\nB.1\\nHODLR Matrices\\nHODLR (Hierarchically Off-Diagonal Low-Rank) matrices are structured matrices built via recursive\\npartitioning, where off-diagonal blocks are low-rank at every level. This structure is formalized\\nusing a cluster tree [39]. Let T be the matrix dimension, and let T be a perfectly balanced binary\\ntree of depth L whose nodes are subsets of {1, . . . , T}. We say T is a cluster tree if: (1) the root\\nis I = {1, . . . , T}; (2) each level partitions indices into contiguous blocks; (3) every node I(ℓ)i at\\nlevel ℓhas two children I(ℓ−1)\\n2i−1 and I(ℓ−1)\\n2i\\nthat form a disjoint partition of the parent. See Fig. 8 for a\\nvisual example of such a hierarchical partitioning.\\nNow, let M ∈RT ×T be a square matrix and T a cluster tree as described above. We say that M is a\\n(T , k)-HODLR matrix if,\\nrank\\n\\x10\\nM[I(ℓ)\\ni\\n, I(ℓ)\\nj ]\\n\\x11\\n≤k,\\n∀I(ℓ)\\ni\\n, I(ℓ)\\nj\\n∈sibilings (T )\\nThis hierarchical low-rank structure enables efficient O(T log T) storage and matrix-vector multipli-\\ncation, making HODLR matrices a core component in fast algorithms for dense matrix computations.\\nHODLR belongs to the broader class of rank-structured matrices known as Hierarchical matrices (H\\nmatrices).\\nB.2\\nHSS Matrices\\nThe O(T log T) memory complexity of HODLR matrices arises from their recursive structure: they\\nconsist of O(log T) levels, each storing low-rank factorizations that require O(T) space. In cases\\nwhere these low-rank factors exhibit linear dependencies across levels, it is possible to exploit these\\nrelationships through nested hierarchical low-rank representations, potentially reducing the memory\\ncomplexity to O(T) by eliminating the logarithmic factor [39].\\nLet I(ℓ)\\ni\\nand I(ℓ)\\nj\\ndenote a pair of sibling clusters at level ℓin the cluster tree T . Define n(ℓ) = 2ℓ−1 as\\nthe block size at level ℓ. The off-diagonal block corresponding to these clusters can be parameterized\\nas:\\nM[I(ℓ)\\ni\\n, I(ℓ)\\nj ] = U(ℓ)\\ni Σ(ℓ)\\ni,j\\n\\x10\\nV(ℓ)\\nj\\n\\x11⊤\\n,\\nwhere\\nU(ℓ)\\ni , V(ℓ)\\nj\\n∈Rn(ℓ)×k, Σ(ℓ)\\ni,j ∈Rk×k\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='+\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\nFigure 9: Left: H matrices with strong admissibility. Right: H matrices with weak admissibility.\\nWe call M matrix a Hierarchically Semiseparable matrices (HSS) if low-rank factors at different\\nlevels are linearly related through some “translation operators” T(ℓ)\\nU , T(ℓ)\\nV ∈R2k×k such that,\\nU(ℓ)\\ni\\n=\\n\"\\nU(ℓ−1)\\ni1\\n0\\n0\\nU(ℓ−1)\\ni2\\n#\\nT(ℓ)\\nU,i,\\nV(ℓ)\\nj\\n=\\n\"\\nV(ℓ−1)\\nj1\\n0\\n0\\nV(ℓ−1)\\nj2\\n#\\nT(ℓ)\\nV,j\\nMore broadly, HSS matrices belong to a subclass of H matrices known as H2 matrices.\\nB.3\\nQuasi-Hierarchical Matrix.\\nAs discussed above, when the low-rank basis matrices U(ℓ) and V(ℓ) exhibit linear relationships\\nacross levels ℓ, the matrix M reduces to a semiseparable form. In this case, both storage and\\nmatrix-vector multiplication complexities can be reduced to O(T). Otherwise, M retains the general\\nhierarchical structure with O(T log T) complexity.\\nWe define a Quasi-Hierarchical Matrix as one in which only one of the basis sequences, either U(ℓ)\\nor V(ℓ), satisfies such a linear nesting property across levels, while the other does not. The matrix\\nMH used in the Log-Linear model (Eq. 4) is an instance of this structure.\\nBoth Hierarchical and Quasi-Hierarchical matrices incur O(T log T) complexity for storage and\\ncomputation during training. However, the use of Quasi-Hierarchical matrices plays a crucial role\\nin enabling O(log T) complexity during inference. We are not aware of a recurrent algorithm for\\ngeneral Hierarchical matrices that achieves logarithmic inference complexity.14\\nReparameterization.\\nMore precisely, Eq. 4 represents a Quasi-Hierarchical matrix that has been\\nspecifically re-parameterized as a composition of the scalar weights λ(ℓ) and a sequentially semisep-\\narable (SSS) matrix MS. This reparameterization serves two purposes: first, to highlight the\\nconnection between our use of H matrices and the SSS format adopted in prior work; and second, to\\nenable the block decomposition into a hierarchy of SSS matrices, as shown in Eq. 3.2.\\nWe present this re-parameterization below, along with its 4D tensor variant discussed in §A, where\\nwe additionally assume that the matrices Ui and Vj are invertible.\\nMatrix:\\nTensor:\\nMH\\ni,j := τ (ℓ)\\ni\\nuivj ⇔λ(ℓ)\\ni\\ni\\nY\\nt=j+1\\nαt\\nMH\\ni,j := T(ℓ)\\ni UiV⊤\\nj ⇔Λ(ℓ)\\ni\\nj+1\\nY\\nt=i\\nCt\\n⇒\\nτ (ℓ)\\ni\\n:= λ(ℓ)\\ni , ui :=\\ni\\nY\\nt=0\\nαt, vj :=\\nj\\nY\\nt=0\\n1\\nαt\\nT(ℓ)\\ni\\n:= Λ(ℓ)\\ni , Ui :=\\n0\\nY\\nt=i\\nCt, V⊤\\nj :=\\nj\\nY\\nt=0\\nC−1\\nt\\n⇐\\nλ(ℓ)\\ni\\n:= τ (ℓ)\\ni\\nuivi,\\nat := rt−1\\nrt\\nΛ(ℓ)\\ni\\n:= T(ℓ)\\ni UiV⊤\\ni ,\\nCt := R−1\\nt Rt−1\\nB.4\\nH Matrices with Strong and Weak Admissibility\\nIn the recurrent formulation of Log-Linear Attention, although there are O(log T) states correspond-\\ning to different hierarchical levels, roughly half of them are zero in practice. This sparsity arises from\\n14In fact, our initial attempts involved using fully Hierarchical matrices, but we were unable to derive a\\nrecurrent formulation with O(log T) complexity. This motivated the design of Quasi-Hierarchical matrices\\nspecifically to support efficient recurrence.\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='Figure 10: Left: H matrices with strong admissibility. Right: H matrices with weak admissibility.\\nthe specific structure of HODLR matrices, which belong to a broader class of H matrices known as\\nweakly admissible [21].\\nFigures 10 and 9 illustrate an alternative structure based on strong (or standard) admissibility. Unlike\\nthe weakly admissible variant, strongly admissible H matrices allow for finer-grained partitioning of\\nthe matrix, and their corresponding recurrent forms utilize all hierarchical levels.\\nWhile strong admissibility can yield more accurate approximations, it comes with a significant com-\\nputational cost [21]. In our early experiments, using strong admissibility in a Triton implementation\\nresulted in up to a 4x slowdown, with only marginal improvements in accuracy. As a result, we adopt\\nthe weakly admissible structure throughout this work and refer to it simply as the H-matrix.\\nC\\nImplementations\\n1\\nimport\\ntorch\\n2\\nimport\\nnumpy as np\\n3\\nimport\\ntorch.nn.functional\\nas F\\n4\\n5\\n6\\ndef\\nsegsum(x):\\n7\\nT = x.size (-1)\\n8\\nx_cumsum = torch.cumsum(x, dim=-1)\\n9\\nx_segsum = x_cumsum [... , :, None] - x_cumsum [... , None , :]\\n10\\nmask = torch.tril(torch.ones(T, T, device=x.device , dtype=bool))\\n11\\nx_segsum = x_segsum.masked_fill (~mask , -torch.inf)\\n12\\nreturn\\nx_segsum\\n13\\n14\\n15\\ndef\\nlevel_mask(level , T):\\n16\\nif level == 0:\\n17\\nreturn\\ntorch.eye(T, dtype=torch.bool)\\n18\\n19\\ni, j = torch.meshgrid(torch.arange(T), torch.arange(T), indexing=\"ij\")\\n20\\nhalf = 1 << (level - 1)\\n21\\nclipped = i - (i %\\n22\\nvalid = (i %\\n23\\nreturn\\nvalid\\n24\\n25\\n26\\ndef\\nconstruct_H_matrix (a, L):\\n27\\nT = a.size (-1)\\n28\\nA = torch.exp(segsum(a))\\n29\\nreturn\\nsum([A * L[... , level , :]. unsqueeze (-1) * level_mask(level , T) for\\nlevel in range(int(np.\\nlog2(T)) + 1)])\\n30\\n31\\n32\\ndef\\nhattention(X, A, B, C, L, block_len =8):\\n33\\n\"\"\"\\n34\\nArguments:\\n35\\nX: (batch , length , n_heads , d_head)\\n36\\nA: (batch , length , n_heads)\\n37\\nB: (batch , length , n_heads , d_state)\\n38\\nC: (batch , length , n_heads , d_state)\\n39\\nL: (batch , length , n_heads , num_levels) where\\nnum_levels = log2(length) + 1\\n40\\nReturn:\\n41\\nY: (batch , length , n_heads , d_head)\\n42\\n\"\"\"\\n43\\nT = X.shape [1]\\n44\\nassert X.dtype == A.dtype == B.dtype == C.dtype\\n45\\nassert X.shape [1] %\\n46\\ninput_shape = X.shape\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 19}, page_content='47\\n# Rearrange\\ninto\\nblocks/chunks\\n48\\nb, cl = X.shape [0], X.shape [1]\\n49\\nc = cl //\\nblock_len\\n50\\nX, A, B, C, L = [x.reshape(b, c, block_len , *x.shape [2:])\\nfor x in (X, A, B, C, L)]\\n51\\nA = A.permute (0, 3, 1, 2)\\n# (batch , n_heads , c, block_len)\\n52\\nA_cumsum = torch.cumsum(A, dim=-1)\\n# (batch , n_heads , c, block_len)\\n53\\n54\\nnum_intra_chunk_levels = int(np.log2(block_len)) + 1\\n55\\nnum_inter_chunk_levels = int(np.log2(T)) + 1 - num_intra_chunk_levels\\n56\\n# Partition\\nthe\\nlambda\\ninto intra -chunk\\nand inter -chunk\\nlambda\\n57\\nL_intra , L_inter = L[... , : num_intra_chunk_levels ], L[... ,\\nnum_intra_chunk_levels :]\\n58\\nL_intra = L_intra.permute (0, 3, 1, 4, 2)\\n# (batch , n_heads , num_chunks , num_levels , block_len)\\n59\\n60\\n# Intra -chunk\\nComputation\\n61\\nH = construct_H_matrix (A, L_intra)\\n# Materialize\\nthe H matrix as a dense\\nmatrix\\n62\\nY_diag = torch.einsum(\"bclhn ,bcshn ,bhcls ,bcshp ->bclhp\", C, B, H, X)\\n63\\n64\\n# Inter -chunk\\nComputation\\n65\\ndecay_states = torch.exp (( A_cumsum [... ,\\n-1:] - A_cumsum))\\n66\\nstates = torch.einsum(\"bclhn ,bhcl ,bclhp ->bchpn\", B, decay_states , X)\\n67\\ndecay_chunk = F.pad(torch.exp(segsum(A_cumsum [... ,\\n-1])), (0, 0, 1, 0))[... , :-1, :]\\n68\\nstate_decay_out = torch.exp(A_cumsum)\\n69\\n70\\ndef\\ncompute_Y_off_level (states , level):\\n71\\nmask = level_mask(level + 1, c).unsqueeze (0).unsqueeze (0)\\n72\\ndecay_chunk_level = decay_chunk * mask\\n73\\nstates = torch.einsum(\"bhzc ,bchpn ->bzhpn\", decay_chunk_level , states)\\n74\\nY_off = torch.einsum(\\n75\\n\"bclhn ,bchpn ,bhcl ,bclh ->bclhp\",\\n76\\nC,\\n77\\nstates ,\\n78\\nstate_decay_out ,\\n79\\nL_inter [... ,\\nlevel],\\n80\\n)\\n81\\nreturn\\nY_off\\n82\\n83\\nY_off = torch.zeros_like(Y_diag)\\n84\\nfor i in range( num_inter_chunk_levels ):\\n85\\nY_off +=\\ncompute_Y_off_level (states , i)\\n86\\n87\\nY = (Y_off + Y_diag).reshape( input_shape)\\n88\\nreturn Y\\nAlgorithm 1 Chunkwise Log-Linear Attention Algorithm\\n1: for t ∈[T/C] do\\n2:\\nY[t] =\\n\\x00Q[t]K⊤\\n[t] ⊙MH\\n[t]\\n\\x01\\nV[t]\\n3: end for\\n4:\\n5: for ℓ∈[log2 (T/C)] do\\n6:\\nfor t ∈[T/C] do\\n7:\\nY[t] = Y[t] + mask(ℓ)\\nQ\\n\\x10\\nΛ(ℓ)\\n[t] ⊙Q[t]S[t]\\n\\x11\\n8:\\nS[t+1] = mask(ℓ)\\nA\\n\\x00A[t]S[t]\\n\\x01\\n+ mask(ℓ)\\nK\\n\\x00K[t]V⊤\\n[t]\\n\\x01\\n9:\\nend for\\n10: end for\\n11: return Y\\nA naive implementation computes each level independently using a Mamba-2-style primitive, then\\nsums the outputs—leading to redundant memory access and kernel launches. To improve efficiency,\\nwe fuse computation across four levels into a single Triton kernel, which we found optimal given\\nSRAM constraints on an H100.\\nFor backpropagation, we unify gradient computation across all levels for ∇K and ∇V by analytically\\nfactoring their dependencies. This reduces kernel count and improves memory efficiency, achieving\\nover 3× speedup compared to the naive multi-level version.\\nD\\nAdditional Experiment Details\\nFor the implementation benchmarks, all experiments were conducted on an H100 GPU with a batch\\nsize of 2, using 48 attention heads, a head dimension of 64, and a chunk size of 64. In Mamba-2-style\\nmodels, the attention heads are applied to V (MVA pattern), whereas in FlashAttention-2, we adopt\\nGQA-style attention by applying heads to Q. The dimensions of the Q and K states are set to 128,\\naligning with common training configurations.\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'data/pdf_files/log_linear_attention.pdf', 'file_path': 'data/pdf_files/log_linear_attention.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': 'Log-Linear Attention', 'author': 'Han Guo; Songlin Yang; Tarushii Goel; Eric P. Xing; Tri Dao; Yoon Kim', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content='For the MQAR experiments, we largely follow the setup described in Arora et al. [2]. Models are\\ntrained and evaluated on 256-token sequences containing between 4 and 64 key-value pairs. We\\ndo not evaluate on sequences longer than those used in training (i.e., no length generalization). In\\n(Log-Linear) Mamba-2 models, both the state and head dimensions are set to 16. For (Log-Linear)\\nGated DeltaNet, we use two attention heads by default, except for models with a dimension of 16,\\nwhere a single head is used. We tune the learning rate and, for Log-Linear models, also tune the\\nparameterization of λ. We run each configuration with five seeds. Training was early stopped when\\naccuracy exceeded 99%.\\nFor the language modeling experiments, each run was performed on 8×A100 or 8×H100 GPUs over\\nthe course of several days. We do not tie word embeddings, use a vocabulary size of 32,000, and set\\nthe initializer range to 0.006. Training is performed with a global batch size of approximately 524K\\ntokens for 95K steps (roughly 50B tokens). We use the flash-linear-attention and flame\\nlibraries [66, 74], following most of their default configurations.\\nDetailed Experimental Results. Tables 6 and 7 provide detailed results on the MQAR and Needle-\\nIn-A-Haystack tasks.\\nModel / Dimension\\n16\\n32\\n64\\nTransformer\\n≥99\\n≥99\\n≥99\\nMamba-2\\n46.9 (2.3)\\n75.1 (4.9)\\n89.6 (6.1)\\nw/ Log-Linear\\n55.9 (9.1)\\n76.5 (4.8)\\n92.9 (2.7)\\nGated DeltaNet\\n38.4 (1.0)\\n79.0 (2.1)\\n≥99\\nw/ Log-Linear\\n40.0 (1.4)\\n84.4 (1.2)\\n≥99\\nTable 6: Average accuracies and standard deviations (in parentheses) on MQAR over 5 seeds. Training was\\nearly stopped when accuracy exceeded 99%.\\nS-NIAH-1\\nS-NIAH-2\\nS-NIAH-3\\n(pass-key retrieval)\\n(number in haystack)\\n(uuid in haystack)\\nModel\\n4K\\n8K\\n16K\\n4K\\n8K\\n16K\\n4K\\n8K\\n16K\\nTransformer\\n72.6\\n76.0\\n16.6\\n100.0\\n99.8\\n90.0\\n77.4\\n67.0\\n44.6\\nw/ 24 Layers\\n92.4\\n78.4\\n89.8\\n100.0\\n100.0\\n99.6\\n84.0\\n63.6\\n36.4\\nMamba-2\\n90.4\\n56.8\\n21.6\\n72.4\\n28.0\\n18.6\\n4.0\\n3.6\\n0.8\\nw/ Log-Linear\\n100.0\\n99.8\\n72.4\\n89.8\\n68.2\\n12.8\\n33.6\\n22.6\\n2.0\\nGated DeltaNet\\n100.0\\n100.0\\n100.0\\n95.8\\n46.8\\n5.0\\n66.2\\n14.6\\n6.0\\nw/ Log-Linear\\n100.0\\n100.0\\n100.0\\n95.6\\n59.6\\n9.2\\n48.8\\n13.0\\n8.8\\nMK-NIAH-1\\nMQ-NIAH\\nMV-NIAH\\n(multi-key line retrieval)\\n(multi-query)\\n(multi-value)\\nModel\\n4K\\n8K\\n16K\\n4K\\n8K\\n16K\\n4K\\n8K\\n16K\\nTransformer (L)\\n79.4\\n83.0\\n61.4\\n58.9\\n48.0\\n29.8\\n37.5\\n34.1\\n21.5\\nw/ 24 Layers\\n62.6\\n83.2\\n75.2\\n54.6\\n46.0\\n34.5\\n48.4\\n45.4\\n32.3\\nMamba-2\\n27.2\\n18.6\\n13.6\\n28.7\\n19.4\\n1.3\\n27.9\\n14.8\\n4.4\\nw/ Log-Linear\\n43.2\\n39.8\\n21.2\\n26.6\\n22.4\\n6.6\\n28.1\\n22.8\\n8.9\\nGated DeltaNet\\n23.0\\n21.2\\n5.2\\n21.6\\n16.9\\n7.2\\n16.2\\n14.5\\n7.0\\nw/ Log-Linear\\n49.4\\n27.8\\n10.2\\n34.9\\n22.0\\n9.8\\n31.4\\n25.0\\n13.3\\nTable 7: NIAH experiments, including three single-needle tasks—S-NIAH-1 (passkey retrieval), S-NIAH-2\\n(numerical needle), and S-NIAH-3 (UUID-based needle)—and three multi-needle variants: MK-NIAH-1 (multi-\\nkey line retrieval), MQ-NIAH (multi-query), and MV-NIAH (multi-value).\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 0}, page_content='Attention Mechanism in Neural Networks:\\nWhere it Comes and Where it Goes\\nDerya Soydaner\\nReceived: 22 July 2021 / Accepted: 27 April 2022\\nAbstract A long time ago in the machine learning literature, the idea of\\nincorporating a mechanism inspired by the human visual system into neural\\nnetworks was introduced. This idea is named the attention mechanism, and it\\nhas gone through a long development period. Today, many works have been\\ndevoted to this idea in a variety of tasks. Remarkable performance has re-\\ncently been demonstrated. The goal of this paper is to provide an overview\\nfrom the early work on searching for ways to implement attention idea with\\nneural networks until the recent trends. This review emphasizes the impor-\\ntant milestones during this progress regarding diﬀerent tasks. By this way,\\nthis study aims to provide a road map for researchers to explore the current\\ndevelopment and get inspired for novel approaches beyond the attention.\\nKeywords Attention mechanism · Neural networks · Deep learning · Survey\\n1 Introduction\\nHuman eye sees the world in an interesting way. We suppose as if we see the\\nentire scene at once, but this is an illusion created by the subconscious part\\nof our brain [1]. According to the Scanpath theory [2,3], when the human eye\\nlooks at an image, it can see only a small patch in high resolution. This small\\npatch is called the fovea. It can see the rest of the image in low resolution which\\nis called the periphery. To recognize the entire scene, the eye performs feature\\nextraction based on the fovea. The eye is moved to diﬀerent parts of the image\\nuntil the information obtained from the fovea is suﬃcient for recognition [4].\\nThese eye movements are called saccades. The eye makes successive ﬁxations\\nDerya Soydaner\\nDepartment of Brain and Cognition, University of Leuven (KU Leuven), Leuven, Belgium\\nTel.: +32-16710471\\nE-mail: derya.soydaner@kuleuven.be\\narXiv:2204.13154v1  [cs.LG]  27 Apr 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 1}, page_content='2\\nDerya Soydaner\\nuntil the recognition task is complete. This sequential process happens so\\nquickly that we feel as if it happens all at once.\\nBiologically, this is called visual attention system. Visual attention is de-\\nﬁned as the ability to dynamically restrict processing to a subset of the visual\\nﬁeld [5]. It seeks answers for two main questions: What and where to look?\\nVisual attention has been extensively studied in psychology and neuroscience;\\nfor reviews see [6,7,8,9,10]. Besides, there is a large amount of literature on\\nmodeling eye movements [11,12,13,14]. These studies have been a source of\\ninspiration for many artiﬁcial intelligence tasks. It has been discovered that\\nthe attention idea is useful from image recognition to machine translation.\\nTherefore, diﬀerent types of attention mechanisms inspired from the human\\nvisual system have been developed for years. Since the success of deep neural\\nnetworks has been at the forefront for these artiﬁcial intelligence tasks, these\\nmechanisms have been integrated into neural networks for a long time.\\nThis survey is about the journey of attention mechanisms used with neu-\\nral networks. Researchers have been investigating ways to strengthen neural\\nnetwork architectures with attention mechanisms for many years. The pri-\\nmary aim of these studies is to reduce computational burden and to improve\\nthe model performance as well. Previous work reviewed the attention mecha-\\nnisms from diﬀerent perspectives [15], or examined them in context of natural\\nlanguage processing (NLP) [16,17]. However, in this study, we examine the\\ndevelopment of attention mechanisms over the years, and recent trends. We\\nbegin with the ﬁrst attempts to integrate the visual attention idea to neural\\nnetworks, and continue until the most modern neural networks armed with at-\\ntention mechanisms. One of them is the Transformer, which is used for many\\nstudies including the GPT-3 language model [18], goes beyond convolutions\\nand recurrence by replacing them with only attention layers [19]. Finally, we\\ndiscuss how much more can we move forward, and what’s next?\\n2 From the Late 1980s to Early 2010s: The Attention Awakens\\nThe ﬁrst attempts at adapting attention mechanisms to neural networks go\\nback to the late 1980s. One of the early studies is the improved version of\\nthe Neocognitron [20] with selective attention [21]. This study is then mod-\\niﬁed to recognize and segment connected characters in cursive handwriting\\n[22]. Another study describes VISIT, a novel model that concentrates on its\\nrelationship to a number of visual areas of the brain [5]. Also, a novel archi-\\ntecture named Signal Channelling Attentional Network (SCAN) is presented\\nfor attentional scanning [23].\\nEarly work on improving the attention idea for neural networks includes\\na variety of tasks such as target detection [24]. In another study, a visual at-\\ntention system extracts regions of interest by combining the bottom-up and\\ntop-down information from the image [25]. A recognition model based on se-\\nlective attention which analyses only a small part of the image at each step,\\nand combines results in time is described [4]. Besides, a model based on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 2}, page_content='Attention Mechanism in Neural Networks:\\n3\\nconcept of selective tuning is proposed [26]. As the years go by, several studies\\nthat use the attention idea in diﬀerent ways have been presented for visual\\nperception and recognition [27,28,29,30].\\nBy the 2000s, the studies on making attention mechanisms more useful for\\nneural networks continued. In the early years, a model that integrates an at-\\ntentional orienting where pathway and an object recognition what pathway is\\npresented [31]. A computational model of human eye movements is proposed\\nfor an object class detection task [32]. A serial model is presented for visual pat-\\ntern recognition gathering Markov models and neural networks with selective\\nattention on the handwritten digit recognition and face recognition problems\\n[33]. In that study, a neural network analyses image parts and generates pos-\\nterior probabilities as observations to the Markov model. Also, attention idea\\nis used for object recognition [34], and the analysis of a scene [35]. An inter-\\nesting study proposes to learn sequential attention in real-world visual object\\nrecognition using a Q-learner [36]. Besides, a computational model of visual\\nselective attention is described to automatically detect the most relevant parts\\nof a color picture displayed on a television screen [37]. The attention idea is\\nalso used for identifying and tracking objects in multi-resolution digital video\\nof partially cluttered environments [38].\\nIn 2010, the ﬁrst implemented system inspired by the fovea of human retina\\nwas presented for image classiﬁcation [39]. This system jointly trains a re-\\nstricted Boltzmann machine (RBM) and an attentional component called the\\nﬁxation controller. Similarly, a novel attentional model is implemented for si-\\nmultaneous object tracking and recognition that is driven by gaze data [40].\\nBy taking advantage of reinforcement learning, a novel recurrent neural net-\\nwork (RNN) is described for image classiﬁcation [41]. Deep Attention Selective\\nNetwork (DasNet), a deep neural network with feedback connections that are\\nlearned through reinforcement learning to direct selective attention to certain\\nfeatures extracted from images, is presented [42]. Additionally, a deep learning\\nbased framework using attention has been proposed for generative modeling\\n[43].\\n3 2015: The Rise of Attention\\nIt can be said that 2015 is the golden year of attention mechanisms. Because\\nthe number of attention studies has grown like an avalanche after three main\\nstudies presented in that year. The ﬁrst one proposed a novel approach for\\nneural machine translation (NMT) [44]. As it is known, most of the NMT\\nmodels belong to a family of encoder-decoders [45,46], with an encoder and a\\ndecoder for each language. However, compressing all the necessary information\\nof a source sentence into a ﬁxed-length vector is an important disadvantage of\\nthis encoder-decoder approach. This usually makes it diﬃcult for the neural\\nnetwork to capture all the semantic details of a very long sentence [1].\\nThe idea that [44] introduced is an extension to the conventional NMT\\nmodels. This extension is composed of an encoder and decoder as shown in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 3}, page_content='4\\nDerya Soydaner\\nFig. 1 The extension to the conventional NMT models that is proposed by [44]. It generates\\nthe t-th target word yt given a source sentence (x1, x2, ..., xT ).\\nFig 1. The ﬁrst part, encoder, is a bidirectional RNN (BiRNN) [47] that takes\\nword vectors as input. The forward and backward states of BiRNN are com-\\nputed. Then, an annotation aj for each word xj is obtained by concatenating\\nthese forward and backward hidden states. Thus, the encoder maps the input\\nsentence to a sequence of annotations (a1, ..., aTx). By using a BiRNN rather\\nthan conventional RNN, the annotation of each word can summarize both\\nthe preceding words and the following words. Besides, the annotation aj can\\nfocus on the words around xj because of the inherent nature of RNNs that\\nrepresenting recent inputs better.\\nIn decoder, a weight αij of each annotation aj is obtained by using its\\nassociated energy eij that is computed by a feedforward neural network f as\\nin Eq. (1). This neural network f is deﬁned as an alignment model that can\\nbe jointly trained with the proposed architecture. In order to reduce compu-\\ntational burden, a multilayer perceptron (MLP) with a single hidden layer is\\nproposed as f. This alignment model tells us about the relation between the\\ninputs around position j and the output at position i. By this way, the decoder\\napplies an attention mechanism. As it is seen in Eq. (2), the αij is the output\\nof softmax function:\\neij = f(hi−1, aj)\\n(1)\\nαij =\\nexp(eij)\\nPTx\\nk=1 exp(eik)\\n(2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 4}, page_content='Attention Mechanism in Neural Networks:\\n5\\nHere, the probability αij determines the importance of annotation aj with\\nrespect to the previous hidden state hi−1. Finally, the context vector ci is\\ncomputed as a weighted sum of these annotations as follows [44]:\\nci =\\nTx\\nX\\nj=1\\nαijaj\\n(3)\\nBased on the decoder state, the context and the last generated word, the\\ntarget word yt is predicted. In order to generate a word in a translation, the\\nmodel searches for the most relevant information in the source sentence to\\nconcentrate. When it ﬁnds the appropriate source positions, it makes the pre-\\ndiction. By this way, the input sentence is encoded into a sequence of vectors\\nand a subset of these vectors is selected adaptively by the decoder that is rel-\\nevant to predicting the target [44]. Thus, it is no longer necessary to compress\\nall the information of a source sentence into a ﬁxed-length vector.\\nThe second study is the ﬁrst visual attention model in image captioning\\n[48]. Diﬀerent from the previous study [44], it uses a deep convolutional neural\\nnetwork (CNN) as an encoder. This architecture is an extension of the neural\\nnetwork [49] that encodes an image into a compact representation, followed by\\nan RNN that generates a corresponding sentence. Here, the annotation vectors\\nai ∈RD are extracted from a lower convolutional layer, each of which is a D-\\ndimensional representation corresponding to a part of the image. Thus, the\\ndecoder selectively focuses on certain parts of an image by weighting a subset\\nof all the feature vectors [48]. This extended architecture uses attention for\\nsalient features to dynamically come to the forefront instead of compressing\\nthe entire image into a static representation.\\nThe context vector ct represents the relevant part of the input image at\\ntime t. The weight αi of each annotation vector is computed similar to Eq. (2),\\nwhereas its associated energy is computed similar to Eq. (1) by using an MLP\\nconditioned on the previous hidden state ht−1. The remarkable point of this\\nstudy is a new mechanism φ that computes ct from the annotation vectors ai\\ncorresponding to the features extracted at diﬀerent image locations:\\nct = φ(\\n\\x08\\nai\\n\\t\\n,\\n\\x08\\nαi\\n\\t\\n)\\n(4)\\nThe deﬁnition of the φ function causes two variants of attention mecha-\\nnisms: The hard (stochastic) attention mechanism is trainable by maximizing\\nan approximate variational lower bound, i.e., by REINFORCE [50]. On the\\nother side, the soft (deterministic) attention mechanism is trainable by stan-\\ndard backpropagation methods. The hard attention deﬁnes a location variable\\nst, and uses it to decide where to focus attention when generating the t-th\\nword. When the hard attention is applied, the attention locations are con-\\nsidered as intermediate latent variables. It assigns a multinoulli distribution\\nparametrized by αi, and ct becomes a random variable. Here, st,i is deﬁned\\nas a one-hot variable which is set to 1 if the i-th location is used to extract\\nvisual features [48]:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 5}, page_content='6\\nDerya Soydaner\\np(st,i = 1|sj<t, a) = αt,i\\n(5)\\nct =\\nX\\ni\\nst,iai\\n(6)\\nWhereas learning hard attention requires sampling the attention location\\nst each time, the soft attention mechanism computes a weighted annotation\\nvector similar to [44] and takes the expectation of the context vector ct directly:\\nEp(st|α)[ct] =\\nL\\nX\\ni=1\\nαt,iai\\n(7)\\nFurthermore, in training the deterministic version of the model, an alterna-\\ntive method namely doubly stochastic attention, is proposed with an additional\\nconstraint added to the training objective to encourage the model to pay equal\\nattention to all parts of the image.\\nThe third study should be emphasized presents two classes of attention\\nmechanisms for NMT: the global attention that always attends to all source\\nwords, and the local attention that only looks at a subset of source words at\\na time [51]. These mechanisms derive the context vector ct in diﬀerent ways:\\nWhereas the global attention considers all the hidden states of the encoder, the\\nlocal one selectively focuses on a small window of context. In global attention, a\\nvariable-length alignment vector is derived similar to Eq. (2). Here, the current\\ntarget hidden state ht is compared with each source hidden state ¯hs by using a\\nscore function instead of the associated energy eij. Thus, the alignment vector\\nwhose size equals the number of time steps on the source side is derived.\\nGiven the alignment vector as weights, the context vector ct is computed as\\nthe weighted average over all the source hidden states. Here, score is referred\\nas a content-based function, and three diﬀerent alternatives are considered [51].\\nOn the other side, the local attention is diﬀerentiable. Firstly, an aligned\\nposition pt is generated for each target word at a time t. Then, a window\\ncentered around the source position pt is used to compute the context vector\\nas a weighted average of the source hidden states within the window. The\\nlocal attention selectively focuses on a small window of context, and obtains\\nthe alignment vector from the current target state ht and the source states ¯hs\\nin the window [51].\\nThe introduction of these novel mechanisms in 2015 triggered the rise of\\nattention for neural networks. Based on the proposed attention mechanisms,\\nsigniﬁcant research has been conducted in a variety of tasks. In order to imag-\\nine the attention idea in neural networks better, two visual examples are shown\\nin Fig. 2. A neural image caption generation task is seen in the top row that\\nimplements an attention mechanism [48]. Then, the second example shows\\nhow the attention mechanisms can be used for visual question answering [52].\\nBoth examples demonstrate how attention mechanisms focus on parts of input\\nimages.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 6}, page_content='Attention Mechanism in Neural Networks:\\n7\\nFig. 2 Examples of the attention mechanism in visual. (Top) Attending to the correct\\nobject in neural image caption generation [48]. (Bottom) Visualization of original image\\nand question pairs, and co-attention maps namely word-level, phrase-level and question-\\nlevel, respectively [52].\\n4 2015-2016: Attack of the Attention\\nDuring two years from 2015, the attention mechanisms were used for diﬀerent\\ntasks, and novel neural network architectures were presented applying these\\nmechanisms. After the memory networks [53] that require a supervision signal\\ninstructing them how to use their memory cells, the introduction of the neural\\nTuring machine [54] allows end-to-end training without this supervision signal,\\nvia the use of a content-based soft attention mechanism [1]. Then, end-to-end\\nmemory network [55] that is a form of memory network based on a recurrent\\nattention mechanism is proposed.\\nIn these years, an attention mechanism called self-attention, sometimes\\ncalled intra-attention, was successfully implemented within a neural network\\narchitecture namely Long Short-Term Memory-Networks (LSTMN) [56]. It\\nmodiﬁes the standard LSTM structure by replacing the memory cell with a\\nmemory network [53]. This is because memory networks have a set of key\\nvectors and a set of value vectors, whereas LSTMs maintain a hidden vector\\nand a memory vector [56]. In contrast to attention idea in [44], memory and\\nattention are added within a sequence encoder in LSTMN. In order to compute\\na representation of a sequence, self-attention is described as relating diﬀerent\\npositions of it [19]. One of the ﬁrst approaches of self-attention is applied for\\nnatural language inference [57].\\nMany attention-based models have been proposed for neural image cap-\\ntioning [58], abstractive sentence summarization [59], speech recognition [60,\\n61], automatic video captioning [62], neural machine translation [63], and rec-\\nognizing textual entailment [64]. Diﬀerent attention-based models perform vi-\\nsual question answering [65,66,67]. An attention-based CNN is presented for\\nmodeling sentence pairs [68]. A recurrent soft attention based model learns to\\nfocus selectively on parts of the video frames and classiﬁes videos [69].\\nOn the other side, several neural network architectures have been pre-\\nsented in a variety of tasks. For instance, Stacked Attention Network (SAN)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 7}, page_content='8\\nDerya Soydaner\\nis described for image question answering [70]. Deep Attention Recurrent Q-\\nNetwork (DARQN) integrates soft and hard attention mechanisms into the\\nstructure of Deep Q-Network (DQN) [71]. Wake-Sleep Recurrent Attention\\nModel (WS-RAM) speeds up the training time for image classiﬁcation and\\ncaption generation tasks [72]. alignDRAW model, an extension of the Deep\\nRecurrent Attention Writer (DRAW) [73], is a generative model of images\\nfrom captions using a soft attention mechanism [74]. Generative Adversarial\\nWhat-Where Network (GAWWN) synthesizes images given instructions de-\\nscribing what content to draw in which location [75].\\n5 The Transformer: Return of the Attention\\nAfter the proposed attention mechanisms in 2015, researchers published stud-\\nies that mostly modifying or implementing them to diﬀerent tasks. However,\\nin 2017, a novel neural network architecture, namely the Transformer, based\\nentirely on self-attention was presented [19]. The Transformer achieved great\\nresults on two machine translation tasks in addition to English constituency\\nparsing. The most impressive point about this architecture is that it contains\\nneither recurrence nor convolution. The Transformer performs well by replac-\\ning the conventional recurrent layers in encoder-decoder architecture used for\\nNMT with self-attention.\\nThe Transformer is composed of encoder-decoder stacks each of which has\\nsix identical layers within itself. In Fig. 3, one encoder-decoder stack is shown\\nto illustrate the model [19]. Each stack includes only attention mechanisms\\nand feedforward neural networks. As this architecture does not include any\\nrecurrent or convolutional layer, information about the relative or absolute\\npositions in the input sequence is given at the beginning of both encoder and\\ndecoder using positional encodings.\\nThe calculations of self-attention are slightly diﬀerent from the mechanisms\\ndescribed so far in this paper. It uses three vectors namely query, key and\\nvalue for each word. These vectors are computed by multiplying the input with\\nweight matrices Wq, Wk and Wv which are learned during training. In general,\\neach value is weighted by a function of the query with the corresponding key.\\nThe output is computed as a weighted sum of the values. Based on this idea,\\ntwo attention mechanisms are proposed: In the ﬁrst one, called scaled dot-\\nproduct attention, the dot products of the query with all keys are computed\\nas given in the right side of Fig. 3. Each result is divided to the square root of\\nthe dimension of the keys to have more stable gradients. They pass into the\\nsoftmax function, thus the weights for the values are obtained. Finally each\\nsoftmax score is multiplied with the value as given in Eq. (8). The authors\\npropose computing the attention on a set of queries simultaneously by taking\\nqueries and keys of dimension dk, and values of dimension dv as inputs. The\\nkeys, queries and values are packed together into matrices K, Q and V. Finally,\\nthe output matrix is obtained as follows [19]:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 8}, page_content='Attention Mechanism in Neural Networks:\\n9\\nFig. 3 The Transformer architecture and the attention mechanisms it uses in detail [19].\\n(Left) The Transformer with one encoder-decoder stack. (Center) Multi-head attention.\\n(Right) Scaled dot-product attention.\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(8)\\nThis calculation is performed by every word against the other words. This\\nleads to having values of each word relative to each other. For instance, if\\nthe word x2 is not relevant for the word x1, then the softmax score gives low\\nprobability scores. As a result, the corresponding value is decreased. This leads\\nto an increase in the value of relevant words, and those of others decrease. In\\nthe end, every word obtains a new value for itself.\\nAs seen from Fig. 3, the Transformer model does not directly use scaled\\ndot-product attention. But the attention mechanism it uses is based on these\\ncalculations. The second mechanism proposed, called the multi-head attention,\\nlinearly projects the queries, keys and values h times with diﬀerent, learned\\nlinear projections to dq, dk and dv dimensions, respectively [19]. The attention\\nfunction is performed in parallel on each of these projected versions of queries,\\nkeys and values, i.e., heads. By this way, dv-dimensional output values are\\nobtained. In order to get the ﬁnal values, they are concatenated and projected\\none last time as shown in the center of Fig. 3. By this way, the self-attention is\\ncalculated multiple times using diﬀerent sets of query, key and value vectors.\\nThus, the model can jointly attend to information at diﬀerent positions [19]:\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\n(9)\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nIn the decoder part of the Transformer, masked multi-head attention is\\napplied ﬁrst to ensure that only previous word embeddings are used when\\ntrying to predict the next word in the sentence. Therefore, the embeddings\\nthat shouldn’t be seen by the decoder are masked by multiplying with zero.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 9}, page_content='10\\nDerya Soydaner\\nAn interesting study examines the contribution made by individual atten-\\ntion heads in the encoder [76]. Also, there is an evaluation of the eﬀects of\\nself-attention on gradient propagation in recurrent networks [77]. For a deeper\\nanalysis of multi-head self-attention mechanism from a theoretical perspective\\nsee [78].\\nSelf-attention has been used successfully in a variety of tasks including\\nsentence embedding [79] and abstractive summarization [80]. It is shown that\\nself-attention can lead to improvements to discriminative constituency parser\\n[81], and speech recognition as well [82,83]. Also, the listen-attend-spell model\\n[84] has been improved with the self-attention for acoustic modeling [85].\\nAs soon as these self-attention mechanisms were proposed, they have been\\nincorporated with deep neural networks for a wide range of tasks. For instance,\\na deep learning model learned a number of large-scale tasks from multiple do-\\nmains with the aid of self-attention mechanism [86]. Novel self-attention neural\\nmodels are proposed for cross-target stance classiﬁcation [87] and NMT [88].\\nAnother study points out that a fully self-attentional model can reach com-\\npetitive predictive performance on ImageNet classiﬁcation and COCO object\\ndetection tasks [89]. Besides, developing novel attention mechanisms has been\\ncarried out such as area attention, a novel mechanism that can be used along\\nmulti-head attention [90]. It attends to areas in the memory by deﬁning the\\nkey of an area as the mean vector of the key of each item, and deﬁning the\\nvalue as the sum of all value vectors in the area.\\nWhen a novel mechanism is proposed, it is inevitable to incorporate it\\ninto the GAN framework [91]. Self-Attention Generative Adversarial Networks\\n(SAGANs) [92] introduce a self-attention mechanism into convolutional GANs.\\nDiﬀerent from the traditional convolutional GANs, SAGAN generates high-\\nresolution details using cues from all feature locations. Similarly, Attentional\\nGenerative Adversarial Network (AttnGAN) is presented for text to image\\ngeneration [93]. On the other side, a machine reading and question answering\\narchitecture called QANet [94] is proposed without any recurrent networks. It\\nuses self-attention to learn the global interaction between each pair of words\\nwhereas convolution captures the local structure of the text. In another study,\\nGated Attention Network (GaAN) controls the importance of each attention\\nhead’s output by introducing gates [95]. Another interesting study introduces\\nattentive group convolutions with a generalization of visual self-attention [96].\\nA deep transformer model is implemented for language modeling over long\\nsequences [97].\\n5.1 Self-attention variants\\nIn recent years, self-attention has become an important research direction\\nwithin the deep learning community. Self-attention idea has been examined\\nin diﬀerent aspects. For example, self-attention is handled in a multi-instance\\nlearning framework [98]. The idea of Sparse Adaptive Connection (SAC) is\\npresented for accelerating and structuring self-attention [99]. The research on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 10}, page_content='Attention Mechanism in Neural Networks:\\n11\\nTable 1 Summary of Notation\\nSymbol\\nDeﬁnition\\na\\nannotation\\nc\\ncontext vector\\nα\\nweight\\ne\\nenergy\\nf\\nfeedforward neural network\\nh\\nhidden state\\nφ\\nhard (stochastic) / soft (deterministic) attention\\ns\\nlocation variable\\np\\nsource position\\nK, Q, V\\nkeys, queries and values matrices, respectively\\nWq, Wk, Wv\\nweight matrices for queries, keys and values, respectively\\nimproving self-attention continues as well [100,101,102]. Besides, based on the\\nself-attention mechanisms proposed in the Transformer, important studies that\\nmodify the self-attention have been presented. Some of the most recent and\\nprominent studies are summarized below.\\nRelation-aware self-attention It extends the self-attention mechanism by\\nregarding representations of the relative positions, or distances between se-\\nquence elements [103]. Thus, it can consider the pairwise relationships between\\ninput elements. This type of attention mechanism deﬁnes vectors to represent\\nthe edge between two inputs. It provides learning two distinct edge represen-\\ntations that can be shared across attention heads without requiring additional\\nlinear transformations.\\nDirectional self-attention (DiSA) A novel neural network architecture\\nfor learning sentence embedding named Directional Self-Attention Network\\n(DiSAN) [104] uses directional self-attention followed by a multi-dimensional\\nattention mechanism. Instead of computing a single importance score for each\\nword based on the word embedding, multi-dimensional attention computes a\\nfeature-wise score vector for each token. To extend this mechanism to the self-\\nattention, two variants are presented: The ﬁrst one, called multi-dimensional\\n‘token2token’ self-attention generates context-aware coding for each element.\\nThe second one, called multi-dimensional ‘source2token’ self-attention com-\\npresses the sequence into a vector [104]. On the other side, directional self-\\nattention produces context-aware representations with temporal information\\nencoded by using positional masks. By this way, directional information is en-\\ncoded. First, the input sequence is transformed to a sequence of hidden states\\nby a fully connected layer. Then, multi-dimensional token2token self-attention\\nis applied to these hidden states. Hence, context-aware vector representations\\nare generated for all elements from the input sequence.\\nReinforced self-attention (ReSA) A sentence-encoding model named Re-\\ninforced Self-Attention Network (ReSAN) uses reinforced self-attention (ReSA)\\nthat integrates soft and hard attention mechanisms into a single model. ReSA'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 11}, page_content='12\\nDerya Soydaner\\nselects a subset of head tokens, and relates each head token to a small sub-\\nset of dependent tokens to generate their context-aware representations [105].\\nFor this purpose, a novel hard attention mechanism called reinforced sequence\\nsampling (RSS), which selects tokens from an input sequence in parallel and\\ntrained via policy gradient, is proposed. Given an input sequence, RSS gener-\\nates an equal-length sequence of binary random variables that indicates both\\nthe selected and discarded ones. On the other side, the soft attention provides\\nreward signals back for training the hard attention. The proposed RSS pro-\\nvides a sparse mask to self-attention. ReSA uses two RSS modules to extract\\nthe sparse dependencies between each pair of selected tokens.\\nOuter product attention (OPA) Self-Attentive Associative Memory (SAM)\\nis a novel operator based upon outer product attention (OPA) [106]. This at-\\ntention mechanism is an extension of dot-product attention [19]. OPA diﬀers\\nusing element-wise multiplication, outer product, and tanh function instead of\\nsoftmax.\\nBidirectional block self-attention (Bi-BloSA) Another mechanism, bidi-\\nrectional block self-attention (Bi-BloSA) which is simply a masked block self-\\nattention (mBloSA) with forward and backward masks to encode the tempo-\\nral order information is presented [107]. Here, mBloSA is composed of three\\nparts from its bottom to top namely intra-block self-attention, inter-block\\nself-attention and the context fusion. It splits a sequence into several length-\\nequal blocks, and applies an intra-block self-attention to each block indepen-\\ndently. Then, inter-block self-attention processes the outputs for all blocks.\\nThis stacked self-attention model results a reduction in the amount of mem-\\nory compared to a single one applied to the whole sequence. Finally, a feature\\nfusion gate combines the outputs of intra-block and inter-block self-attention\\nwith the original input, to produce the ﬁnal context-aware representations of\\nall tokens.\\nFixed multi-head attention The ﬁxed multi-head attention proposes ﬁxing\\nthe head size of the Transformer in the aim of improving the representation\\npower [108]. This study emphasizes its importance by setting the head size of\\nattention units to input sequence length.\\nSparse sinkhorn attention It is based on the idea of diﬀerentiable sorting\\nof internal representations within the self-attention module [109]. Instead of\\nallowing tokens to only attend to tokens within the same block, it operates\\non block sorted sequences. Each token attends to tokens in the sorted block.\\nThus, tokens that may be far apart in the unsorted sequence can be considered.\\nAdditionally, a variant of this mechanism named SortCut sinkhorn attention\\napplies a post-sorting truncation of the input sequence.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 12}, page_content='Attention Mechanism in Neural Networks:\\n13\\nAdaptive attention span Adaptive attention span is proposed as an alter-\\nnative to self-attention [110]. It learns the attention span of each head inde-\\npendently. To this end, a masking function inspired by [111] is used to control\\nthe attention span for each head. The purpose of this novel mechanism is to\\nreduce the computational burden of the Transformer. Additionally, dynamic\\nattention span approach is presented to dynamically change the attention span\\nbased on the current input as an extension [51,112].\\n5.2 Transformer variants\\nDiﬀerent from developing novel self-attention mechanisms, several studies have\\nbeen published in the aim of improving the performance of the Transformer.\\nThese studies mostly modify the model architecture. For instance, an addi-\\ntional recurrence encoder is preferred to model recurrence for Transformer di-\\nrectly [113]. In another study, a new weight initialization scheme is applied to\\nimprove Transformer optimization [114]. A novel positional encoding scheme\\nis used to extend the Transformer to tree-structured data [115]. Investigating\\nmodel size by handling Transformer width and depth for eﬃcient training is\\nalso an active research area [116]. Transformer is used in reinforcement learn-\\ning settings [117,118,119] and for time series forecasting in adversarial training\\nsetting [120].\\nBesides, many Transformer variants have been presented in the recent past.\\nCOMmonsEnse Transformer (COMET) is introduced for automatic construc-\\ntion of commonsense knowledge bases [121]. Evolved Transformer applies neu-\\nral architecture search for a better Transformer model [122]. Transformer Au-\\ntoencoder is a sequential autoencoder for conditional music generation [123].\\nCrossTransformer takes a small number of labeled images and an unlabeled\\nquery, and computes distances between spatially-corresponding features to in-\\nfer class membership [124]. DEtection TRansformer (DETR) is a new design\\nfor object detection systems [125], and Deformable DETR is an improved ver-\\nsion that achieves better performance in less time [126]. FLOw-bAsed Trans-\\nformER (FLOATER) emphasizes the importance of position encoding in the\\nTransformer, and models the position information via a continuous dynamical\\nmodel [127]. Disentangled Context (DisCo) Transformer simultaneously gener-\\nates all tokens given diﬀerent contexts by predicting every word in a sentence\\nconditioned on an arbitrary subset of the rest of the words [128]. Genera-\\ntive Adversarial Transformer (GANsformer) is presented for visual generative\\nmodeling [129].\\nRecent work has demonstrated signiﬁcant performance on NLP tasks. In\\nOpenAI GPT, there is a left-to-right architecture, where every token can only\\nattend to previous tokens in the self-attention layers of the Transformer [130].\\nGPT-2 [131] and GPT-3 [18] models have improved the progress. In addition\\nto these variants, some prominent Transformer-based models are summarized\\nbelow.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 13}, page_content='14\\nDerya Soydaner\\nUniversal Transformer A generalization of the Transformer model named\\nthe Universal Transformer [132] iteratively computes representations Ht at\\nstep t for all positions in the sequence in parallel. To this end, it uses the\\nscaled dot-product attention in Eq. (8) where d is the number of columns\\nof Q, K and V. In the Universal Transformer, the multi-head self-attention\\nwith k heads is used. The representations Ht is mapped to queries, keys and\\nvalues with aﬃne projections using learned parameter matrices W Q ∈ℜd×d/k,\\nW K ∈ℜd×d/k, W V ∈ℜd×d/k and W O ∈ℜd×d [132]:\\nMultiHead(Ht) = Concat(head1, ..., headk)W O\\n(10)\\nwhere headi = Attention(HtW Q\\ni , HtW K\\ni , HtW V\\ni )\\nImage Transformer Image Transformer [133] demonstrates that self-attention\\nbased models can also be well-suited for images instead of text. This Trans-\\nformer type restricts the self-attention mechanism to attend to local neigh-\\nborhoods. Thus, the size of images that the model can process is increased.\\nIts larger receptive ﬁelds allow the Image Transformer to signiﬁcantly improve\\nthe model performance on image generation as well as image super-resolution.\\nTransformer-XL This study aims to improve the ﬁxed-length context of the\\nTransformer [19] for language modeling. Transformer-XL [134] makes model-\\ning very long-term dependency possible by reusing the hidden states obtained\\nin previous segments. Hence, information can be propagated through the recur-\\nrent connections. In order to reuse the hidden states without causing temporal\\nconfusion, Transformer-XL uses relative positional encodings. Based on this\\narchitecture, a modiﬁed version named the Gated Transformer-XL (GTrXL)\\nis presented in the reinforcement learning setting [135].\\nTensorized Transformer Tensorized Transformer [136] compresses the multi-\\nhead attention in Transformer. To this end, it uses a novel self-attention model\\nmulti-linear attention with Block-Term Tensor Decomposition (BTD) [137]. It\\nbuilds a single-block attention based on the Tucker decomposition [138]. Then,\\nit uses a multi-linear attention constructed by a BTD to compress the multi-\\nhead attention mechanism. In Tensorized Transformer, the factor matrices are\\nshared across multiple blocks.\\nBERT The Bidirectional Encoder Representations from Transformers (BERT)\\naims to pre-train deep bidirectional representations from unlabeled text [139].\\nBERT uses a multilayer bidirectional Transformer as the encoder. Besides,\\ninspired by the Cloze task [140], it has a masked language model pre-training\\nobjective. BERT randomly masks some of the tokens from the input, and pre-\\ndicts the original vocabulary id of the masked word based only on its context.\\nThis model can pre-train a deep bidirectional Transformer. In all layers, the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 14}, page_content='Attention Mechanism in Neural Networks:\\n15\\npre-training is carried out by jointly conditioning on both left and right con-\\ntext. BERT diﬀers from the left-to-right language model pre-training from this\\naspect.\\nRecently, BERT model has been examined in detail. For instance, the be-\\nhaviour of attention heads are analysed [141]. Various methods have been\\ninvestigated for compressing [142,143], pruning [144], and quantization [145].\\nAlso, BERT model has been considered for diﬀerent tasks such as coreference\\nresolution [146]. A novel method is proposed in order to accelerate BERT\\ntraining [147].\\nFurthermore, various BERT variants have been presented. ALBERT aims\\nto increase the training speed of BERT, and presents two parameter reduction\\ntechniques [148]. Similarly, PoWER-BERT [149] is developed to improve the\\ninference time of BERT. This scheme is also used to accelerate ALBERT. Also,\\nTinyBERT is proposed to accelerate inference and reduce model size while\\nmaintaining accuracy [150]. In order to obtain better representations, Span-\\nBERT is proposed as a pre-training method [151]. As a robustly optimized\\nBERT approach, RoBERTa shows that BERT was signiﬁcantly undertrained\\n[152]. Also, DeBERTa improves RoBERTa using the disentangled attention\\nmechanism [153]. On the other side, DistilBERT shows that it is possible to\\nreach similar performances using much smaller language models pre-trained\\nwith knowledge distillation [154]. StructBERT proposes two novel lineariza-\\ntion strategies [155]. Q-BERT is introduced for quantizing BERT models [156],\\nBioBERT is for biomedical text mining [157], and RareBERT is for rare dis-\\nease diagnosis [158].\\nSince 2017 when the Transformer was presented, research directions have\\ngenerally focused on novel self-attention mechanisms, adapting the Trans-\\nformer for various tasks, or making them more understandable. In one of the\\nmost recent studies, NLP becomes possible in the mobile setting with Lite\\nTransformer. It applies long-short range attention where some heads specialize\\nin the local context modeling while the others specialize in the long-distance\\nrelationship modeling [159]. A deep and light-weight Transformer DeLighT\\n[160] and a hypernetwork-based model namely HyperGrid Transformers [161]\\nperform with fewer parameters. Graph Transformer Network is introduced\\nfor learning node representations on heterogeneous graphs [162] and diﬀerent\\napplications are performed for molecular data [163] or textual graph represen-\\ntation [164]. Also, Transformer-XH applies eXtra Hop attention for structured\\ntext data [165]. AttentionXML is a tree-based model for extreme multi-label\\ntext classiﬁcation [166]. Besides, attention mechanism is handled in a Bayesian\\nframework [167]. For a better understanding of Transformers, an identiﬁabil-\\nity analysis of self-attention weights is conducted in addition to presenting\\neﬀective attention to improve explanatory interpretations [168]. Lastly, Vision\\nTransformer (ViT) processes an image using a standard Transformer encoder\\nas used in NLP by interpreting it as a sequence of patches, and performs well\\non image classiﬁcation tasks [169].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 15}, page_content='16\\nDerya Soydaner\\n5.3 What about complexity?\\nAll these aforementioned studies undoubtedly demonstrate signiﬁcant success.\\nBut success not make one great. The Transformer also brings a very high\\ncomputational complexity and memory cost. The necessity of storing atten-\\ntion matrix to compute the gradients with respect to queries, keys and val-\\nues causes a non-negligible quadratic computation and memory requirements.\\nTraining the Transformer is a slow process for very long sequences because\\nof its quadratic complexity. There is also time complexity which is quadratic\\nwith respect to the sequence length. In order to improve the Transformer in\\nthis respect, recent studies have been conducted to improve this issue. One\\nof them is Linear Transformer which expresses the self-attention as a linear\\ndot-product of kernel feature maps [170]. Linear Transformer reduces both\\nmemory and time complexity by changing the self-attention from the softmax\\nfunction in Eq. (8) to a feature map based dot-product attention. Its per-\\nformance is competitive with the vanilla Transformer architecture on image\\ngeneration and automatic speech recognition tasks while being faster during\\ninference. On the other side, FMMformers which use the idea of the fast multi-\\npole method (FMM) [171] outperform the linear Transformer by decomposing\\nthe attention matrix into near-ﬁeld and far-ﬁeld attention with linear time and\\nmemory complexity [172].\\nAnother suggestion made in response to the Transformer’s quadratic na-\\nture is The Reformer that replaces dot-product attention by one that uses\\nlocality-sensitive hashing [173]. It reduces the complexity but one limitation\\nof the Reformer is its requirement for the queries and keys to be identical. Set\\nTransformer aims to reduce computation time of self-attention from quadratic\\nto linear by using an attention mechanism based on sparse Gaussian process\\nliterature [174]. Routing Transformer aims to reduce the overall complexity\\nof attention by learning dynamic sparse attention patterns by using routing\\nattention with clustering [175]. It applies k-means clustering to model sparse\\nattention matrices. At ﬁrst, queries and keys are assigned to clusters. The at-\\ntention scheme is determined by considering only queries and keys from the\\nsame cluster. Thus, queries are routed to keys belonging to the same cluster\\n[175].\\nSparse Transformer introduces sparse factorizations of the attention ma-\\ntrix by using factorized self-attention, and avoids the quadratic growth of com-\\nputational burden [176]. It also shows the possibility of modeling sequences\\nof length one million or more by using self-attention in theory. In the Trans-\\nformer, all the attention heads with the softmax attention assign a non-zero\\nweight to all context words. Adaptively Sparse Transformer replaces softmax\\nwith α-entmax which is a diﬀerentiable generalization of softmax allowing\\nlow-scoring words to receive precisely zero weight [177]. By means of context-\\ndependent sparsity patterns, the attention heads become ﬂexible in the Adap-\\ntively Sparse Transformer. Random feature attention approximates softmax\\nattention with random feature methods [178]. Skyformer replaces softmax\\nwith a Gaussian kernel and adapts Nystr¨om method [179]. A sparse atten-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 16}, page_content='Attention Mechanism in Neural Networks:\\n17\\ntion mechanism named BIGBIRD aims to reduce the quadratic dependency\\nof Transformer-based models to linear [180]. Diﬀerent from the similar stud-\\nies, BIGBIRD performs well for genomics data alongside NLP tasks such as\\nquestion answering.\\nMusic Transformer [181] shows that self-attention can also be useful for\\nmodeling music. This study emphasizes the infeasibility of the relative po-\\nsition representations introduced by [103] for long sequences because of the\\nquadratic intermediate relative information in the sequence length. Therefore,\\nthis study presents an extended version of relative attention named relative\\nlocal attention that improves the relative attention for longer musical com-\\npositions by reducing its intermediate memory requirement to linear in the\\nsequence length. A softmax-free Transformer (SOFT) is presented to improve\\nthe computational eﬃciency of ViT. It uses Gaussian kernel function instead\\nof the dot-product similarity [182].\\nAdditionally, various approaches have been presented in Hierarchical Vi-\\nsual Transformer [183], Long-Short Transformer (Transformer-LS) [184], Per-\\nceiver [185], and Performer [186]. Image Transformer based on the cross-\\ncovariance matrix between keys and queries is applied [187], and a new vi-\\nsion Transformer is proposed [188]. Furthermore, a Bernoulli sampling atten-\\ntion mechanism decreases the quadratic complexity to linear [189]. A novel\\nlinearized attention mechanism performs well on object detection, instance\\nsegmentation, and stereo depth estimation [190]. A study shows that kernel-\\nized attention with relative positional encoding can be calculated using Fast\\nFourier Transform and it leads to get rid of the quadratic complexity for long\\nsequences [191]. A linear uniﬁed nested attention mechanism namely Luna\\nuses two nested attention functions to approximate the softmax attention in\\nTransformer to achieve linear time and space complexity [192].\\n6 Concluding Remarks: A New Hope\\nInspired by the human visual system, the attention mechanisms in neural net-\\nworks have been developing for a long time. In this study, we examine this\\nduration beginning with its roots up to the present time. Some mechanisms\\nhave been modiﬁed, or novel mechanisms have emerged in this period. Today,\\nthis journey has reached a very important stage. The idea of incorporating\\nattention mechanisms into deep neural networks has led to state-of-the-art re-\\nsults for a large variety of tasks. Self-attention mechanisms and GPT-n family\\nmodels have become a new hope for more advanced models. These promising\\nprogress bring the questions whether the attention could help further devel-\\nopment, replace the popular neural network layers, or could be a better idea\\nthan the existing attention mechanisms? It is still an active research area and\\nmuch to learn we still have, but it is obvious that more powerful systems are\\nawaiting when neural networks and attention mechanisms join forces.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 17}, page_content='18\\nDerya Soydaner\\nConﬂict of interest\\nThe author declares that she has no conﬂict of interest.\\nReferences\\n1. I. Goodfellow, Y. Bengio, A. Courville, The MIT Press (2016)\\n2. D. Noton, L. Stark, Scientiﬁc American 224(6), 34 (1971)\\n3. D. Noton, L. Stark, Vision Research 11, 929 (1971)\\n4. E. Alpaydın, Advances in Neural Information Processing Systems 8 pp. 771–777 (1995)\\n5. S. Ahmad, Advances in Neural Information Processing Systems 4 pp. 420–427 (1991)\\n6. M. Posner, S. Petersen, Annual Review of Neuroscience 13(1), 25 (1990)\\n7. C. Bundesen, Psychological Review 97(4), 523 (1990)\\n8. R. Desimone, J. Duncan, Annual Review of Neuroscience 18(1), 193 (1995)\\n9. M. Corbetta, G. Shulman, Nature Reviews Neuroscience 3(3), 201 (2002)\\n10. S. Petersen, M. Posner, Annual Review of Neuroscience 35, 73 (2012)\\n11. R. Rimey, C. Brown, Technical Report, University of Rochester (1990)\\n12. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 98(3), 507 (1994)\\n13. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 105(2), 261 (1995)\\n14. J. Hoﬀman, B. Subramaniam, Perception and Psychophysics 57(6), 787 (1995)\\n15. S. Chaudhari, et al., ACM Transactions on Intelligent Systems and Technology (TIST)\\npp. 1–32 (2021)\\n16. A. Galassi, et al., IEEE Transactions on Neural Networks and Learning Systems (2020)\\n17. J. Lee, et al., ACM Transactions on Knowledge Discovery from Data (TKDD) 13(6),\\n1 (2019)\\n18. T. Brown, et al., Advances in Neural Information Processing Systems 33 pp. 1877–1901\\n(2020)\\n19. A. Vaswani, et al., Advances in Neural Information Processing Systems 30 pp. 5998–\\n6008 (2017)\\n20. K. Fukushima, Biological Cybernetics 36, 193 (1980)\\n21. K. Fukushima, Applied Optics 26(23), 4985 (1987)\\n22. K. Fukushima, T. Imagawa, Neural Networks 6(1), 33 (1993)\\n23. E. Postma, H.V. den Herik, P. Hudson, Neural Networks 10(6), 993 (1997)\\n24. J. Schmidhuber, R. Huber, International Journal of Neural Systems pp. 125–134 (1991)\\n25. R. Milanese, et al., IEEE Computer Society Conference on Computer Vision and Pat-\\ntern Recoginition, Seattle, WA, USA pp. 781–785 (1994)\\n26. J. Tsotsos, et al., Artiﬁcial Intelligence 78(1-2), 507 (1995)\\n27. S. Culhane, J. Tsotsos, Proceedings of the 11th IAPR International Conference on\\nPattern Recognition, The Hague, Netherlands pp. 36–40 (1992)\\n28. D. Reisfeld, H. Wolfson, Y. Yeshurun, International Journal of Computer Vision 14(2),\\n119 (1995)\\n29. I. Rybak, et al., Vision Research 38(15-16), 2387 (1998)\\n30. J. Keller, et al., Pattern Analysis and Applications 2(3) (1999)\\n31. F. Miau, L. Itti, Proceedings of the 23rd Annual International Conference of the IEEE\\nEngineering in Medicine and Biology Society, Istanbul, Turkey pp. 789–792 (2001)\\n32. W. Zhang, et al., Advances in Neural Information Processing Systems 19 pp. 1609–1616\\n(2006)\\n33. A. Salah, E. Alpaydın, L. Akarun, IEEE Transactions on Pattern Analysis and Machine\\nIntelligence 24(3), 420 (2002)\\n34. D. Walther, et al., International Workshop on Biologically Motivated Computer Vision,\\nSpringer, Berlin, Heidelberg pp. 472–479 (2002)\\n35. K. Schill, et al., Journal of Electronic Imaging 10(1), 152 (2001)\\n36. L. Paletta, G. Fritz, C. Seifert, International Conference on Machine Learning (2005)\\n37. O.L. Meur, et al., IEEE Transactions on Pattern Analysis and Machine Intelligence\\n28(5), 802– (2006)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 18}, page_content='Attention Mechanism in Neural Networks:\\n19\\n38. S. Gould, et al., International Joint Conference on Artiﬁcial Intelligence (IJCAI) pp.\\n2115–2121 (2007)\\n39. H. Larochelle, G. Hinton, Advances in Neural Information Processing Systems 23 pp.\\n1243–1251 (2010)\\n40. L. Bazzani, et al., International Conference on Machine Learning (2011)\\n41. V. Mnih, et al., Advances in Neural Information Processing Systems 27 pp. 2204–2212\\n(2014)\\n42. M. Stollenga, et al., Advances in Neural Information Processing Systems 27 pp. 3545–\\n3553 (2014)\\n43. Y. Tang, N. Srivastava, R. Salakhutdinov, Advances in Neural Information Processing\\nSystems 27 (2014)\\n44. D. Bahdanau, K. Cho, Y. Bengio, International Conference on Learning Representa-\\ntions (2015)\\n45. I. Sutskever, O. Vinyals, Q. Le, Advances in Neural Information Processing Systems\\n27 pp. 3104–3112 (2014)\\n46. K. Cho, et al., Proceedings of the 2014 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) pp. 1724–1734 (2014)\\n47. M. Schuster, K. Paliwal, IEEE Transactions on Signal Processing 45(11), 2673 (1997)\\n48. K. Xu, et al., International Conference on Machine Learning pp. 2048–2057 (2015)\\n49. O. Vinyals, et al., In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition pp. 3156–3164 (2015)\\n50. R. Williams, Machine Learning 8(3-4), 229 (1992)\\n51. M.T. Luong, H.P..C. Manning, Proceedings of the 2015 Conference on Empirical Meth-\\nods in Natural Language Processing, Lisbon, Portugal pp. 1412–1421 (2015)\\n52. J. Lu, et al., Advances in Neural Information Processing Systems 29 (2016)\\n53. J. Weston, S. Chopra, A. Bordes, International Conference on Learning Representa-\\ntions (2014)\\n54. A. Graves, G. Wayne, I. Danihelka, arXiv preprint arXiv:1410.5401 (2014)\\n55. S. Sukhbaatar, et al., Advances in Neural Information Processing Systems 28 pp. 2440–\\n2448 (2015)\\n56. J. Cheng, L. Dong, M. Lapata, Proceedings of the 2016 Conference on Empirical\\nMethods in Natural Language Processing pp. 551–561 (2016)\\n57. A. Parikh, et al., Proceedings of the 2016 Conference on Empirical Methods in Natural\\nLanguage Processing, Austin, Texas pp. 2249–2255 (2016)\\n58. Q. You, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), Las Vegas, NV pp. 4651–4659 (2016)\\n59. A. Rush, S. Chopra, J. Weston, Proceedings of the 2015 Conference on Empirical\\nMethods in Natural Language Processing, Lisbon, Portugal pp. 379–389 (2015)\\n60. D. Yu, et al., Interspeech pp. 17–21 (2016)\\n61. J. Chorowski, et al., Advances in Neural Information Processing Systems 28 pp. 577–\\n585 (2015)\\n62. M. Zanﬁr, E. Marinoiu, C. Sminchisescu, In Asian Conference on Computer Vision,\\nSpringer, Cham pp. 104—-119 (2016)\\n63. Y. Cheng, et al., Proceedings of the 25th International Joint Conference on Artiﬁcial\\nIntelligence (2016)\\n64. T. Rockt International Conference on Learning Representations (2016)\\n65. Y. Zhu, et al., Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition pp. 4995–5004 (2016)\\n66. K. Chen, et al., arXiv preprint arXiv:1511.05960 (2015)\\n67. H. Xu, K. Saenko, In European Conference on Computer Vision pp. 451–466 (2016)\\n68. W. Yin, et al., Transactions of the Association for Computational Linguistics 4, 259\\n(2016)\\n69. S. Sharma, R. Kiros, R. Salakhutdinov, International Conference on Learning Repre-\\nsentations (2016)\\n70. Z. Yang, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition pp. 21–29 (2016)\\n71. I. Sorokin, et al., arXiv preprint arXiv:1512.01693 (2015)\\n72. J. Ba, et al., Advances in Neural Information Processing Systems 28 pp. 2593–2601\\n(2015)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 19}, page_content='20\\nDerya Soydaner\\n73. K. Gregor, et al., International Conference on Machine Learning pp. 1462–1471 (2015)\\n74. E. Mansimov, et al., International Conference on Learning Representations (2016)\\n75. S. Reed, et al., Advances in Neural Information Processing Systems 29 pp. 217–225\\n(2016)\\n76. E. Voita, et al., In Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, Florence, Italy pp. 5797–5808 (2019)\\n77. G. Kerg, et al., Advances in Neural Information Processing Systems 33 (2020)\\n78. J.B. Cordonnier, A. Loukas, M. Jaggi, International Conference on Learning Repre-\\nsentations (2020)\\n79. Z. Lin, et al., International Conference on Learning Representations (2017)\\n80. R. Paulus, C. Xiong, R. Socher, International Conference on Learning Representations\\n(2018)\\n81. N. Kitaev, D. Klein, In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Long papers) pp. 2676–2686 (2018)\\n82. D. Povey, et al., IEEE International Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP), IEEE pp. 5874–5878 (2018)\\n83. A. Vyas, et al., Advances in Neural Information Processing Systems 33 (2020)\\n84. W. Chan, et al., IEEE International Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP), Shanghai pp. 4960—-4964 (2016)\\n85. M. Sperber, et al., In proceedings of Annual Conference of the International Speech\\nCommunication Association (InterSpeech) pp. 3723–3727 (2018)\\n86. L. Kaiser, et al., arXiv preprint arXiv:1706.05137 (2017)\\n87. C. Xu, et al., Proceedings of the 56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Short papers), Melbourne, Australia pp. 778–783 (2018)\\n88. S. Maruf, A. Martins, G. Haﬀari, Proceedings of NAACL-HLT, Minneapolis, Minnesota\\npp. 3092–3102 (2019)\\n89. P. Ramachandran, et al., Advances in Neural Information Processing Systems 32 pp.\\n68–80 (2019)\\n90. Y. Li, et al., International Conference on Machine Learning (2019)\\n91. I. Goodfellow, et al., Advances in Neural Information Processing Systems 27 pp. 2672–\\n2680 (2014)\\n92. H. Zhang, et al., International Conference on Machine Learning pp. 7354–7363 (2019)\\n93. T. Xu, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR) pp. 1316–1324 (2018)\\n94. A. Yu, et al., International Conference on Learning Representations (2018)\\n95. J. Zhang, et al., Conference on Uncertainty in Artiﬁcial Intelligence (2018)\\n96. D. Romero, et al., International Conference on Machine Learning (2020)\\n97. R. Al-Rfou, et al., AAAI Conference on Artiﬁcial Intelligence 33, 3159 (2019)\\n98. J. Du, et al., Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing pp. 2216–2225 (2018)\\n99. X. Li, et al., Advances in Neural Information Processing Systems 33 (2020)\\n100. B. Yang, et al., AAAI Conference on Artiﬁcial Intelligence 33, 387 (2019)\\n101. B. Yang, et al., Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing, Brussels, Belgium pp. 4449–4458 (2018)\\n102. Proceedings of the IEEE International Conference on Computer Vision pp. 3286–3295\\n103. P. Shaw, J. Uszkoreit, A. Vaswani, Proceedings of NAACL-HLT, New Orleans,\\nLouisiana pp. 464–468 (2018)\\n104. T. Shen, et al., AAAI Conference on Artiﬁcial Intelligence pp. 5446–5455 (2018)\\n105. T. Shen, et al., In Proceedings of the 27th International Joint Conference on Artiﬁcial\\nIntelligence, (IJCAI-18) pp. 4345–4352 (2018)\\n106. H. Le, T. Tran, S. Venkatesh, International Conference on Machine Learning (2020)\\n107. T. Shen, et al., International Conference on Learning Representations (2018)\\n108. S. Bhojanapalli, et al., International Conference on Machine Learning (2020)\\n109. Y. Tay, et al., International Conference on Machine Learning (2020)\\n110. S. Sukhbaatar, et al., Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, Florence, Italy pp. 331–335 (2019)\\n111. Y. Jernite, et al., International Conference on Learning Representations (2017)\\n112. R. Shu, H. Nakayama, In Proceedings of the First Workshop on Neural Machine Trans-\\nlation, Vancouver, Canada pp. 1–10 (2017)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 20}, page_content='Attention Mechanism in Neural Networks:\\n21\\n113. J. Hao, et al., Proceedings of NAACL-HLT, Minneapolis, Minnesota pp. 1198–1207\\n(2019)\\n114. X. Huang, et al., International Conference on Machine Learning (2020)\\n115. V. Shiv, C. Quirk, Advances in Neural Information Processing Systems 32 pp. 12,081–\\n12,091 (2019)\\n116. Z. Li, et al., International Conference on Machine Learning (2020)\\n117. Y. Hoshen, Advances in Neural Information Processing Systems 30, Long Beach, CA,\\nUSA (2017)\\n118. S. Hu, et al., International Conference on Learning Representations (2021)\\n119. E. Parisotto, R. Salakhutdinov, International Conference on Learning Representations\\n(2021)\\n120. S. Wu, et al., Advances in Neural Information Processing Systems 33 (2020)\\n121. A. Bosselut, et al., Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics (2019)\\n122. D. So, C. Liang, Q. Le, International Conference on Machine Learning (2019)\\n123. K. Choi, et al., International Conference on Machine Learning (2020)\\n124. C. Doersch, A. Gupta, A. Zisserman, Advances in Neural Information Processing Sys-\\ntems 33 pp. 21,981–21,993 (2020)\\n125. N. Carion, et al., European Conference on Computer Vision pp. 213—-229 (2020)\\n126. X. Zhu, et al., International Conference on Learning Representations (2021)\\n127. X. Liu, et al., International Conference on Machine Learning pp. 6327–6335 (2020)\\n128. J. Kasai, et al., International Conference on Machine Learning (2020)\\n129. D. Hudson, L. Zitnick, International Conference on Machine Learning pp. 4487–4499\\n(2021)\\n130. A. Radford, et al., Technical Report, OpenAI (2018)\\n131. A. Radford, et al., OpenAI blog p. 9 (2019)\\n132. M. Dehghani, et al., International Conference on Learning Representations (2019)\\n133. N. Parmar, International Conference on Machine Learning (2018)\\n134. Z. Dai, et al., Proceedings of the 57th Annual Meeting of the Association for Compu-\\ntational Linguistics pp. 2978–2988 (2019)\\n135. E. Parisotto, International Conference on Machine Learning (2020)\\n136. X. Ma, et al., Advances in Neural Information Processing Systems 32 pp. 2232–2242\\n(2019)\\n137. L. Lathauwer, SIAM Journal on Matrix Analysis and Applications 30(3), 1033 (2008)\\n138. L. Tucker, Psychometrika 31(3), 279 (1966)\\n139. J. Devlin, et al., Proceedings of NAACL-HLT 2019 pp. 4171–4186 (2019)\\n140. W. Taylor, Journalism Bulletin 30(4), 415 (1953)\\n141. K. Clark, et al., arXiv preprint arXiv:1906.04341 (2019)\\n142. S. Sun, et al., Proceedings of the 2019 Conference on Empirical Methods in Natural\\nLanguage Processing and the 9th International Joint Conference on Natural Language\\nProcessing, Hong Kong, China pp. 4323–4332 (2019)\\n143. W. Wang, et al., Advances in Neural Information Processing Systems 33 (2020)\\n144. J. McCarley, R. Chakravarti, A. Sil, arXiv preprint arXiv:1910.06360 (2020)\\n145. O. Zafrir, et al., The 5th Workshop on Energy Eﬃcient Machine Learning and Cogni-\\ntive Computing - NeurIPS (2019)\\n146. M. Joshi, et al., In Proceedings of the 2019 Conference on Empirical Methods in\\nNatural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing pp. 5803–5808 (2019)\\n147. L. Gong, et al., International Conference on Machine Learning pp. 2337–2346 (2019)\\n148. Z. Lan, et al., International Conference on Learning Representations (2020)\\n149. S. Goyal, et al., International Conference on Machine Learning (2020)\\n150. X. Jiao, et al., arXiv preprint arXiv:1909.10351 (2019)\\n151. M. Joshi, et al., Transactions of the Association for Computational Linguistics 8, 64\\n(2020)\\n152. Y. Liu, et al., arXiv preprint arXiv:1907.11692 (2019)\\n153. P. He, et al., International Conference on Learning Representations (2021)\\n154. V. Sanh, et al., the 5th Workshop on Energy Eﬃcient Machine Learning and Cognitive\\nComputing - NeurIPS (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 21}, page_content='22\\nDerya Soydaner\\n155. W. Wang, et al., International Conference on Learning Representations (2020)\\n156. S. Shen, et al., AAAI Conference on Artiﬁcial Intelligence 34, 8815 (2020)\\n157. J. Lee, et al., Bioinformatics 36(4), 1234 (2020)\\n158. P. Prakash, et al., AAAI Conference on Artiﬁcial Intelligence 35, 453 (2021)\\n159. Z. Wu, et al., International Conference on Learning Representations (2020)\\n160. S. Mehta, et al., International Conference on Learning Representations (2021)\\n161. Y. Tay, et al., International Conference on Learning Representations (2021)\\n162. S. Yun, et al., International Conference on Learning Representations (2018)\\n163. Y. Rong, et al., Advances in Neural Information Processing Systems 33 (2020)\\n164. J. Yang, et al., Advances in Neural Information Processing Systems 34 (2021)\\n165. C. Zhao, et al., International Conference on Learning Representations (2020)\\n166. R. You, et al., Advances in Neural Information Processing Systems 32 (2019)\\n167. X. Fan, et al., Advances in Neural Information Processing Systems 33 (2020)\\n168. G. Brunner, et al., International Conference on Learning Representations (2020)\\n169. A. Dosovitskiy, et al., International Conference on Learning Representations (2021)\\n170. A. Katharopoulos, et al., International Conference on Machine Learning (2020)\\n171. L. Greengard, V. Rokhlin, Journal of Computational Physics 73(2), 325– (1987)\\n172. T. Nguyen, et al., Advances in Neural Information Processing Systems 34 (2021)\\n173. N. Kitaev, L. Kaiser, A. Levskaya, International Conference on Learning Representa-\\ntions (2020)\\n174. J. Lee, et al., International Conference on Machine Learning pp. 3744–3753 (2019)\\n175. A. Roy, et al., Transactions of the Association for Computational Linguistics pp. 53–68\\n(2020)\\n176. R. Child, et al., arXiv preprint arXiv:1904.10509 (2019)\\n177. G. Correia, V. Niculae, A. Martins, Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th International Joint Conference\\non Natural Language Processing pp. 2174–2184 (2019)\\n178. H. Peng, et al., International Conference on Learning Representations (2021)\\n179. Y. Chen, et al., Advances in Neural Information Processing Systems 34 (2021)\\n180. M. Zaheer, et al., Advances in Neural Information Processing Systems 33 (2020)\\n181. C.Z. Huang, et al., International Conference on Learning Representations (2019)\\n182. J. Lu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n183. Z. Pan, et al., Proceedings of the IEEE/CVF International Conference on Computer\\nVision pp. 377–386 (2021)\\n184. C. Zhu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n185. A. Jaegle, et al., International Conference on Machine Learning pp. 4651–4664 (2021)\\n186. K. Choromanski, et al., International Conference on Learning Representations (2021)\\n187. A. El-Nouby, et al., Advances in Neural Information Processing Systems 34 (2021)\\n188. Q. Yu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n189. Z. Zeng, et al., International Conference on Machine Learning pp. 12,321–12,332 (2021)\\n190. Z. Shen, et al., Proceedings of the IEEE/CVF Winter Conference on Applications of\\nComputer Vision pp. 3531–3539 (2021)\\n191. S. Luo, et al., Advances in Neural Information Processing Systems 34 (2021)\\n192. X. Ma, et al., Advances in Neural Information Processing Systems 34 (2021)')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
    "\n",
    "# load all the files\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"data/pdf_files\",\n",
    "    glob='**/*.pdf',\n",
    "    loader_cls=PyMuPDFLoader,\n",
    "    show_progress=True\n",
    "    )\n",
    "\n",
    "pdf_docs=dir_loader.load()\n",
    "pdf_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0588f5",
   "metadata": {},
   "source": [
    "# Embedding and VectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "394ab554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List,Dict,Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e0f5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str='all-MiniLM-L6-V2'):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model Loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model{self.model_name}:{e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts:List[str])->np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts,show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4b5eb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-V2\n",
      "Model Loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x380807bd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize embedding manager\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c06b791",
   "metadata": {},
   "source": [
    "# VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "735f4561",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, collection_name: str = \"pdf_docs\", persist_directory:str='data/vector_store'):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client=None\n",
    "        self.collection = None\n",
    "        self._initialize_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15147a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
