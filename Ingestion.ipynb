{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08427696",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46469e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/AI_Projects_Production/LLM_Projects/RAG-Pipelines/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae7b1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files to process\n",
      "\n",
      " Processing : attention.pdf\n",
      " ✓ Loaded 15 pages\n",
      "\n",
      " Processing : NN_attention.pdf\n",
      " ✓ Loaded 22 pages\n",
      "\n",
      "Total documents loaded : 37\n"
     ]
    }
   ],
   "source": [
    "# Read all pdfs inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "\n",
    "    pdf_files = list(pdf_dir.glob(\"*/*.pdf\"))\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\n Processing : {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "            print(f\" ✓ Loaded {len(documents)} pages\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" ✗ Error : {e}\")\n",
    "    print(f\"\\nTotal documents loaded : {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in a data directory\n",
    "all_pdf_documents = process_all_pdfs(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a76185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 37 documents into 137 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n",
      "Metadata : {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "# Text Splitting into Chunks\n",
    "\n",
    "def split_documents(documents, chunk_size = 1000, chunk_overlap=200):\n",
    "    \"\"\"Splitting docs into smaller chunks for better RAG Performance\"\"\"\n",
    "    text_spitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_spitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    # Show example chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata : {split_docs[0].metadata}\")\n",
    "    return split_docs\n",
    "\n",
    "chunks = split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3799af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b93b8bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'shireesha', 'date_created': '2025-12-23'}, page_content='Let this be the main content I am using to create RAG')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"Let this be the main content I am using to create RAG\",\n",
    "    metadata={\n",
    "        \"source\":\"example.txt\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"shireesha\",\n",
    "        \"date_created\": \"2025-12-23\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc4b8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a simple txt file\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fe023d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"data/text_files/python_intro.txt\":\"\"\"Python is a high-level, interpreted programming language known for its clear syntax and code readability, which was created in 1991. It is popular for beginners due to its gentle learning curve and is widely used in web development, data science, artificial intelligence, and automation. \n",
    "\n",
    "Key features include:\n",
    "Dynamic Typing: Variable types are automatically determined at runtime, which simplifies coding.\n",
    "Multiple Paradigms: It supports object-oriented, functional, and procedural programming styles.\n",
    "Extensive Libraries: A vast ecosystem of libraries and the Python Package Index (PyPI) offer ready-to-use code for various tasks.\"\"\",\n",
    "    \"data/text_files/machine_learning.txt\":\"\"\"Machine Learning (ML) is a type of Artificial Intelligence (AI) that lets computers learn from data to find patterns, make decisions, and predict outcomes, without being explicitly programmed for every task, using algorithms trained on vast datasets for tasks like image recognition, recommendations, and translation. Key types include Supervised Learning (labeled data, e.g., spam filters), Unsupervised Learning (unlabeled data, e.g., customer grouping), and Reinforcement Learning (learning via rewards/penalties for decision-making). \"\"\"\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "print(\"Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7effa2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/text_files/python_intro.txt'}, page_content='Python is a high-level, interpreted programming language known for its clear syntax and code readability, which was created in 1991. It is popular for beginners due to its gentle learning curve and is widely used in web development, data science, artificial intelligence, and automation. \\n\\nKey features include:\\nDynamic Typing: Variable types are automatically determined at runtime, which simplifies coding.\\nMultiple Paradigms: It supports object-oriented, functional, and procedural programming styles.\\nExtensive Libraries: A vast ecosystem of libraries and the Python Package Index (PyPI) offer ready-to-use code for various tasks.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Read the text using text loaders of langchain\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/text_files/python_intro.txt\", encoding=\"utf-8\")\n",
    "\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b9787a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 2038.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/text_files/python_intro.txt'}, page_content='Python is a high-level, interpreted programming language known for its clear syntax and code readability, which was created in 1991. It is popular for beginners due to its gentle learning curve and is widely used in web development, data science, artificial intelligence, and automation. \\n\\nKey features include:\\nDynamic Typing: Variable types are automatically determined at runtime, which simplifies coding.\\nMultiple Paradigms: It supports object-oriented, functional, and procedural programming styles.\\nExtensive Libraries: A vast ecosystem of libraries and the Python Package Index (PyPI) offer ready-to-use code for various tasks.'),\n",
       " Document(metadata={'source': 'data/text_files/machine_learning.txt'}, page_content='Machine Learning (ML) is a type of Artificial Intelligence (AI) that lets computers learn from data to find patterns, make decisions, and predict outcomes, without being explicitly programmed for every task, using algorithms trained on vast datasets for tasks like image recognition, recommendations, and translation. Key types include Supervised Learning (labeled data, e.g., spam filters), Unsupervised Learning (unlabeled data, e.g., customer grouping), and Reinforcement Learning (learning via rewards/penalties for decision-making). ')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"data/text_files\",\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "text_docs=dir_loader.load()\n",
    "text_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3017d9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 1}, page_content='1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 3}, page_content='Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 4}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 6}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 9}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 10}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 11}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 12}, page_content='Attention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 13}, page_content='The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf_files/attention.pdf', 'file_path': 'data/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 14}, page_content='The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 0}, page_content='Attention Mechanism in Neural Networks:\\nWhere it Comes and Where it Goes\\nDerya Soydaner\\nReceived: 22 July 2021 / Accepted: 27 April 2022\\nAbstract A long time ago in the machine learning literature, the idea of\\nincorporating a mechanism inspired by the human visual system into neural\\nnetworks was introduced. This idea is named the attention mechanism, and it\\nhas gone through a long development period. Today, many works have been\\ndevoted to this idea in a variety of tasks. Remarkable performance has re-\\ncently been demonstrated. The goal of this paper is to provide an overview\\nfrom the early work on searching for ways to implement attention idea with\\nneural networks until the recent trends. This review emphasizes the impor-\\ntant milestones during this progress regarding diﬀerent tasks. By this way,\\nthis study aims to provide a road map for researchers to explore the current\\ndevelopment and get inspired for novel approaches beyond the attention.\\nKeywords Attention mechanism · Neural networks · Deep learning · Survey\\n1 Introduction\\nHuman eye sees the world in an interesting way. We suppose as if we see the\\nentire scene at once, but this is an illusion created by the subconscious part\\nof our brain [1]. According to the Scanpath theory [2,3], when the human eye\\nlooks at an image, it can see only a small patch in high resolution. This small\\npatch is called the fovea. It can see the rest of the image in low resolution which\\nis called the periphery. To recognize the entire scene, the eye performs feature\\nextraction based on the fovea. The eye is moved to diﬀerent parts of the image\\nuntil the information obtained from the fovea is suﬃcient for recognition [4].\\nThese eye movements are called saccades. The eye makes successive ﬁxations\\nDerya Soydaner\\nDepartment of Brain and Cognition, University of Leuven (KU Leuven), Leuven, Belgium\\nTel.: +32-16710471\\nE-mail: derya.soydaner@kuleuven.be\\narXiv:2204.13154v1  [cs.LG]  27 Apr 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 1}, page_content='2\\nDerya Soydaner\\nuntil the recognition task is complete. This sequential process happens so\\nquickly that we feel as if it happens all at once.\\nBiologically, this is called visual attention system. Visual attention is de-\\nﬁned as the ability to dynamically restrict processing to a subset of the visual\\nﬁeld [5]. It seeks answers for two main questions: What and where to look?\\nVisual attention has been extensively studied in psychology and neuroscience;\\nfor reviews see [6,7,8,9,10]. Besides, there is a large amount of literature on\\nmodeling eye movements [11,12,13,14]. These studies have been a source of\\ninspiration for many artiﬁcial intelligence tasks. It has been discovered that\\nthe attention idea is useful from image recognition to machine translation.\\nTherefore, diﬀerent types of attention mechanisms inspired from the human\\nvisual system have been developed for years. Since the success of deep neural\\nnetworks has been at the forefront for these artiﬁcial intelligence tasks, these\\nmechanisms have been integrated into neural networks for a long time.\\nThis survey is about the journey of attention mechanisms used with neu-\\nral networks. Researchers have been investigating ways to strengthen neural\\nnetwork architectures with attention mechanisms for many years. The pri-\\nmary aim of these studies is to reduce computational burden and to improve\\nthe model performance as well. Previous work reviewed the attention mecha-\\nnisms from diﬀerent perspectives [15], or examined them in context of natural\\nlanguage processing (NLP) [16,17]. However, in this study, we examine the\\ndevelopment of attention mechanisms over the years, and recent trends. We\\nbegin with the ﬁrst attempts to integrate the visual attention idea to neural\\nnetworks, and continue until the most modern neural networks armed with at-\\ntention mechanisms. One of them is the Transformer, which is used for many\\nstudies including the GPT-3 language model [18], goes beyond convolutions\\nand recurrence by replacing them with only attention layers [19]. Finally, we\\ndiscuss how much more can we move forward, and what’s next?\\n2 From the Late 1980s to Early 2010s: The Attention Awakens\\nThe ﬁrst attempts at adapting attention mechanisms to neural networks go\\nback to the late 1980s. One of the early studies is the improved version of\\nthe Neocognitron [20] with selective attention [21]. This study is then mod-\\niﬁed to recognize and segment connected characters in cursive handwriting\\n[22]. Another study describes VISIT, a novel model that concentrates on its\\nrelationship to a number of visual areas of the brain [5]. Also, a novel archi-\\ntecture named Signal Channelling Attentional Network (SCAN) is presented\\nfor attentional scanning [23].\\nEarly work on improving the attention idea for neural networks includes\\na variety of tasks such as target detection [24]. In another study, a visual at-\\ntention system extracts regions of interest by combining the bottom-up and\\ntop-down information from the image [25]. A recognition model based on se-\\nlective attention which analyses only a small part of the image at each step,\\nand combines results in time is described [4]. Besides, a model based on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 2}, page_content='Attention Mechanism in Neural Networks:\\n3\\nconcept of selective tuning is proposed [26]. As the years go by, several studies\\nthat use the attention idea in diﬀerent ways have been presented for visual\\nperception and recognition [27,28,29,30].\\nBy the 2000s, the studies on making attention mechanisms more useful for\\nneural networks continued. In the early years, a model that integrates an at-\\ntentional orienting where pathway and an object recognition what pathway is\\npresented [31]. A computational model of human eye movements is proposed\\nfor an object class detection task [32]. A serial model is presented for visual pat-\\ntern recognition gathering Markov models and neural networks with selective\\nattention on the handwritten digit recognition and face recognition problems\\n[33]. In that study, a neural network analyses image parts and generates pos-\\nterior probabilities as observations to the Markov model. Also, attention idea\\nis used for object recognition [34], and the analysis of a scene [35]. An inter-\\nesting study proposes to learn sequential attention in real-world visual object\\nrecognition using a Q-learner [36]. Besides, a computational model of visual\\nselective attention is described to automatically detect the most relevant parts\\nof a color picture displayed on a television screen [37]. The attention idea is\\nalso used for identifying and tracking objects in multi-resolution digital video\\nof partially cluttered environments [38].\\nIn 2010, the ﬁrst implemented system inspired by the fovea of human retina\\nwas presented for image classiﬁcation [39]. This system jointly trains a re-\\nstricted Boltzmann machine (RBM) and an attentional component called the\\nﬁxation controller. Similarly, a novel attentional model is implemented for si-\\nmultaneous object tracking and recognition that is driven by gaze data [40].\\nBy taking advantage of reinforcement learning, a novel recurrent neural net-\\nwork (RNN) is described for image classiﬁcation [41]. Deep Attention Selective\\nNetwork (DasNet), a deep neural network with feedback connections that are\\nlearned through reinforcement learning to direct selective attention to certain\\nfeatures extracted from images, is presented [42]. Additionally, a deep learning\\nbased framework using attention has been proposed for generative modeling\\n[43].\\n3 2015: The Rise of Attention\\nIt can be said that 2015 is the golden year of attention mechanisms. Because\\nthe number of attention studies has grown like an avalanche after three main\\nstudies presented in that year. The ﬁrst one proposed a novel approach for\\nneural machine translation (NMT) [44]. As it is known, most of the NMT\\nmodels belong to a family of encoder-decoders [45,46], with an encoder and a\\ndecoder for each language. However, compressing all the necessary information\\nof a source sentence into a ﬁxed-length vector is an important disadvantage of\\nthis encoder-decoder approach. This usually makes it diﬃcult for the neural\\nnetwork to capture all the semantic details of a very long sentence [1].\\nThe idea that [44] introduced is an extension to the conventional NMT\\nmodels. This extension is composed of an encoder and decoder as shown in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 3}, page_content='4\\nDerya Soydaner\\nFig. 1 The extension to the conventional NMT models that is proposed by [44]. It generates\\nthe t-th target word yt given a source sentence (x1, x2, ..., xT ).\\nFig 1. The ﬁrst part, encoder, is a bidirectional RNN (BiRNN) [47] that takes\\nword vectors as input. The forward and backward states of BiRNN are com-\\nputed. Then, an annotation aj for each word xj is obtained by concatenating\\nthese forward and backward hidden states. Thus, the encoder maps the input\\nsentence to a sequence of annotations (a1, ..., aTx). By using a BiRNN rather\\nthan conventional RNN, the annotation of each word can summarize both\\nthe preceding words and the following words. Besides, the annotation aj can\\nfocus on the words around xj because of the inherent nature of RNNs that\\nrepresenting recent inputs better.\\nIn decoder, a weight αij of each annotation aj is obtained by using its\\nassociated energy eij that is computed by a feedforward neural network f as\\nin Eq. (1). This neural network f is deﬁned as an alignment model that can\\nbe jointly trained with the proposed architecture. In order to reduce compu-\\ntational burden, a multilayer perceptron (MLP) with a single hidden layer is\\nproposed as f. This alignment model tells us about the relation between the\\ninputs around position j and the output at position i. By this way, the decoder\\napplies an attention mechanism. As it is seen in Eq. (2), the αij is the output\\nof softmax function:\\neij = f(hi−1, aj)\\n(1)\\nαij =\\nexp(eij)\\nPTx\\nk=1 exp(eik)\\n(2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 4}, page_content='Attention Mechanism in Neural Networks:\\n5\\nHere, the probability αij determines the importance of annotation aj with\\nrespect to the previous hidden state hi−1. Finally, the context vector ci is\\ncomputed as a weighted sum of these annotations as follows [44]:\\nci =\\nTx\\nX\\nj=1\\nαijaj\\n(3)\\nBased on the decoder state, the context and the last generated word, the\\ntarget word yt is predicted. In order to generate a word in a translation, the\\nmodel searches for the most relevant information in the source sentence to\\nconcentrate. When it ﬁnds the appropriate source positions, it makes the pre-\\ndiction. By this way, the input sentence is encoded into a sequence of vectors\\nand a subset of these vectors is selected adaptively by the decoder that is rel-\\nevant to predicting the target [44]. Thus, it is no longer necessary to compress\\nall the information of a source sentence into a ﬁxed-length vector.\\nThe second study is the ﬁrst visual attention model in image captioning\\n[48]. Diﬀerent from the previous study [44], it uses a deep convolutional neural\\nnetwork (CNN) as an encoder. This architecture is an extension of the neural\\nnetwork [49] that encodes an image into a compact representation, followed by\\nan RNN that generates a corresponding sentence. Here, the annotation vectors\\nai ∈RD are extracted from a lower convolutional layer, each of which is a D-\\ndimensional representation corresponding to a part of the image. Thus, the\\ndecoder selectively focuses on certain parts of an image by weighting a subset\\nof all the feature vectors [48]. This extended architecture uses attention for\\nsalient features to dynamically come to the forefront instead of compressing\\nthe entire image into a static representation.\\nThe context vector ct represents the relevant part of the input image at\\ntime t. The weight αi of each annotation vector is computed similar to Eq. (2),\\nwhereas its associated energy is computed similar to Eq. (1) by using an MLP\\nconditioned on the previous hidden state ht−1. The remarkable point of this\\nstudy is a new mechanism φ that computes ct from the annotation vectors ai\\ncorresponding to the features extracted at diﬀerent image locations:\\nct = φ(\\n\\x08\\nai\\n\\t\\n,\\n\\x08\\nαi\\n\\t\\n)\\n(4)\\nThe deﬁnition of the φ function causes two variants of attention mecha-\\nnisms: The hard (stochastic) attention mechanism is trainable by maximizing\\nan approximate variational lower bound, i.e., by REINFORCE [50]. On the\\nother side, the soft (deterministic) attention mechanism is trainable by stan-\\ndard backpropagation methods. The hard attention deﬁnes a location variable\\nst, and uses it to decide where to focus attention when generating the t-th\\nword. When the hard attention is applied, the attention locations are con-\\nsidered as intermediate latent variables. It assigns a multinoulli distribution\\nparametrized by αi, and ct becomes a random variable. Here, st,i is deﬁned\\nas a one-hot variable which is set to 1 if the i-th location is used to extract\\nvisual features [48]:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 5}, page_content='6\\nDerya Soydaner\\np(st,i = 1|sj<t, a) = αt,i\\n(5)\\nct =\\nX\\ni\\nst,iai\\n(6)\\nWhereas learning hard attention requires sampling the attention location\\nst each time, the soft attention mechanism computes a weighted annotation\\nvector similar to [44] and takes the expectation of the context vector ct directly:\\nEp(st|α)[ct] =\\nL\\nX\\ni=1\\nαt,iai\\n(7)\\nFurthermore, in training the deterministic version of the model, an alterna-\\ntive method namely doubly stochastic attention, is proposed with an additional\\nconstraint added to the training objective to encourage the model to pay equal\\nattention to all parts of the image.\\nThe third study should be emphasized presents two classes of attention\\nmechanisms for NMT: the global attention that always attends to all source\\nwords, and the local attention that only looks at a subset of source words at\\na time [51]. These mechanisms derive the context vector ct in diﬀerent ways:\\nWhereas the global attention considers all the hidden states of the encoder, the\\nlocal one selectively focuses on a small window of context. In global attention, a\\nvariable-length alignment vector is derived similar to Eq. (2). Here, the current\\ntarget hidden state ht is compared with each source hidden state ¯hs by using a\\nscore function instead of the associated energy eij. Thus, the alignment vector\\nwhose size equals the number of time steps on the source side is derived.\\nGiven the alignment vector as weights, the context vector ct is computed as\\nthe weighted average over all the source hidden states. Here, score is referred\\nas a content-based function, and three diﬀerent alternatives are considered [51].\\nOn the other side, the local attention is diﬀerentiable. Firstly, an aligned\\nposition pt is generated for each target word at a time t. Then, a window\\ncentered around the source position pt is used to compute the context vector\\nas a weighted average of the source hidden states within the window. The\\nlocal attention selectively focuses on a small window of context, and obtains\\nthe alignment vector from the current target state ht and the source states ¯hs\\nin the window [51].\\nThe introduction of these novel mechanisms in 2015 triggered the rise of\\nattention for neural networks. Based on the proposed attention mechanisms,\\nsigniﬁcant research has been conducted in a variety of tasks. In order to imag-\\nine the attention idea in neural networks better, two visual examples are shown\\nin Fig. 2. A neural image caption generation task is seen in the top row that\\nimplements an attention mechanism [48]. Then, the second example shows\\nhow the attention mechanisms can be used for visual question answering [52].\\nBoth examples demonstrate how attention mechanisms focus on parts of input\\nimages.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 6}, page_content='Attention Mechanism in Neural Networks:\\n7\\nFig. 2 Examples of the attention mechanism in visual. (Top) Attending to the correct\\nobject in neural image caption generation [48]. (Bottom) Visualization of original image\\nand question pairs, and co-attention maps namely word-level, phrase-level and question-\\nlevel, respectively [52].\\n4 2015-2016: Attack of the Attention\\nDuring two years from 2015, the attention mechanisms were used for diﬀerent\\ntasks, and novel neural network architectures were presented applying these\\nmechanisms. After the memory networks [53] that require a supervision signal\\ninstructing them how to use their memory cells, the introduction of the neural\\nTuring machine [54] allows end-to-end training without this supervision signal,\\nvia the use of a content-based soft attention mechanism [1]. Then, end-to-end\\nmemory network [55] that is a form of memory network based on a recurrent\\nattention mechanism is proposed.\\nIn these years, an attention mechanism called self-attention, sometimes\\ncalled intra-attention, was successfully implemented within a neural network\\narchitecture namely Long Short-Term Memory-Networks (LSTMN) [56]. It\\nmodiﬁes the standard LSTM structure by replacing the memory cell with a\\nmemory network [53]. This is because memory networks have a set of key\\nvectors and a set of value vectors, whereas LSTMs maintain a hidden vector\\nand a memory vector [56]. In contrast to attention idea in [44], memory and\\nattention are added within a sequence encoder in LSTMN. In order to compute\\na representation of a sequence, self-attention is described as relating diﬀerent\\npositions of it [19]. One of the ﬁrst approaches of self-attention is applied for\\nnatural language inference [57].\\nMany attention-based models have been proposed for neural image cap-\\ntioning [58], abstractive sentence summarization [59], speech recognition [60,\\n61], automatic video captioning [62], neural machine translation [63], and rec-\\nognizing textual entailment [64]. Diﬀerent attention-based models perform vi-\\nsual question answering [65,66,67]. An attention-based CNN is presented for\\nmodeling sentence pairs [68]. A recurrent soft attention based model learns to\\nfocus selectively on parts of the video frames and classiﬁes videos [69].\\nOn the other side, several neural network architectures have been pre-\\nsented in a variety of tasks. For instance, Stacked Attention Network (SAN)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 7}, page_content='8\\nDerya Soydaner\\nis described for image question answering [70]. Deep Attention Recurrent Q-\\nNetwork (DARQN) integrates soft and hard attention mechanisms into the\\nstructure of Deep Q-Network (DQN) [71]. Wake-Sleep Recurrent Attention\\nModel (WS-RAM) speeds up the training time for image classiﬁcation and\\ncaption generation tasks [72]. alignDRAW model, an extension of the Deep\\nRecurrent Attention Writer (DRAW) [73], is a generative model of images\\nfrom captions using a soft attention mechanism [74]. Generative Adversarial\\nWhat-Where Network (GAWWN) synthesizes images given instructions de-\\nscribing what content to draw in which location [75].\\n5 The Transformer: Return of the Attention\\nAfter the proposed attention mechanisms in 2015, researchers published stud-\\nies that mostly modifying or implementing them to diﬀerent tasks. However,\\nin 2017, a novel neural network architecture, namely the Transformer, based\\nentirely on self-attention was presented [19]. The Transformer achieved great\\nresults on two machine translation tasks in addition to English constituency\\nparsing. The most impressive point about this architecture is that it contains\\nneither recurrence nor convolution. The Transformer performs well by replac-\\ning the conventional recurrent layers in encoder-decoder architecture used for\\nNMT with self-attention.\\nThe Transformer is composed of encoder-decoder stacks each of which has\\nsix identical layers within itself. In Fig. 3, one encoder-decoder stack is shown\\nto illustrate the model [19]. Each stack includes only attention mechanisms\\nand feedforward neural networks. As this architecture does not include any\\nrecurrent or convolutional layer, information about the relative or absolute\\npositions in the input sequence is given at the beginning of both encoder and\\ndecoder using positional encodings.\\nThe calculations of self-attention are slightly diﬀerent from the mechanisms\\ndescribed so far in this paper. It uses three vectors namely query, key and\\nvalue for each word. These vectors are computed by multiplying the input with\\nweight matrices Wq, Wk and Wv which are learned during training. In general,\\neach value is weighted by a function of the query with the corresponding key.\\nThe output is computed as a weighted sum of the values. Based on this idea,\\ntwo attention mechanisms are proposed: In the ﬁrst one, called scaled dot-\\nproduct attention, the dot products of the query with all keys are computed\\nas given in the right side of Fig. 3. Each result is divided to the square root of\\nthe dimension of the keys to have more stable gradients. They pass into the\\nsoftmax function, thus the weights for the values are obtained. Finally each\\nsoftmax score is multiplied with the value as given in Eq. (8). The authors\\npropose computing the attention on a set of queries simultaneously by taking\\nqueries and keys of dimension dk, and values of dimension dv as inputs. The\\nkeys, queries and values are packed together into matrices K, Q and V. Finally,\\nthe output matrix is obtained as follows [19]:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 8}, page_content='Attention Mechanism in Neural Networks:\\n9\\nFig. 3 The Transformer architecture and the attention mechanisms it uses in detail [19].\\n(Left) The Transformer with one encoder-decoder stack. (Center) Multi-head attention.\\n(Right) Scaled dot-product attention.\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(8)\\nThis calculation is performed by every word against the other words. This\\nleads to having values of each word relative to each other. For instance, if\\nthe word x2 is not relevant for the word x1, then the softmax score gives low\\nprobability scores. As a result, the corresponding value is decreased. This leads\\nto an increase in the value of relevant words, and those of others decrease. In\\nthe end, every word obtains a new value for itself.\\nAs seen from Fig. 3, the Transformer model does not directly use scaled\\ndot-product attention. But the attention mechanism it uses is based on these\\ncalculations. The second mechanism proposed, called the multi-head attention,\\nlinearly projects the queries, keys and values h times with diﬀerent, learned\\nlinear projections to dq, dk and dv dimensions, respectively [19]. The attention\\nfunction is performed in parallel on each of these projected versions of queries,\\nkeys and values, i.e., heads. By this way, dv-dimensional output values are\\nobtained. In order to get the ﬁnal values, they are concatenated and projected\\none last time as shown in the center of Fig. 3. By this way, the self-attention is\\ncalculated multiple times using diﬀerent sets of query, key and value vectors.\\nThus, the model can jointly attend to information at diﬀerent positions [19]:\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\n(9)\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nIn the decoder part of the Transformer, masked multi-head attention is\\napplied ﬁrst to ensure that only previous word embeddings are used when\\ntrying to predict the next word in the sentence. Therefore, the embeddings\\nthat shouldn’t be seen by the decoder are masked by multiplying with zero.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 9}, page_content='10\\nDerya Soydaner\\nAn interesting study examines the contribution made by individual atten-\\ntion heads in the encoder [76]. Also, there is an evaluation of the eﬀects of\\nself-attention on gradient propagation in recurrent networks [77]. For a deeper\\nanalysis of multi-head self-attention mechanism from a theoretical perspective\\nsee [78].\\nSelf-attention has been used successfully in a variety of tasks including\\nsentence embedding [79] and abstractive summarization [80]. It is shown that\\nself-attention can lead to improvements to discriminative constituency parser\\n[81], and speech recognition as well [82,83]. Also, the listen-attend-spell model\\n[84] has been improved with the self-attention for acoustic modeling [85].\\nAs soon as these self-attention mechanisms were proposed, they have been\\nincorporated with deep neural networks for a wide range of tasks. For instance,\\na deep learning model learned a number of large-scale tasks from multiple do-\\nmains with the aid of self-attention mechanism [86]. Novel self-attention neural\\nmodels are proposed for cross-target stance classiﬁcation [87] and NMT [88].\\nAnother study points out that a fully self-attentional model can reach com-\\npetitive predictive performance on ImageNet classiﬁcation and COCO object\\ndetection tasks [89]. Besides, developing novel attention mechanisms has been\\ncarried out such as area attention, a novel mechanism that can be used along\\nmulti-head attention [90]. It attends to areas in the memory by deﬁning the\\nkey of an area as the mean vector of the key of each item, and deﬁning the\\nvalue as the sum of all value vectors in the area.\\nWhen a novel mechanism is proposed, it is inevitable to incorporate it\\ninto the GAN framework [91]. Self-Attention Generative Adversarial Networks\\n(SAGANs) [92] introduce a self-attention mechanism into convolutional GANs.\\nDiﬀerent from the traditional convolutional GANs, SAGAN generates high-\\nresolution details using cues from all feature locations. Similarly, Attentional\\nGenerative Adversarial Network (AttnGAN) is presented for text to image\\ngeneration [93]. On the other side, a machine reading and question answering\\narchitecture called QANet [94] is proposed without any recurrent networks. It\\nuses self-attention to learn the global interaction between each pair of words\\nwhereas convolution captures the local structure of the text. In another study,\\nGated Attention Network (GaAN) controls the importance of each attention\\nhead’s output by introducing gates [95]. Another interesting study introduces\\nattentive group convolutions with a generalization of visual self-attention [96].\\nA deep transformer model is implemented for language modeling over long\\nsequences [97].\\n5.1 Self-attention variants\\nIn recent years, self-attention has become an important research direction\\nwithin the deep learning community. Self-attention idea has been examined\\nin diﬀerent aspects. For example, self-attention is handled in a multi-instance\\nlearning framework [98]. The idea of Sparse Adaptive Connection (SAC) is\\npresented for accelerating and structuring self-attention [99]. The research on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 10}, page_content='Attention Mechanism in Neural Networks:\\n11\\nTable 1 Summary of Notation\\nSymbol\\nDeﬁnition\\na\\nannotation\\nc\\ncontext vector\\nα\\nweight\\ne\\nenergy\\nf\\nfeedforward neural network\\nh\\nhidden state\\nφ\\nhard (stochastic) / soft (deterministic) attention\\ns\\nlocation variable\\np\\nsource position\\nK, Q, V\\nkeys, queries and values matrices, respectively\\nWq, Wk, Wv\\nweight matrices for queries, keys and values, respectively\\nimproving self-attention continues as well [100,101,102]. Besides, based on the\\nself-attention mechanisms proposed in the Transformer, important studies that\\nmodify the self-attention have been presented. Some of the most recent and\\nprominent studies are summarized below.\\nRelation-aware self-attention It extends the self-attention mechanism by\\nregarding representations of the relative positions, or distances between se-\\nquence elements [103]. Thus, it can consider the pairwise relationships between\\ninput elements. This type of attention mechanism deﬁnes vectors to represent\\nthe edge between two inputs. It provides learning two distinct edge represen-\\ntations that can be shared across attention heads without requiring additional\\nlinear transformations.\\nDirectional self-attention (DiSA) A novel neural network architecture\\nfor learning sentence embedding named Directional Self-Attention Network\\n(DiSAN) [104] uses directional self-attention followed by a multi-dimensional\\nattention mechanism. Instead of computing a single importance score for each\\nword based on the word embedding, multi-dimensional attention computes a\\nfeature-wise score vector for each token. To extend this mechanism to the self-\\nattention, two variants are presented: The ﬁrst one, called multi-dimensional\\n‘token2token’ self-attention generates context-aware coding for each element.\\nThe second one, called multi-dimensional ‘source2token’ self-attention com-\\npresses the sequence into a vector [104]. On the other side, directional self-\\nattention produces context-aware representations with temporal information\\nencoded by using positional masks. By this way, directional information is en-\\ncoded. First, the input sequence is transformed to a sequence of hidden states\\nby a fully connected layer. Then, multi-dimensional token2token self-attention\\nis applied to these hidden states. Hence, context-aware vector representations\\nare generated for all elements from the input sequence.\\nReinforced self-attention (ReSA) A sentence-encoding model named Re-\\ninforced Self-Attention Network (ReSAN) uses reinforced self-attention (ReSA)\\nthat integrates soft and hard attention mechanisms into a single model. ReSA'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 11}, page_content='12\\nDerya Soydaner\\nselects a subset of head tokens, and relates each head token to a small sub-\\nset of dependent tokens to generate their context-aware representations [105].\\nFor this purpose, a novel hard attention mechanism called reinforced sequence\\nsampling (RSS), which selects tokens from an input sequence in parallel and\\ntrained via policy gradient, is proposed. Given an input sequence, RSS gener-\\nates an equal-length sequence of binary random variables that indicates both\\nthe selected and discarded ones. On the other side, the soft attention provides\\nreward signals back for training the hard attention. The proposed RSS pro-\\nvides a sparse mask to self-attention. ReSA uses two RSS modules to extract\\nthe sparse dependencies between each pair of selected tokens.\\nOuter product attention (OPA) Self-Attentive Associative Memory (SAM)\\nis a novel operator based upon outer product attention (OPA) [106]. This at-\\ntention mechanism is an extension of dot-product attention [19]. OPA diﬀers\\nusing element-wise multiplication, outer product, and tanh function instead of\\nsoftmax.\\nBidirectional block self-attention (Bi-BloSA) Another mechanism, bidi-\\nrectional block self-attention (Bi-BloSA) which is simply a masked block self-\\nattention (mBloSA) with forward and backward masks to encode the tempo-\\nral order information is presented [107]. Here, mBloSA is composed of three\\nparts from its bottom to top namely intra-block self-attention, inter-block\\nself-attention and the context fusion. It splits a sequence into several length-\\nequal blocks, and applies an intra-block self-attention to each block indepen-\\ndently. Then, inter-block self-attention processes the outputs for all blocks.\\nThis stacked self-attention model results a reduction in the amount of mem-\\nory compared to a single one applied to the whole sequence. Finally, a feature\\nfusion gate combines the outputs of intra-block and inter-block self-attention\\nwith the original input, to produce the ﬁnal context-aware representations of\\nall tokens.\\nFixed multi-head attention The ﬁxed multi-head attention proposes ﬁxing\\nthe head size of the Transformer in the aim of improving the representation\\npower [108]. This study emphasizes its importance by setting the head size of\\nattention units to input sequence length.\\nSparse sinkhorn attention It is based on the idea of diﬀerentiable sorting\\nof internal representations within the self-attention module [109]. Instead of\\nallowing tokens to only attend to tokens within the same block, it operates\\non block sorted sequences. Each token attends to tokens in the sorted block.\\nThus, tokens that may be far apart in the unsorted sequence can be considered.\\nAdditionally, a variant of this mechanism named SortCut sinkhorn attention\\napplies a post-sorting truncation of the input sequence.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 12}, page_content='Attention Mechanism in Neural Networks:\\n13\\nAdaptive attention span Adaptive attention span is proposed as an alter-\\nnative to self-attention [110]. It learns the attention span of each head inde-\\npendently. To this end, a masking function inspired by [111] is used to control\\nthe attention span for each head. The purpose of this novel mechanism is to\\nreduce the computational burden of the Transformer. Additionally, dynamic\\nattention span approach is presented to dynamically change the attention span\\nbased on the current input as an extension [51,112].\\n5.2 Transformer variants\\nDiﬀerent from developing novel self-attention mechanisms, several studies have\\nbeen published in the aim of improving the performance of the Transformer.\\nThese studies mostly modify the model architecture. For instance, an addi-\\ntional recurrence encoder is preferred to model recurrence for Transformer di-\\nrectly [113]. In another study, a new weight initialization scheme is applied to\\nimprove Transformer optimization [114]. A novel positional encoding scheme\\nis used to extend the Transformer to tree-structured data [115]. Investigating\\nmodel size by handling Transformer width and depth for eﬃcient training is\\nalso an active research area [116]. Transformer is used in reinforcement learn-\\ning settings [117,118,119] and for time series forecasting in adversarial training\\nsetting [120].\\nBesides, many Transformer variants have been presented in the recent past.\\nCOMmonsEnse Transformer (COMET) is introduced for automatic construc-\\ntion of commonsense knowledge bases [121]. Evolved Transformer applies neu-\\nral architecture search for a better Transformer model [122]. Transformer Au-\\ntoencoder is a sequential autoencoder for conditional music generation [123].\\nCrossTransformer takes a small number of labeled images and an unlabeled\\nquery, and computes distances between spatially-corresponding features to in-\\nfer class membership [124]. DEtection TRansformer (DETR) is a new design\\nfor object detection systems [125], and Deformable DETR is an improved ver-\\nsion that achieves better performance in less time [126]. FLOw-bAsed Trans-\\nformER (FLOATER) emphasizes the importance of position encoding in the\\nTransformer, and models the position information via a continuous dynamical\\nmodel [127]. Disentangled Context (DisCo) Transformer simultaneously gener-\\nates all tokens given diﬀerent contexts by predicting every word in a sentence\\nconditioned on an arbitrary subset of the rest of the words [128]. Genera-\\ntive Adversarial Transformer (GANsformer) is presented for visual generative\\nmodeling [129].\\nRecent work has demonstrated signiﬁcant performance on NLP tasks. In\\nOpenAI GPT, there is a left-to-right architecture, where every token can only\\nattend to previous tokens in the self-attention layers of the Transformer [130].\\nGPT-2 [131] and GPT-3 [18] models have improved the progress. In addition\\nto these variants, some prominent Transformer-based models are summarized\\nbelow.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 13}, page_content='14\\nDerya Soydaner\\nUniversal Transformer A generalization of the Transformer model named\\nthe Universal Transformer [132] iteratively computes representations Ht at\\nstep t for all positions in the sequence in parallel. To this end, it uses the\\nscaled dot-product attention in Eq. (8) where d is the number of columns\\nof Q, K and V. In the Universal Transformer, the multi-head self-attention\\nwith k heads is used. The representations Ht is mapped to queries, keys and\\nvalues with aﬃne projections using learned parameter matrices W Q ∈ℜd×d/k,\\nW K ∈ℜd×d/k, W V ∈ℜd×d/k and W O ∈ℜd×d [132]:\\nMultiHead(Ht) = Concat(head1, ..., headk)W O\\n(10)\\nwhere headi = Attention(HtW Q\\ni , HtW K\\ni , HtW V\\ni )\\nImage Transformer Image Transformer [133] demonstrates that self-attention\\nbased models can also be well-suited for images instead of text. This Trans-\\nformer type restricts the self-attention mechanism to attend to local neigh-\\nborhoods. Thus, the size of images that the model can process is increased.\\nIts larger receptive ﬁelds allow the Image Transformer to signiﬁcantly improve\\nthe model performance on image generation as well as image super-resolution.\\nTransformer-XL This study aims to improve the ﬁxed-length context of the\\nTransformer [19] for language modeling. Transformer-XL [134] makes model-\\ning very long-term dependency possible by reusing the hidden states obtained\\nin previous segments. Hence, information can be propagated through the recur-\\nrent connections. In order to reuse the hidden states without causing temporal\\nconfusion, Transformer-XL uses relative positional encodings. Based on this\\narchitecture, a modiﬁed version named the Gated Transformer-XL (GTrXL)\\nis presented in the reinforcement learning setting [135].\\nTensorized Transformer Tensorized Transformer [136] compresses the multi-\\nhead attention in Transformer. To this end, it uses a novel self-attention model\\nmulti-linear attention with Block-Term Tensor Decomposition (BTD) [137]. It\\nbuilds a single-block attention based on the Tucker decomposition [138]. Then,\\nit uses a multi-linear attention constructed by a BTD to compress the multi-\\nhead attention mechanism. In Tensorized Transformer, the factor matrices are\\nshared across multiple blocks.\\nBERT The Bidirectional Encoder Representations from Transformers (BERT)\\naims to pre-train deep bidirectional representations from unlabeled text [139].\\nBERT uses a multilayer bidirectional Transformer as the encoder. Besides,\\ninspired by the Cloze task [140], it has a masked language model pre-training\\nobjective. BERT randomly masks some of the tokens from the input, and pre-\\ndicts the original vocabulary id of the masked word based only on its context.\\nThis model can pre-train a deep bidirectional Transformer. In all layers, the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 14}, page_content='Attention Mechanism in Neural Networks:\\n15\\npre-training is carried out by jointly conditioning on both left and right con-\\ntext. BERT diﬀers from the left-to-right language model pre-training from this\\naspect.\\nRecently, BERT model has been examined in detail. For instance, the be-\\nhaviour of attention heads are analysed [141]. Various methods have been\\ninvestigated for compressing [142,143], pruning [144], and quantization [145].\\nAlso, BERT model has been considered for diﬀerent tasks such as coreference\\nresolution [146]. A novel method is proposed in order to accelerate BERT\\ntraining [147].\\nFurthermore, various BERT variants have been presented. ALBERT aims\\nto increase the training speed of BERT, and presents two parameter reduction\\ntechniques [148]. Similarly, PoWER-BERT [149] is developed to improve the\\ninference time of BERT. This scheme is also used to accelerate ALBERT. Also,\\nTinyBERT is proposed to accelerate inference and reduce model size while\\nmaintaining accuracy [150]. In order to obtain better representations, Span-\\nBERT is proposed as a pre-training method [151]. As a robustly optimized\\nBERT approach, RoBERTa shows that BERT was signiﬁcantly undertrained\\n[152]. Also, DeBERTa improves RoBERTa using the disentangled attention\\nmechanism [153]. On the other side, DistilBERT shows that it is possible to\\nreach similar performances using much smaller language models pre-trained\\nwith knowledge distillation [154]. StructBERT proposes two novel lineariza-\\ntion strategies [155]. Q-BERT is introduced for quantizing BERT models [156],\\nBioBERT is for biomedical text mining [157], and RareBERT is for rare dis-\\nease diagnosis [158].\\nSince 2017 when the Transformer was presented, research directions have\\ngenerally focused on novel self-attention mechanisms, adapting the Trans-\\nformer for various tasks, or making them more understandable. In one of the\\nmost recent studies, NLP becomes possible in the mobile setting with Lite\\nTransformer. It applies long-short range attention where some heads specialize\\nin the local context modeling while the others specialize in the long-distance\\nrelationship modeling [159]. A deep and light-weight Transformer DeLighT\\n[160] and a hypernetwork-based model namely HyperGrid Transformers [161]\\nperform with fewer parameters. Graph Transformer Network is introduced\\nfor learning node representations on heterogeneous graphs [162] and diﬀerent\\napplications are performed for molecular data [163] or textual graph represen-\\ntation [164]. Also, Transformer-XH applies eXtra Hop attention for structured\\ntext data [165]. AttentionXML is a tree-based model for extreme multi-label\\ntext classiﬁcation [166]. Besides, attention mechanism is handled in a Bayesian\\nframework [167]. For a better understanding of Transformers, an identiﬁabil-\\nity analysis of self-attention weights is conducted in addition to presenting\\neﬀective attention to improve explanatory interpretations [168]. Lastly, Vision\\nTransformer (ViT) processes an image using a standard Transformer encoder\\nas used in NLP by interpreting it as a sequence of patches, and performs well\\non image classiﬁcation tasks [169].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 15}, page_content='16\\nDerya Soydaner\\n5.3 What about complexity?\\nAll these aforementioned studies undoubtedly demonstrate signiﬁcant success.\\nBut success not make one great. The Transformer also brings a very high\\ncomputational complexity and memory cost. The necessity of storing atten-\\ntion matrix to compute the gradients with respect to queries, keys and val-\\nues causes a non-negligible quadratic computation and memory requirements.\\nTraining the Transformer is a slow process for very long sequences because\\nof its quadratic complexity. There is also time complexity which is quadratic\\nwith respect to the sequence length. In order to improve the Transformer in\\nthis respect, recent studies have been conducted to improve this issue. One\\nof them is Linear Transformer which expresses the self-attention as a linear\\ndot-product of kernel feature maps [170]. Linear Transformer reduces both\\nmemory and time complexity by changing the self-attention from the softmax\\nfunction in Eq. (8) to a feature map based dot-product attention. Its per-\\nformance is competitive with the vanilla Transformer architecture on image\\ngeneration and automatic speech recognition tasks while being faster during\\ninference. On the other side, FMMformers which use the idea of the fast multi-\\npole method (FMM) [171] outperform the linear Transformer by decomposing\\nthe attention matrix into near-ﬁeld and far-ﬁeld attention with linear time and\\nmemory complexity [172].\\nAnother suggestion made in response to the Transformer’s quadratic na-\\nture is The Reformer that replaces dot-product attention by one that uses\\nlocality-sensitive hashing [173]. It reduces the complexity but one limitation\\nof the Reformer is its requirement for the queries and keys to be identical. Set\\nTransformer aims to reduce computation time of self-attention from quadratic\\nto linear by using an attention mechanism based on sparse Gaussian process\\nliterature [174]. Routing Transformer aims to reduce the overall complexity\\nof attention by learning dynamic sparse attention patterns by using routing\\nattention with clustering [175]. It applies k-means clustering to model sparse\\nattention matrices. At ﬁrst, queries and keys are assigned to clusters. The at-\\ntention scheme is determined by considering only queries and keys from the\\nsame cluster. Thus, queries are routed to keys belonging to the same cluster\\n[175].\\nSparse Transformer introduces sparse factorizations of the attention ma-\\ntrix by using factorized self-attention, and avoids the quadratic growth of com-\\nputational burden [176]. It also shows the possibility of modeling sequences\\nof length one million or more by using self-attention in theory. In the Trans-\\nformer, all the attention heads with the softmax attention assign a non-zero\\nweight to all context words. Adaptively Sparse Transformer replaces softmax\\nwith α-entmax which is a diﬀerentiable generalization of softmax allowing\\nlow-scoring words to receive precisely zero weight [177]. By means of context-\\ndependent sparsity patterns, the attention heads become ﬂexible in the Adap-\\ntively Sparse Transformer. Random feature attention approximates softmax\\nattention with random feature methods [178]. Skyformer replaces softmax\\nwith a Gaussian kernel and adapts Nystr¨om method [179]. A sparse atten-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 16}, page_content='Attention Mechanism in Neural Networks:\\n17\\ntion mechanism named BIGBIRD aims to reduce the quadratic dependency\\nof Transformer-based models to linear [180]. Diﬀerent from the similar stud-\\nies, BIGBIRD performs well for genomics data alongside NLP tasks such as\\nquestion answering.\\nMusic Transformer [181] shows that self-attention can also be useful for\\nmodeling music. This study emphasizes the infeasibility of the relative po-\\nsition representations introduced by [103] for long sequences because of the\\nquadratic intermediate relative information in the sequence length. Therefore,\\nthis study presents an extended version of relative attention named relative\\nlocal attention that improves the relative attention for longer musical com-\\npositions by reducing its intermediate memory requirement to linear in the\\nsequence length. A softmax-free Transformer (SOFT) is presented to improve\\nthe computational eﬃciency of ViT. It uses Gaussian kernel function instead\\nof the dot-product similarity [182].\\nAdditionally, various approaches have been presented in Hierarchical Vi-\\nsual Transformer [183], Long-Short Transformer (Transformer-LS) [184], Per-\\nceiver [185], and Performer [186]. Image Transformer based on the cross-\\ncovariance matrix between keys and queries is applied [187], and a new vi-\\nsion Transformer is proposed [188]. Furthermore, a Bernoulli sampling atten-\\ntion mechanism decreases the quadratic complexity to linear [189]. A novel\\nlinearized attention mechanism performs well on object detection, instance\\nsegmentation, and stereo depth estimation [190]. A study shows that kernel-\\nized attention with relative positional encoding can be calculated using Fast\\nFourier Transform and it leads to get rid of the quadratic complexity for long\\nsequences [191]. A linear uniﬁed nested attention mechanism namely Luna\\nuses two nested attention functions to approximate the softmax attention in\\nTransformer to achieve linear time and space complexity [192].\\n6 Concluding Remarks: A New Hope\\nInspired by the human visual system, the attention mechanisms in neural net-\\nworks have been developing for a long time. In this study, we examine this\\nduration beginning with its roots up to the present time. Some mechanisms\\nhave been modiﬁed, or novel mechanisms have emerged in this period. Today,\\nthis journey has reached a very important stage. The idea of incorporating\\nattention mechanisms into deep neural networks has led to state-of-the-art re-\\nsults for a large variety of tasks. Self-attention mechanisms and GPT-n family\\nmodels have become a new hope for more advanced models. These promising\\nprogress bring the questions whether the attention could help further devel-\\nopment, replace the popular neural network layers, or could be a better idea\\nthan the existing attention mechanisms? It is still an active research area and\\nmuch to learn we still have, but it is obvious that more powerful systems are\\nawaiting when neural networks and attention mechanisms join forces.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 17}, page_content='18\\nDerya Soydaner\\nConﬂict of interest\\nThe author declares that she has no conﬂict of interest.\\nReferences\\n1. I. Goodfellow, Y. Bengio, A. Courville, The MIT Press (2016)\\n2. D. Noton, L. Stark, Scientiﬁc American 224(6), 34 (1971)\\n3. D. Noton, L. Stark, Vision Research 11, 929 (1971)\\n4. E. Alpaydın, Advances in Neural Information Processing Systems 8 pp. 771–777 (1995)\\n5. S. Ahmad, Advances in Neural Information Processing Systems 4 pp. 420–427 (1991)\\n6. M. Posner, S. Petersen, Annual Review of Neuroscience 13(1), 25 (1990)\\n7. C. Bundesen, Psychological Review 97(4), 523 (1990)\\n8. R. Desimone, J. Duncan, Annual Review of Neuroscience 18(1), 193 (1995)\\n9. M. Corbetta, G. Shulman, Nature Reviews Neuroscience 3(3), 201 (2002)\\n10. S. Petersen, M. Posner, Annual Review of Neuroscience 35, 73 (2012)\\n11. R. Rimey, C. Brown, Technical Report, University of Rochester (1990)\\n12. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 98(3), 507 (1994)\\n13. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 105(2), 261 (1995)\\n14. J. Hoﬀman, B. Subramaniam, Perception and Psychophysics 57(6), 787 (1995)\\n15. S. Chaudhari, et al., ACM Transactions on Intelligent Systems and Technology (TIST)\\npp. 1–32 (2021)\\n16. A. Galassi, et al., IEEE Transactions on Neural Networks and Learning Systems (2020)\\n17. J. Lee, et al., ACM Transactions on Knowledge Discovery from Data (TKDD) 13(6),\\n1 (2019)\\n18. T. Brown, et al., Advances in Neural Information Processing Systems 33 pp. 1877–1901\\n(2020)\\n19. A. Vaswani, et al., Advances in Neural Information Processing Systems 30 pp. 5998–\\n6008 (2017)\\n20. K. Fukushima, Biological Cybernetics 36, 193 (1980)\\n21. K. Fukushima, Applied Optics 26(23), 4985 (1987)\\n22. K. Fukushima, T. Imagawa, Neural Networks 6(1), 33 (1993)\\n23. E. Postma, H.V. den Herik, P. Hudson, Neural Networks 10(6), 993 (1997)\\n24. J. Schmidhuber, R. Huber, International Journal of Neural Systems pp. 125–134 (1991)\\n25. R. Milanese, et al., IEEE Computer Society Conference on Computer Vision and Pat-\\ntern Recoginition, Seattle, WA, USA pp. 781–785 (1994)\\n26. J. Tsotsos, et al., Artiﬁcial Intelligence 78(1-2), 507 (1995)\\n27. S. Culhane, J. Tsotsos, Proceedings of the 11th IAPR International Conference on\\nPattern Recognition, The Hague, Netherlands pp. 36–40 (1992)\\n28. D. Reisfeld, H. Wolfson, Y. Yeshurun, International Journal of Computer Vision 14(2),\\n119 (1995)\\n29. I. Rybak, et al., Vision Research 38(15-16), 2387 (1998)\\n30. J. Keller, et al., Pattern Analysis and Applications 2(3) (1999)\\n31. F. Miau, L. Itti, Proceedings of the 23rd Annual International Conference of the IEEE\\nEngineering in Medicine and Biology Society, Istanbul, Turkey pp. 789–792 (2001)\\n32. W. Zhang, et al., Advances in Neural Information Processing Systems 19 pp. 1609–1616\\n(2006)\\n33. A. Salah, E. Alpaydın, L. Akarun, IEEE Transactions on Pattern Analysis and Machine\\nIntelligence 24(3), 420 (2002)\\n34. D. Walther, et al., International Workshop on Biologically Motivated Computer Vision,\\nSpringer, Berlin, Heidelberg pp. 472–479 (2002)\\n35. K. Schill, et al., Journal of Electronic Imaging 10(1), 152 (2001)\\n36. L. Paletta, G. Fritz, C. Seifert, International Conference on Machine Learning (2005)\\n37. O.L. Meur, et al., IEEE Transactions on Pattern Analysis and Machine Intelligence\\n28(5), 802– (2006)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 18}, page_content='Attention Mechanism in Neural Networks:\\n19\\n38. S. Gould, et al., International Joint Conference on Artiﬁcial Intelligence (IJCAI) pp.\\n2115–2121 (2007)\\n39. H. Larochelle, G. Hinton, Advances in Neural Information Processing Systems 23 pp.\\n1243–1251 (2010)\\n40. L. Bazzani, et al., International Conference on Machine Learning (2011)\\n41. V. Mnih, et al., Advances in Neural Information Processing Systems 27 pp. 2204–2212\\n(2014)\\n42. M. Stollenga, et al., Advances in Neural Information Processing Systems 27 pp. 3545–\\n3553 (2014)\\n43. Y. Tang, N. Srivastava, R. Salakhutdinov, Advances in Neural Information Processing\\nSystems 27 (2014)\\n44. D. Bahdanau, K. Cho, Y. Bengio, International Conference on Learning Representa-\\ntions (2015)\\n45. I. Sutskever, O. Vinyals, Q. Le, Advances in Neural Information Processing Systems\\n27 pp. 3104–3112 (2014)\\n46. K. Cho, et al., Proceedings of the 2014 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) pp. 1724–1734 (2014)\\n47. M. Schuster, K. Paliwal, IEEE Transactions on Signal Processing 45(11), 2673 (1997)\\n48. K. Xu, et al., International Conference on Machine Learning pp. 2048–2057 (2015)\\n49. O. Vinyals, et al., In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition pp. 3156–3164 (2015)\\n50. R. Williams, Machine Learning 8(3-4), 229 (1992)\\n51. M.T. Luong, H.P..C. Manning, Proceedings of the 2015 Conference on Empirical Meth-\\nods in Natural Language Processing, Lisbon, Portugal pp. 1412–1421 (2015)\\n52. J. Lu, et al., Advances in Neural Information Processing Systems 29 (2016)\\n53. J. Weston, S. Chopra, A. Bordes, International Conference on Learning Representa-\\ntions (2014)\\n54. A. Graves, G. Wayne, I. Danihelka, arXiv preprint arXiv:1410.5401 (2014)\\n55. S. Sukhbaatar, et al., Advances in Neural Information Processing Systems 28 pp. 2440–\\n2448 (2015)\\n56. J. Cheng, L. Dong, M. Lapata, Proceedings of the 2016 Conference on Empirical\\nMethods in Natural Language Processing pp. 551–561 (2016)\\n57. A. Parikh, et al., Proceedings of the 2016 Conference on Empirical Methods in Natural\\nLanguage Processing, Austin, Texas pp. 2249–2255 (2016)\\n58. Q. You, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), Las Vegas, NV pp. 4651–4659 (2016)\\n59. A. Rush, S. Chopra, J. Weston, Proceedings of the 2015 Conference on Empirical\\nMethods in Natural Language Processing, Lisbon, Portugal pp. 379–389 (2015)\\n60. D. Yu, et al., Interspeech pp. 17–21 (2016)\\n61. J. Chorowski, et al., Advances in Neural Information Processing Systems 28 pp. 577–\\n585 (2015)\\n62. M. Zanﬁr, E. Marinoiu, C. Sminchisescu, In Asian Conference on Computer Vision,\\nSpringer, Cham pp. 104—-119 (2016)\\n63. Y. Cheng, et al., Proceedings of the 25th International Joint Conference on Artiﬁcial\\nIntelligence (2016)\\n64. T. Rockt International Conference on Learning Representations (2016)\\n65. Y. Zhu, et al., Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition pp. 4995–5004 (2016)\\n66. K. Chen, et al., arXiv preprint arXiv:1511.05960 (2015)\\n67. H. Xu, K. Saenko, In European Conference on Computer Vision pp. 451–466 (2016)\\n68. W. Yin, et al., Transactions of the Association for Computational Linguistics 4, 259\\n(2016)\\n69. S. Sharma, R. Kiros, R. Salakhutdinov, International Conference on Learning Repre-\\nsentations (2016)\\n70. Z. Yang, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition pp. 21–29 (2016)\\n71. I. Sorokin, et al., arXiv preprint arXiv:1512.01693 (2015)\\n72. J. Ba, et al., Advances in Neural Information Processing Systems 28 pp. 2593–2601\\n(2015)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 19}, page_content='20\\nDerya Soydaner\\n73. K. Gregor, et al., International Conference on Machine Learning pp. 1462–1471 (2015)\\n74. E. Mansimov, et al., International Conference on Learning Representations (2016)\\n75. S. Reed, et al., Advances in Neural Information Processing Systems 29 pp. 217–225\\n(2016)\\n76. E. Voita, et al., In Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, Florence, Italy pp. 5797–5808 (2019)\\n77. G. Kerg, et al., Advances in Neural Information Processing Systems 33 (2020)\\n78. J.B. Cordonnier, A. Loukas, M. Jaggi, International Conference on Learning Repre-\\nsentations (2020)\\n79. Z. Lin, et al., International Conference on Learning Representations (2017)\\n80. R. Paulus, C. Xiong, R. Socher, International Conference on Learning Representations\\n(2018)\\n81. N. Kitaev, D. Klein, In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Long papers) pp. 2676–2686 (2018)\\n82. D. Povey, et al., IEEE International Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP), IEEE pp. 5874–5878 (2018)\\n83. A. Vyas, et al., Advances in Neural Information Processing Systems 33 (2020)\\n84. W. Chan, et al., IEEE International Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP), Shanghai pp. 4960—-4964 (2016)\\n85. M. Sperber, et al., In proceedings of Annual Conference of the International Speech\\nCommunication Association (InterSpeech) pp. 3723–3727 (2018)\\n86. L. Kaiser, et al., arXiv preprint arXiv:1706.05137 (2017)\\n87. C. Xu, et al., Proceedings of the 56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Short papers), Melbourne, Australia pp. 778–783 (2018)\\n88. S. Maruf, A. Martins, G. Haﬀari, Proceedings of NAACL-HLT, Minneapolis, Minnesota\\npp. 3092–3102 (2019)\\n89. P. Ramachandran, et al., Advances in Neural Information Processing Systems 32 pp.\\n68–80 (2019)\\n90. Y. Li, et al., International Conference on Machine Learning (2019)\\n91. I. Goodfellow, et al., Advances in Neural Information Processing Systems 27 pp. 2672–\\n2680 (2014)\\n92. H. Zhang, et al., International Conference on Machine Learning pp. 7354–7363 (2019)\\n93. T. Xu, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR) pp. 1316–1324 (2018)\\n94. A. Yu, et al., International Conference on Learning Representations (2018)\\n95. J. Zhang, et al., Conference on Uncertainty in Artiﬁcial Intelligence (2018)\\n96. D. Romero, et al., International Conference on Machine Learning (2020)\\n97. R. Al-Rfou, et al., AAAI Conference on Artiﬁcial Intelligence 33, 3159 (2019)\\n98. J. Du, et al., Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing pp. 2216–2225 (2018)\\n99. X. Li, et al., Advances in Neural Information Processing Systems 33 (2020)\\n100. B. Yang, et al., AAAI Conference on Artiﬁcial Intelligence 33, 387 (2019)\\n101. B. Yang, et al., Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing, Brussels, Belgium pp. 4449–4458 (2018)\\n102. Proceedings of the IEEE International Conference on Computer Vision pp. 3286–3295\\n103. P. Shaw, J. Uszkoreit, A. Vaswani, Proceedings of NAACL-HLT, New Orleans,\\nLouisiana pp. 464–468 (2018)\\n104. T. Shen, et al., AAAI Conference on Artiﬁcial Intelligence pp. 5446–5455 (2018)\\n105. T. Shen, et al., In Proceedings of the 27th International Joint Conference on Artiﬁcial\\nIntelligence, (IJCAI-18) pp. 4345–4352 (2018)\\n106. H. Le, T. Tran, S. Venkatesh, International Conference on Machine Learning (2020)\\n107. T. Shen, et al., International Conference on Learning Representations (2018)\\n108. S. Bhojanapalli, et al., International Conference on Machine Learning (2020)\\n109. Y. Tay, et al., International Conference on Machine Learning (2020)\\n110. S. Sukhbaatar, et al., Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, Florence, Italy pp. 331–335 (2019)\\n111. Y. Jernite, et al., International Conference on Learning Representations (2017)\\n112. R. Shu, H. Nakayama, In Proceedings of the First Workshop on Neural Machine Trans-\\nlation, Vancouver, Canada pp. 1–10 (2017)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 20}, page_content='Attention Mechanism in Neural Networks:\\n21\\n113. J. Hao, et al., Proceedings of NAACL-HLT, Minneapolis, Minnesota pp. 1198–1207\\n(2019)\\n114. X. Huang, et al., International Conference on Machine Learning (2020)\\n115. V. Shiv, C. Quirk, Advances in Neural Information Processing Systems 32 pp. 12,081–\\n12,091 (2019)\\n116. Z. Li, et al., International Conference on Machine Learning (2020)\\n117. Y. Hoshen, Advances in Neural Information Processing Systems 30, Long Beach, CA,\\nUSA (2017)\\n118. S. Hu, et al., International Conference on Learning Representations (2021)\\n119. E. Parisotto, R. Salakhutdinov, International Conference on Learning Representations\\n(2021)\\n120. S. Wu, et al., Advances in Neural Information Processing Systems 33 (2020)\\n121. A. Bosselut, et al., Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics (2019)\\n122. D. So, C. Liang, Q. Le, International Conference on Machine Learning (2019)\\n123. K. Choi, et al., International Conference on Machine Learning (2020)\\n124. C. Doersch, A. Gupta, A. Zisserman, Advances in Neural Information Processing Sys-\\ntems 33 pp. 21,981–21,993 (2020)\\n125. N. Carion, et al., European Conference on Computer Vision pp. 213—-229 (2020)\\n126. X. Zhu, et al., International Conference on Learning Representations (2021)\\n127. X. Liu, et al., International Conference on Machine Learning pp. 6327–6335 (2020)\\n128. J. Kasai, et al., International Conference on Machine Learning (2020)\\n129. D. Hudson, L. Zitnick, International Conference on Machine Learning pp. 4487–4499\\n(2021)\\n130. A. Radford, et al., Technical Report, OpenAI (2018)\\n131. A. Radford, et al., OpenAI blog p. 9 (2019)\\n132. M. Dehghani, et al., International Conference on Learning Representations (2019)\\n133. N. Parmar, International Conference on Machine Learning (2018)\\n134. Z. Dai, et al., Proceedings of the 57th Annual Meeting of the Association for Compu-\\ntational Linguistics pp. 2978–2988 (2019)\\n135. E. Parisotto, International Conference on Machine Learning (2020)\\n136. X. Ma, et al., Advances in Neural Information Processing Systems 32 pp. 2232–2242\\n(2019)\\n137. L. Lathauwer, SIAM Journal on Matrix Analysis and Applications 30(3), 1033 (2008)\\n138. L. Tucker, Psychometrika 31(3), 279 (1966)\\n139. J. Devlin, et al., Proceedings of NAACL-HLT 2019 pp. 4171–4186 (2019)\\n140. W. Taylor, Journalism Bulletin 30(4), 415 (1953)\\n141. K. Clark, et al., arXiv preprint arXiv:1906.04341 (2019)\\n142. S. Sun, et al., Proceedings of the 2019 Conference on Empirical Methods in Natural\\nLanguage Processing and the 9th International Joint Conference on Natural Language\\nProcessing, Hong Kong, China pp. 4323–4332 (2019)\\n143. W. Wang, et al., Advances in Neural Information Processing Systems 33 (2020)\\n144. J. McCarley, R. Chakravarti, A. Sil, arXiv preprint arXiv:1910.06360 (2020)\\n145. O. Zafrir, et al., The 5th Workshop on Energy Eﬃcient Machine Learning and Cogni-\\ntive Computing - NeurIPS (2019)\\n146. M. Joshi, et al., In Proceedings of the 2019 Conference on Empirical Methods in\\nNatural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing pp. 5803–5808 (2019)\\n147. L. Gong, et al., International Conference on Machine Learning pp. 2337–2346 (2019)\\n148. Z. Lan, et al., International Conference on Learning Representations (2020)\\n149. S. Goyal, et al., International Conference on Machine Learning (2020)\\n150. X. Jiao, et al., arXiv preprint arXiv:1909.10351 (2019)\\n151. M. Joshi, et al., Transactions of the Association for Computational Linguistics 8, 64\\n(2020)\\n152. Y. Liu, et al., arXiv preprint arXiv:1907.11692 (2019)\\n153. P. He, et al., International Conference on Learning Representations (2021)\\n154. V. Sanh, et al., the 5th Workshop on Energy Eﬃcient Machine Learning and Cognitive\\nComputing - NeurIPS (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'source': 'data/pdf_files/NN_attention.pdf', 'file_path': 'data/pdf_files/NN_attention.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'trapped': '', 'modDate': 'D:20220429002620Z', 'creationDate': 'D:20220429002620Z', 'page': 21}, page_content='22\\nDerya Soydaner\\n155. W. Wang, et al., International Conference on Learning Representations (2020)\\n156. S. Shen, et al., AAAI Conference on Artiﬁcial Intelligence 34, 8815 (2020)\\n157. J. Lee, et al., Bioinformatics 36(4), 1234 (2020)\\n158. P. Prakash, et al., AAAI Conference on Artiﬁcial Intelligence 35, 453 (2021)\\n159. Z. Wu, et al., International Conference on Learning Representations (2020)\\n160. S. Mehta, et al., International Conference on Learning Representations (2021)\\n161. Y. Tay, et al., International Conference on Learning Representations (2021)\\n162. S. Yun, et al., International Conference on Learning Representations (2018)\\n163. Y. Rong, et al., Advances in Neural Information Processing Systems 33 (2020)\\n164. J. Yang, et al., Advances in Neural Information Processing Systems 34 (2021)\\n165. C. Zhao, et al., International Conference on Learning Representations (2020)\\n166. R. You, et al., Advances in Neural Information Processing Systems 32 (2019)\\n167. X. Fan, et al., Advances in Neural Information Processing Systems 33 (2020)\\n168. G. Brunner, et al., International Conference on Learning Representations (2020)\\n169. A. Dosovitskiy, et al., International Conference on Learning Representations (2021)\\n170. A. Katharopoulos, et al., International Conference on Machine Learning (2020)\\n171. L. Greengard, V. Rokhlin, Journal of Computational Physics 73(2), 325– (1987)\\n172. T. Nguyen, et al., Advances in Neural Information Processing Systems 34 (2021)\\n173. N. Kitaev, L. Kaiser, A. Levskaya, International Conference on Learning Representa-\\ntions (2020)\\n174. J. Lee, et al., International Conference on Machine Learning pp. 3744–3753 (2019)\\n175. A. Roy, et al., Transactions of the Association for Computational Linguistics pp. 53–68\\n(2020)\\n176. R. Child, et al., arXiv preprint arXiv:1904.10509 (2019)\\n177. G. Correia, V. Niculae, A. Martins, Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th International Joint Conference\\non Natural Language Processing pp. 2174–2184 (2019)\\n178. H. Peng, et al., International Conference on Learning Representations (2021)\\n179. Y. Chen, et al., Advances in Neural Information Processing Systems 34 (2021)\\n180. M. Zaheer, et al., Advances in Neural Information Processing Systems 33 (2020)\\n181. C.Z. Huang, et al., International Conference on Learning Representations (2019)\\n182. J. Lu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n183. Z. Pan, et al., Proceedings of the IEEE/CVF International Conference on Computer\\nVision pp. 377–386 (2021)\\n184. C. Zhu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n185. A. Jaegle, et al., International Conference on Machine Learning pp. 4651–4664 (2021)\\n186. K. Choromanski, et al., International Conference on Learning Representations (2021)\\n187. A. El-Nouby, et al., Advances in Neural Information Processing Systems 34 (2021)\\n188. Q. Yu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n189. Z. Zeng, et al., International Conference on Machine Learning pp. 12,321–12,332 (2021)\\n190. Z. Shen, et al., Proceedings of the IEEE/CVF Winter Conference on Applications of\\nComputer Vision pp. 3531–3539 (2021)\\n191. S. Luo, et al., Advances in Neural Information Processing Systems 34 (2021)\\n192. X. Ma, et al., Advances in Neural Information Processing Systems 34 (2021)')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
    "\n",
    "# load all the files\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"data/pdf_files\",\n",
    "    glob='**/*.pdf',\n",
    "    loader_cls=PyMuPDFLoader,\n",
    "    show_progress=True\n",
    "    )\n",
    "\n",
    "pdf_docs=dir_loader.load()\n",
    "pdf_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0588f5",
   "metadata": {},
   "source": [
    "# Embedding and VectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "394ab554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List,Dict,Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e0f5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str='all-MiniLM-L6-V2'):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model Loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model{self.model_name}:{e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts:List[str])->np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts,show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4b5eb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-V2\n",
      "Model Loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x1238de890>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize embedding manager\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c06b791",
   "metadata": {},
   "source": [
    "# VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735f4561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection : pdf_docs\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x123db8590>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, collection_name: str = \"pdf_docs\", persist_directory:str='data/vector_store'):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client=None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection : {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents:List[Any], embeddings:np.ndarray):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "\n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "\n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text =[]\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i,(doc,embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique id for each record\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            # Embeddings\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids = ids,\n",
    "                embeddings= embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection : {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vector_store = VectorStore()\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0189081a",
   "metadata": {},
   "source": [
    "## Convert the text to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79d417de",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [doc.page_content for doc in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327938c1",
   "metadata": {},
   "source": [
    "## Generate the Enbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08ec384c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 137 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:02<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (137, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedding_manager.generate_embeddings(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd07a2",
   "metadata": {},
   "source": [
    "## Store everything in  the vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ddffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b99e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
