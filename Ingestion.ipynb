{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08427696",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46469e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae7b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all pdfs inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "\n",
    "    pdf_files = list(pdf_dir.glob(\"*/*.pdf\"))\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\n Processing : {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "            print(f\" ✓ Loaded {len(documents)} pages\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" ✗ Error : {e}\")\n",
    "    print(f\"\\nTotal documents loaded : {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in a data directory\n",
    "all_pdf_documents = process_all_pdfs(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a76185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Splitting into Chunks\n",
    "\n",
    "def split_documents(documents, chunk_size = 1000, chunk_overlap=200):\n",
    "    \"\"\"Splitting docs into smaller chunks for better RAG Performance\"\"\"\n",
    "    text_spitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_spitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    # Show example chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata : {split_docs[0].metadata}\")\n",
    "    return split_docs\n",
    "\n",
    "chunks = split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b8bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=Document(\n",
    "    page_content=\"Let this be the main content I am using to create RAG\",\n",
    "    metadata={\n",
    "        \"source\":\"example.txt\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"shireesha\",\n",
    "        \"date_created\": \"2025-12-23\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4b8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a simple txt file\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe023d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts={\n",
    "    \"data/text_files/python_intro.txt\":\"\"\"Python is a high-level, interpreted programming language known for its clear syntax and code readability, which was created in 1991. It is popular for beginners due to its gentle learning curve and is widely used in web development, data science, artificial intelligence, and automation. \n",
    "\n",
    "Key features include:\n",
    "Dynamic Typing: Variable types are automatically determined at runtime, which simplifies coding.\n",
    "Multiple Paradigms: It supports object-oriented, functional, and procedural programming styles.\n",
    "Extensive Libraries: A vast ecosystem of libraries and the Python Package Index (PyPI) offer ready-to-use code for various tasks.\"\"\",\n",
    "    \"data/text_files/machine_learning.txt\":\"\"\"Machine Learning (ML) is a type of Artificial Intelligence (AI) that lets computers learn from data to find patterns, make decisions, and predict outcomes, without being explicitly programmed for every task, using algorithms trained on vast datasets for tasks like image recognition, recommendations, and translation. Key types include Supervised Learning (labeled data, e.g., spam filters), Unsupervised Learning (unlabeled data, e.g., customer grouping), and Reinforcement Learning (learning via rewards/penalties for decision-making). \"\"\"\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "print(\"Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7effa2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the text using text loaders of langchain\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/text_files/python_intro.txt\", encoding=\"utf-8\")\n",
    "\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b9787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"data/text_files\",\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "text_docs=dir_loader.load()\n",
    "text_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3017d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
    "\n",
    "# load all the files\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"data/pdf_files\",\n",
    "    glob='**/*.pdf',\n",
    "    loader_cls=PyMuPDFLoader,\n",
    "    show_progress=True\n",
    "    )\n",
    "\n",
    "pdf_docs=dir_loader.load()\n",
    "pdf_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0588f5",
   "metadata": {},
   "source": [
    "# Embedding and VectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ab554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List,Dict,Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str='all-MiniLM-L6-V2'):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model Loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model{self.model_name}:{e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts:List[str])->np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts,show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b5eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding manager\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c06b791",
   "metadata": {},
   "source": [
    "# VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735f4561",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, collection_name: str = \"pdf_docs\", persist_directory:str='data/vector_store'):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client=None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection : {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents:List[Any], embeddings:np.ndarray):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "\n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "\n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text =[]\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i,(doc,embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique id for each record\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            # Embeddings\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids = ids,\n",
    "                embeddings= embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection : {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vector_store = VectorStore()\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0189081a",
   "metadata": {},
   "source": [
    "## Convert the text to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d417de",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [doc.page_content for doc in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327938c1",
   "metadata": {},
   "source": [
    "## Generate the Enbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding_manager.generate_embeddings(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd07a2",
   "metadata": {},
   "source": [
    "## Store everything in  the vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ddffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601cf5c5",
   "metadata": {},
   "source": [
    "## Retriever Pipeline from Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b99e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    def __init__(self, vector_store:VectorStore, embedding_manager:EmbeddingManager):\n",
    "        self.vector_store=vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "        \n",
    "    def retrieve(self, query:str, top_k :int=5, score_threshold:float=0.0) -> List[Dict[str, Any]]:\n",
    "        print(f\"Retrieving documents for query : {query}\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        # Search in Vector Store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings = [query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "\n",
    "            # Process results as \"Context in form of list\"\n",
    "            retrieved_docs = []\n",
    "\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents,metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content':document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i+1                            \n",
    "                        })\n",
    "\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return[]\n",
    "\n",
    "\n",
    "rag_retriever = RAGRetriever(vector_store, embedding_manager)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf944f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1489358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
